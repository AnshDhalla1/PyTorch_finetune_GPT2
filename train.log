nohup: ignoring input
INFO:__main__:Logging initialized. Log file: outputs/lora_experiment/training_20250830_193330.log
INFO:__main__:🚀 TASK 3: Training Integration Pipeline
INFO:__main__:============================================================
INFO:__main__:Configuration:
INFO:__main__:  adapter_size: 64
INFO:__main__:  beta1: 0.9
INFO:__main__:  beta2: 0.999
INFO:__main__:  dataset_path: /mnt/pytorch/final files/dataset/alpaca_data.json
INFO:__main__:  epochs: 2
INFO:__main__:  eps: 1e-08
INFO:__main__:  eval_dataset_path: None
INFO:__main__:  eval_interval: 1
INFO:__main__:  freeze_layers: 0,1,2,3
INFO:__main__:  gradient_accumulation_steps: 1
INFO:__main__:  hidden_size: 768
INFO:__main__:  learning_rate: 0.0003
INFO:__main__:  log_interval: 10
INFO:__main__:  log_level: INFO
INFO:__main__:  lora_alpha: 8.0
INFO:__main__:  lora_rank: 4
INFO:__main__:  max_grad_norm: 1.0
INFO:__main__:  max_position_embeddings: 1024
INFO:__main__:  max_steps: None
INFO:__main__:  min_lr: 1e-06
INFO:__main__:  modification_type: lora
INFO:__main__:  no_cuda: False
INFO:__main__:  num_attention_heads: 12
INFO:__main__:  num_classes: None
INFO:__main__:  num_hidden_layers: 12
INFO:__main__:  output_dir: ./outputs/lora_experiment
INFO:__main__:  prefix_length: 50
INFO:__main__:  scheduler: warmup_cosine
INFO:__main__:  vocab_size: 50257
INFO:__main__:  warmup_steps: 100
INFO:__main__:  weight_decay: 0.01
INFO:__main__:============================================================
INFO:__main__:🔧 Using device: cuda
INFO:__main__:🚀 Creating model with Task 2 modifications...
🚀 Starting GPT-2 Model Modification...
✅ Base model created: GPT2LMHeadModel
   Architecture: 12 layers, 768 hidden size
   Output shape: [batch_size, seq_len, 50257]

🔄 Applying LORA modification...
   ✅ Applied LoRA to Conv1D: transformer.h.0.attn.c_attn
   ✅ Applied LoRA to Conv1D: transformer.h.0.attn.c_proj
   ✅ Applied LoRA to Conv1D: transformer.h.0.mlp.c_fc
   ✅ Applied LoRA to Conv1D: transformer.h.0.mlp.c_proj
   ✅ Applied LoRA to Conv1D: transformer.h.1.attn.c_attn
   ✅ Applied LoRA to Conv1D: transformer.h.1.attn.c_proj
   ✅ Applied LoRA to Conv1D: transformer.h.1.mlp.c_fc
   ✅ Applied LoRA to Conv1D: transformer.h.1.mlp.c_proj
   ✅ Applied LoRA to Conv1D: transformer.h.2.attn.c_attn
   ✅ Applied LoRA to Conv1D: transformer.h.2.attn.c_proj
   ✅ Applied LoRA to Conv1D: transformer.h.2.mlp.c_fc
   ✅ Applied LoRA to Conv1D: transformer.h.2.mlp.c_proj
   ✅ Applied LoRA to Conv1D: transformer.h.3.attn.c_attn
   ✅ Applied LoRA to Conv1D: transformer.h.3.attn.c_proj
   ✅ Applied LoRA to Conv1D: transformer.h.3.mlp.c_fc
   ✅ Applied LoRA to Conv1D: transformer.h.3.mlp.c_proj
   ✅ Applied LoRA to Conv1D: transformer.h.4.attn.c_attn
   ✅ Applied LoRA to Conv1D: transformer.h.4.attn.c_proj
   ✅ Applied LoRA to Conv1D: transformer.h.4.mlp.c_fc
   ✅ Applied LoRA to Conv1D: transformer.h.4.mlp.c_proj
   ✅ Applied LoRA to Conv1D: transformer.h.5.attn.c_attn
   ✅ Applied LoRA to Conv1D: transformer.h.5.attn.c_proj
   ✅ Applied LoRA to Conv1D: transformer.h.5.mlp.c_fc
   ✅ Applied LoRA to Conv1D: transformer.h.5.mlp.c_proj
   ✅ Applied LoRA to Conv1D: transformer.h.6.attn.c_attn
   ✅ Applied LoRA to Conv1D: transformer.h.6.attn.c_proj
   ✅ Applied LoRA to Conv1D: transformer.h.6.mlp.c_fc
   ✅ Applied LoRA to Conv1D: transformer.h.6.mlp.c_proj
   ✅ Applied LoRA to Conv1D: transformer.h.7.attn.c_attn
   ✅ Applied LoRA to Conv1D: transformer.h.7.attn.c_proj
   ✅ Applied LoRA to Conv1D: transformer.h.7.mlp.c_fc
   ✅ Applied LoRA to Conv1D: transformer.h.7.mlp.c_proj
   ✅ Applied LoRA to Conv1D: transformer.h.8.attn.c_attn
   ✅ Applied LoRA to Conv1D: transformer.h.8.attn.c_proj
   ✅ Applied LoRA to Conv1D: transformer.h.8.mlp.c_fc
   ✅ Applied LoRA to Conv1D: transformer.h.8.mlp.c_proj
   ✅ Applied LoRA to Conv1D: transformer.h.9.attn.c_attn
   ✅ Applied LoRA to Conv1D: transformer.h.9.attn.c_proj
   ✅ Applied LoRA to Conv1D: transformer.h.9.mlp.c_fc
   ✅ Applied LoRA to Conv1D: transformer.h.9.mlp.c_proj
   ✅ Applied LoRA to Conv1D: transformer.h.10.attn.c_attn
   ✅ Applied LoRA to Conv1D: transformer.h.10.attn.c_proj
   ✅ Applied LoRA to Conv1D: transformer.h.10.mlp.c_fc
   ✅ Applied LoRA to Conv1D: transformer.h.10.mlp.c_proj
   ✅ Applied LoRA to Conv1D: transformer.h.11.attn.c_attn
   ✅ Applied LoRA to Conv1D: transformer.h.11.attn.c_proj
   ✅ Applied LoRA to Conv1D: transformer.h.11.mlp.c_fc
   ✅ Applied LoRA to Conv1D: transformer.h.11.mlp.c_proj

✅ Applied LoRA with rank=4, alpha=8.0
   Modified 48 layers

📊 Final Parameter Statistics:
   Total parameters: 125,029,632
   Trainable parameters: 40,012,032
   Trainable ratio: 0.3200 (32.00%)

🔍 Parameter Trainability Status:
  ✅ transformer.wte.weight: Trainable (38,597,376 params)
  ✅ transformer.wpe.weight: Trainable (786,432 params)
  ✅ transformer.h.0.ln_1.weight: Trainable (768 params)
  ✅ transformer.h.0.ln_1.bias: Trainable (768 params)
  ✅ transformer.h.0.attn.c_attn.lora_A: Trainable (3,072 params)
  ✅ transformer.h.0.attn.c_attn.lora_B: Trainable (9,216 params)
  ❄️  transformer.h.0.attn.c_attn.original_layer.weight: Frozen (1,769,472 params)
  ❄️  transformer.h.0.attn.c_attn.original_layer.bias: Frozen (2,304 params)
  ✅ transformer.h.0.attn.c_proj.lora_A: Trainable (3,072 params)
  ✅ transformer.h.0.attn.c_proj.lora_B: Trainable (3,072 params)
  ❄️  transformer.h.0.attn.c_proj.original_layer.weight: Frozen (589,824 params)
  ❄️  transformer.h.0.attn.c_proj.original_layer.bias: Frozen (768 params)
  ✅ transformer.h.0.ln_2.weight: Trainable (768 params)
  ✅ transformer.h.0.ln_2.bias: Trainable (768 params)
  ✅ transformer.h.0.mlp.c_fc.lora_A: Trainable (3,072 params)
  ✅ transformer.h.0.mlp.c_fc.lora_B: Trainable (12,288 params)
  ❄️  transformer.h.0.mlp.c_fc.original_layer.weight: Frozen (2,359,296 params)
  ❄️  transformer.h.0.mlp.c_fc.original_layer.bias: Frozen (3,072 params)
  ✅ transformer.h.0.mlp.c_proj.lora_A: Trainable (12,288 params)
  ✅ transformer.h.0.mlp.c_proj.lora_B: Trainable (3,072 params)
  ❄️  transformer.h.0.mlp.c_proj.original_layer.weight: Frozen (2,359,296 params)
  ❄️  transformer.h.0.mlp.c_proj.original_layer.bias: Frozen (768 params)
  ✅ transformer.h.1.ln_1.weight: Trainable (768 params)
  ✅ transformer.h.1.ln_1.bias: Trainable (768 params)
  ✅ transformer.h.1.attn.c_attn.lora_A: Trainable (3,072 params)
  ✅ transformer.h.1.attn.c_attn.lora_B: Trainable (9,216 params)
  ❄️  transformer.h.1.attn.c_attn.original_layer.weight: Frozen (1,769,472 params)
  ❄️  transformer.h.1.attn.c_attn.original_layer.bias: Frozen (2,304 params)
  ✅ transformer.h.1.attn.c_proj.lora_A: Trainable (3,072 params)
  ✅ transformer.h.1.attn.c_proj.lora_B: Trainable (3,072 params)
  ❄️  transformer.h.1.attn.c_proj.original_layer.weight: Frozen (589,824 params)
  ❄️  transformer.h.1.attn.c_proj.original_layer.bias: Frozen (768 params)
  ✅ transformer.h.1.ln_2.weight: Trainable (768 params)
  ✅ transformer.h.1.ln_2.bias: Trainable (768 params)
  ✅ transformer.h.1.mlp.c_fc.lora_A: Trainable (3,072 params)
  ✅ transformer.h.1.mlp.c_fc.lora_B: Trainable (12,288 params)
  ❄️  transformer.h.1.mlp.c_fc.original_layer.weight: Frozen (2,359,296 params)
  ❄️  transformer.h.1.mlp.c_fc.original_layer.bias: Frozen (3,072 params)
  ✅ transformer.h.1.mlp.c_proj.lora_A: Trainable (12,288 params)
  ✅ transformer.h.1.mlp.c_proj.lora_B: Trainable (3,072 params)
  ❄️  transformer.h.1.mlp.c_proj.original_layer.weight: Frozen (2,359,296 params)
  ❄️  transformer.h.1.mlp.c_proj.original_layer.bias: Frozen (768 params)
  ✅ transformer.h.2.ln_1.weight: Trainable (768 params)
  ✅ transformer.h.2.ln_1.bias: Trainable (768 params)
  ✅ transformer.h.2.attn.c_attn.lora_A: Trainable (3,072 params)
  ✅ transformer.h.2.attn.c_attn.lora_B: Trainable (9,216 params)
  ❄️  transformer.h.2.attn.c_attn.original_layer.weight: Frozen (1,769,472 params)
  ❄️  transformer.h.2.attn.c_attn.original_layer.bias: Frozen (2,304 params)
  ✅ transformer.h.2.attn.c_proj.lora_A: Trainable (3,072 params)
  ✅ transformer.h.2.attn.c_proj.lora_B: Trainable (3,072 params)
  ❄️  transformer.h.2.attn.c_proj.original_layer.weight: Frozen (589,824 params)
  ❄️  transformer.h.2.attn.c_proj.original_layer.bias: Frozen (768 params)
  ✅ transformer.h.2.ln_2.weight: Trainable (768 params)
  ✅ transformer.h.2.ln_2.bias: Trainable (768 params)
  ✅ transformer.h.2.mlp.c_fc.lora_A: Trainable (3,072 params)
  ✅ transformer.h.2.mlp.c_fc.lora_B: Trainable (12,288 params)
  ❄️  transformer.h.2.mlp.c_fc.original_layer.weight: Frozen (2,359,296 params)
  ❄️  transformer.h.2.mlp.c_fc.original_layer.bias: Frozen (3,072 params)
  ✅ transformer.h.2.mlp.c_proj.lora_A: Trainable (12,288 params)
  ✅ transformer.h.2.mlp.c_proj.lora_B: Trainable (3,072 params)
  ❄️  transformer.h.2.mlp.c_proj.original_layer.weight: Frozen (2,359,296 params)
  ❄️  transformer.h.2.mlp.c_proj.original_layer.bias: Frozen (768 params)
  ✅ transformer.h.3.ln_1.weight: Trainable (768 params)
  ✅ transformer.h.3.ln_1.bias: Trainable (768 params)
  ✅ transformer.h.3.attn.c_attn.lora_A: Trainable (3,072 params)
  ✅ transformer.h.3.attn.c_attn.lora_B: Trainable (9,216 params)
  ❄️  transformer.h.3.attn.c_attn.original_layer.weight: Frozen (1,769,472 params)
  ❄️  transformer.h.3.attn.c_attn.original_layer.bias: Frozen (2,304 params)
  ✅ transformer.h.3.attn.c_proj.lora_A: Trainable (3,072 params)
  ✅ transformer.h.3.attn.c_proj.lora_B: Trainable (3,072 params)
  ❄️  transformer.h.3.attn.c_proj.original_layer.weight: Frozen (589,824 params)
  ❄️  transformer.h.3.attn.c_proj.original_layer.bias: Frozen (768 params)
  ✅ transformer.h.3.ln_2.weight: Trainable (768 params)
  ✅ transformer.h.3.ln_2.bias: Trainable (768 params)
  ✅ transformer.h.3.mlp.c_fc.lora_A: Trainable (3,072 params)
  ✅ transformer.h.3.mlp.c_fc.lora_B: Trainable (12,288 params)
  ❄️  transformer.h.3.mlp.c_fc.original_layer.weight: Frozen (2,359,296 params)
  ❄️  transformer.h.3.mlp.c_fc.original_layer.bias: Frozen (3,072 params)
  ✅ transformer.h.3.mlp.c_proj.lora_A: Trainable (12,288 params)
  ✅ transformer.h.3.mlp.c_proj.lora_B: Trainable (3,072 params)
  ❄️  transformer.h.3.mlp.c_proj.original_layer.weight: Frozen (2,359,296 params)
  ❄️  transformer.h.3.mlp.c_proj.original_layer.bias: Frozen (768 params)
  ✅ transformer.h.4.ln_1.weight: Trainable (768 params)
  ✅ transformer.h.4.ln_1.bias: Trainable (768 params)
  ✅ transformer.h.4.attn.c_attn.lora_A: Trainable (3,072 params)
  ✅ transformer.h.4.attn.c_attn.lora_B: Trainable (9,216 params)
  ❄️  transformer.h.4.attn.c_attn.original_layer.weight: Frozen (1,769,472 params)
  ❄️  transformer.h.4.attn.c_attn.original_layer.bias: Frozen (2,304 params)
  ✅ transformer.h.4.attn.c_proj.lora_A: Trainable (3,072 params)
  ✅ transformer.h.4.attn.c_proj.lora_B: Trainable (3,072 params)
  ❄️  transformer.h.4.attn.c_proj.original_layer.weight: Frozen (589,824 params)
  ❄️  transformer.h.4.attn.c_proj.original_layer.bias: Frozen (768 params)
  ✅ transformer.h.4.ln_2.weight: Trainable (768 params)
  ✅ transformer.h.4.ln_2.bias: Trainable (768 params)
  ✅ transformer.h.4.mlp.c_fc.lora_A: Trainable (3,072 params)
  ✅ transformer.h.4.mlp.c_fc.lora_B: Trainable (12,288 params)
  ❄️  transformer.h.4.mlp.c_fc.original_layer.weight: Frozen (2,359,296 params)
  ❄️  transformer.h.4.mlp.c_fc.original_layer.bias: Frozen (3,072 params)
  ✅ transformer.h.4.mlp.c_proj.lora_A: Trainable (12,288 params)
  ✅ transformer.h.4.mlp.c_proj.lora_B: Trainable (3,072 params)
  ❄️  transformer.h.4.mlp.c_proj.original_layer.weight: Frozen (2,359,296 params)
  ❄️  transformer.h.4.mlp.c_proj.original_layer.bias: Frozen (768 params)
  ✅ transformer.h.5.ln_1.weight: Trainable (768 params)
  ✅ transformer.h.5.ln_1.bias: Trainable (768 params)
  ✅ transformer.h.5.attn.c_attn.lora_A: Trainable (3,072 params)
  ✅ transformer.h.5.attn.c_attn.lora_B: Trainable (9,216 params)
  ❄️  transformer.h.5.attn.c_attn.original_layer.weight: Frozen (1,769,472 params)
  ❄️  transformer.h.5.attn.c_attn.original_layer.bias: Frozen (2,304 params)
  ✅ transformer.h.5.attn.c_proj.lora_A: Trainable (3,072 params)
  ✅ transformer.h.5.attn.c_proj.lora_B: Trainable (3,072 params)
  ❄️  transformer.h.5.attn.c_proj.original_layer.weight: Frozen (589,824 params)
  ❄️  transformer.h.5.attn.c_proj.original_layer.bias: Frozen (768 params)
  ✅ transformer.h.5.ln_2.weight: Trainable (768 params)
  ✅ transformer.h.5.ln_2.bias: Trainable (768 params)
  ✅ transformer.h.5.mlp.c_fc.lora_A: Trainable (3,072 params)
  ✅ transformer.h.5.mlp.c_fc.lora_B: Trainable (12,288 params)
  ❄️  transformer.h.5.mlp.c_fc.original_layer.weight: Frozen (2,359,296 params)
  ❄️  transformer.h.5.mlp.c_fc.original_layer.bias: Frozen (3,072 params)
  ✅ transformer.h.5.mlp.c_proj.lora_A: Trainable (12,288 params)
  ✅ transformer.h.5.mlp.c_proj.lora_B: Trainable (3,072 params)
  ❄️  transformer.h.5.mlp.c_proj.original_layer.weight: Frozen (2,359,296 params)
  ❄️  transformer.h.5.mlp.c_proj.original_layer.bias: Frozen (768 params)
  ✅ transformer.h.6.ln_1.weight: Trainable (768 params)
  ✅ transformer.h.6.ln_1.bias: Trainable (768 params)
  ✅ transformer.h.6.attn.c_attn.lora_A: Trainable (3,072 params)
  ✅ transformer.h.6.attn.c_attn.lora_B: Trainable (9,216 params)
  ❄️  transformer.h.6.attn.c_attn.original_layer.weight: Frozen (1,769,472 params)
  ❄️  transformer.h.6.attn.c_attn.original_layer.bias: Frozen (2,304 params)
  ✅ transformer.h.6.attn.c_proj.lora_A: Trainable (3,072 params)
  ✅ transformer.h.6.attn.c_proj.lora_B: Trainable (3,072 params)
  ❄️  transformer.h.6.attn.c_proj.original_layer.weight: Frozen (589,824 params)
  ❄️  transformer.h.6.attn.c_proj.original_layer.bias: Frozen (768 params)
  ✅ transformer.h.6.ln_2.weight: Trainable (768 params)
  ✅ transformer.h.6.ln_2.bias: Trainable (768 params)
  ✅ transformer.h.6.mlp.c_fc.lora_A: Trainable (3,072 params)
  ✅ transformer.h.6.mlp.c_fc.lora_B: Trainable (12,288 params)
  ❄️  transformer.h.6.mlp.c_fc.original_layer.weight: Frozen (2,359,296 params)
  ❄️  transformer.h.6.mlp.c_fc.original_layer.bias: Frozen (3,072 params)
  ✅ transformer.h.6.mlp.c_proj.lora_A: Trainable (12,288 params)
  ✅ transformer.h.6.mlp.c_proj.lora_B: Trainable (3,072 params)
  ❄️  transformer.h.6.mlp.c_proj.original_layer.weight: Frozen (2,359,296 params)
  ❄️  transformer.h.6.mlp.c_proj.original_layer.bias: Frozen (768 params)
  ✅ transformer.h.7.ln_1.weight: Trainable (768 params)
  ✅ transformer.h.7.ln_1.bias: Trainable (768 params)
  ✅ transformer.h.7.attn.c_attn.lora_A: Trainable (3,072 params)
  ✅ transformer.h.7.attn.c_attn.lora_B: Trainable (9,216 params)
  ❄️  transformer.h.7.attn.c_attn.original_layer.weight: Frozen (1,769,472 params)
  ❄️  transformer.h.7.attn.c_attn.original_layer.bias: Frozen (2,304 params)
  ✅ transformer.h.7.attn.c_proj.lora_A: Trainable (3,072 params)
  ✅ transformer.h.7.attn.c_proj.lora_B: Trainable (3,072 params)
  ❄️  transformer.h.7.attn.c_proj.original_layer.weight: Frozen (589,824 params)
  ❄️  transformer.h.7.attn.c_proj.original_layer.bias: Frozen (768 params)
  ✅ transformer.h.7.ln_2.weight: Trainable (768 params)
  ✅ transformer.h.7.ln_2.bias: Trainable (768 params)
  ✅ transformer.h.7.mlp.c_fc.lora_A: Trainable (3,072 params)
  ✅ transformer.h.7.mlp.c_fc.lora_B: Trainable (12,288 params)
  ❄️  transformer.h.7.mlp.c_fc.original_layer.weight: Frozen (2,359,296 params)
  ❄️  transformer.h.7.mlp.c_fc.original_layer.bias: Frozen (3,072 params)
  ✅ transformer.h.7.mlp.c_proj.lora_A: Trainable (12,288 params)
  ✅ transformer.h.7.mlp.c_proj.lora_B: Trainable (3,072 params)
  ❄️  transformer.h.7.mlp.c_proj.original_layer.weight: Frozen (2,359,296 params)
  ❄️  transformer.h.7.mlp.c_proj.original_layer.bias: Frozen (768 params)
  ✅ transformer.h.8.ln_1.weight: Trainable (768 params)
  ✅ transformer.h.8.ln_1.bias: Trainable (768 params)
  ✅ transformer.h.8.attn.c_attn.lora_A: Trainable (3,072 params)
  ✅ transformer.h.8.attn.c_attn.lora_B: Trainable (9,216 params)
  ❄️  transformer.h.8.attn.c_attn.original_layer.weight: Frozen (1,769,472 params)
  ❄️  transformer.h.8.attn.c_attn.original_layer.bias: Frozen (2,304 params)
  ✅ transformer.h.8.attn.c_proj.lora_A: Trainable (3,072 params)
  ✅ transformer.h.8.attn.c_proj.lora_B: Trainable (3,072 params)
  ❄️  transformer.h.8.attn.c_proj.original_layer.weight: Frozen (589,824 params)
  ❄️  transformer.h.8.attn.c_proj.original_layer.bias: Frozen (768 params)
  ✅ transformer.h.8.ln_2.weight: Trainable (768 params)
  ✅ transformer.h.8.ln_2.bias: Trainable (768 params)
  ✅ transformer.h.8.mlp.c_fc.lora_A: Trainable (3,072 params)
  ✅ transformer.h.8.mlp.c_fc.lora_B: Trainable (12,288 params)
  ❄️  transformer.h.8.mlp.c_fc.original_layer.weight: Frozen (2,359,296 params)
  ❄️  transformer.h.8.mlp.c_fc.original_layer.bias: Frozen (3,072 params)
  ✅ transformer.h.8.mlp.c_proj.lora_A: Trainable (12,288 params)
  ✅ transformer.h.8.mlp.c_proj.lora_B: Trainable (3,072 params)
  ❄️  transformer.h.8.mlp.c_proj.original_layer.weight: Frozen (2,359,296 params)
  ❄️  transformer.h.8.mlp.c_proj.original_layer.bias: Frozen (768 params)
  ✅ transformer.h.9.ln_1.weight: Trainable (768 params)
  ✅ transformer.h.9.ln_1.bias: Trainable (768 params)
  ✅ transformer.h.9.attn.c_attn.lora_A: Trainable (3,072 params)
  ✅ transformer.h.9.attn.c_attn.lora_B: Trainable (9,216 params)
  ❄️  transformer.h.9.attn.c_attn.original_layer.weight: Frozen (1,769,472 params)INFO:__main__:📊 Model Parameters:
INFO:__main__:   Total: 125,029,632
INFO:__main__:   Trainable: 40,012,032
INFO:__main__:   Frozen: 85,017,600
INFO:__main__:   Trainable Ratio: 0.3200 (32.00%)
INFO:__main__:📁 Loading dataset using Task 1 pipeline...
INFO:root:Dataset loaded successfully with 52002 entries.
INFO:root:Dataset columns: ['instruction', 'input', 'output']
INFO:root:First few rows: 
                                         instruction  ...                                             output
0               Give three tips for staying healthy.  ...  1.Eat a balanced diet and make sure to include...
1                 What are the three primary colors?  ...  The three primary colors are red, blue, and ye...
2                 Describe the structure of an atom.  ...  An atom is made up of a nucleus, which contain...
3                   How can we reduce air pollution?  ...  There are a number of ways to reduce air pollu...
4  Describe a time when you had to make a difficu...  ...  I had to make a difficult decision when I was ...

[5 rows x 3 columns]
INFO:root:Preprocessing and tokenizing the dataset...
INFO:root:Tokenization complete with shape: torch.Size([52002, 512])
INFO:root:DataLoader created with batch size 8 and 4 workers.
INFO:__main__:📊 Using portion of training data for evaluation
INFO:__main__:🔢 Training batches: 6501
INFO:__main__:🔢 Evaluation batches: 6501
INFO:__main__:⚙️ Optimizer: AdamW
INFO:__main__:📈 Scheduler: SequentialLR
INFO:__main__:🎯 Total training steps: 13002
INFO:__main__:🎬 Starting training...
INFO:__main__:================================================================================

  ❄️  transformer.h.9.attn.c_attn.original_layer.bias: Frozen (2,304 params)
  ✅ transformer.h.9.attn.c_proj.lora_A: Trainable (3,072 params)
  ✅ transformer.h.9.attn.c_proj.lora_B: Trainable (3,072 params)
  ❄️  transformer.h.9.attn.c_proj.original_layer.weight: Frozen (589,824 params)
  ❄️  transformer.h.9.attn.c_proj.original_layer.bias: Frozen (768 params)
  ✅ transformer.h.9.ln_2.weight: Trainable (768 params)
  ✅ transformer.h.9.ln_2.bias: Trainable (768 params)
  ✅ transformer.h.9.mlp.c_fc.lora_A: Trainable (3,072 params)
  ✅ transformer.h.9.mlp.c_fc.lora_B: Trainable (12,288 params)
  ❄️  transformer.h.9.mlp.c_fc.original_layer.weight: Frozen (2,359,296 params)
  ❄️  transformer.h.9.mlp.c_fc.original_layer.bias: Frozen (3,072 params)
  ✅ transformer.h.9.mlp.c_proj.lora_A: Trainable (12,288 params)
  ✅ transformer.h.9.mlp.c_proj.lora_B: Trainable (3,072 params)
  ❄️  transformer.h.9.mlp.c_proj.original_layer.weight: Frozen (2,359,296 params)
  ❄️  transformer.h.9.mlp.c_proj.original_layer.bias: Frozen (768 params)
  ✅ transformer.h.10.ln_1.weight: Trainable (768 params)
  ✅ transformer.h.10.ln_1.bias: Trainable (768 params)
  ✅ transformer.h.10.attn.c_attn.lora_A: Trainable (3,072 params)
  ✅ transformer.h.10.attn.c_attn.lora_B: Trainable (9,216 params)
  ❄️  transformer.h.10.attn.c_attn.original_layer.weight: Frozen (1,769,472 params)
  ❄️  transformer.h.10.attn.c_attn.original_layer.bias: Frozen (2,304 params)
  ✅ transformer.h.10.attn.c_proj.lora_A: Trainable (3,072 params)
  ✅ transformer.h.10.attn.c_proj.lora_B: Trainable (3,072 params)
  ❄️  transformer.h.10.attn.c_proj.original_layer.weight: Frozen (589,824 params)
  ❄️  transformer.h.10.attn.c_proj.original_layer.bias: Frozen (768 params)
  ✅ transformer.h.10.ln_2.weight: Trainable (768 params)
  ✅ transformer.h.10.ln_2.bias: Trainable (768 params)
  ✅ transformer.h.10.mlp.c_fc.lora_A: Trainable (3,072 params)
  ✅ transformer.h.10.mlp.c_fc.lora_B: Trainable (12,288 params)
  ❄️  transformer.h.10.mlp.c_fc.original_layer.weight: Frozen (2,359,296 params)
  ❄️  transformer.h.10.mlp.c_fc.original_layer.bias: Frozen (3,072 params)
  ✅ transformer.h.10.mlp.c_proj.lora_A: Trainable (12,288 params)
  ✅ transformer.h.10.mlp.c_proj.lora_B: Trainable (3,072 params)
  ❄️  transformer.h.10.mlp.c_proj.original_layer.weight: Frozen (2,359,296 params)
  ❄️  transformer.h.10.mlp.c_proj.original_layer.bias: Frozen (768 params)
  ✅ transformer.h.11.ln_1.weight: Trainable (768 params)
  ✅ transformer.h.11.ln_1.bias: Trainable (768 params)
  ✅ transformer.h.11.attn.c_attn.lora_A: Trainable (3,072 params)
  ✅ transformer.h.11.attn.c_attn.lora_B: Trainable (9,216 params)
  ❄️  transformer.h.11.attn.c_attn.original_layer.weight: Frozen (1,769,472 params)
  ❄️  transformer.h.11.attn.c_attn.original_layer.bias: Frozen (2,304 params)
  ✅ transformer.h.11.attn.c_proj.lora_A: Trainable (3,072 params)
  ✅ transformer.h.11.attn.c_proj.lora_B: Trainable (3,072 params)
  ❄️  transformer.h.11.attn.c_proj.original_layer.weight: Frozen (589,824 params)
  ❄️  transformer.h.11.attn.c_proj.original_layer.bias: Frozen (768 params)
  ✅ transformer.h.11.ln_2.weight: Trainable (768 params)
  ✅ transformer.h.11.ln_2.bias: Trainable (768 params)
  ✅ transformer.h.11.mlp.c_fc.lora_A: Trainable (3,072 params)
  ✅ transformer.h.11.mlp.c_fc.lora_B: Trainable (12,288 params)
  ❄️  transformer.h.11.mlp.c_fc.original_layer.weight: Frozen (2,359,296 params)
  ❄️  transformer.h.11.mlp.c_fc.original_layer.bias: Frozen (3,072 params)
  ✅ transformer.h.11.mlp.c_proj.lora_A: Trainable (12,288 params)
  ✅ transformer.h.11.mlp.c_proj.lora_B: Trainable (3,072 params)
  ❄️  transformer.h.11.mlp.c_proj.original_layer.weight: Frozen (2,359,296 params)
  ❄️  transformer.h.11.mlp.c_proj.original_layer.bias: Frozen (768 params)
  ✅ transformer.ln_f.weight: Trainable (768 params)
  ✅ transformer.ln_f.bias: Trainable (768 params)

📊 Summary:
  Trainable: 40,012,032 / 125,029,632 params
  Ratio: 0.3200 (32.00%)

🎉 Model modification complete and ready for training!
`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.
INFO:__main__:Epoch 0 | Step    0/6501 | Loss: 8.9351 | PPL: 7594.22 | LR: 3.27e-05 | Grad: 20.697 | Time: 1.073s
INFO:__main__:Epoch 0 | Step   10/6501 | Loss: 8.2445 | PPL: 4455.51 | LR: 5.97e-05 | Grad: 25.399 | Time: 0.507s
INFO:__main__:Epoch 0 | Step   20/6501 | Loss: 6.6806 | PPL: 2484.50 | LR: 8.67e-05 | Grad: 21.756 | Time: 0.510s
INFO:__main__:Epoch 0 | Step   30/6501 | Loss: 5.0276 | PPL: 1684.87 | LR: 1.14e-04 | Grad: 1.029 | Time: 0.514s
INFO:__main__:Epoch 0 | Step   40/6501 | Loss: 4.1230 | PPL: 1274.87 | LR: 1.41e-04 | Grad: 3.884 | Time: 0.516s
INFO:__main__:Epoch 0 | Step   50/6501 | Loss: 3.4210 | PPL: 894.14 | LR: 1.68e-04 | Grad: 0.519 | Time: 0.518s
INFO:__main__:Epoch 0 | Step   60/6501 | Loss: 1.9511 | PPL: 66.31 | LR: 1.95e-04 | Grad: 0.457 | Time: 0.519s
INFO:__main__:Epoch 0 | Step   70/6501 | Loss: 1.1044 | PPL: 3.49 | LR: 2.22e-04 | Grad: 0.169 | Time: 0.523s
INFO:__main__:Epoch 0 | Step   80/6501 | Loss: 0.9558 | PPL: 2.83 | LR: 2.49e-04 | Grad: 0.390 | Time: 0.527s
INFO:__main__:Epoch 0 | Step   90/6501 | Loss: 0.8753 | PPL: 2.57 | LR: 2.76e-04 | Grad: 0.214 | Time: 0.531s
/home/Alok/miniconda3/envs/pytorch/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:209: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
INFO:__main__:Epoch 0 | Step  100/6501 | Loss: 0.8005 | PPL: 2.35 | LR: 3.00e-04 | Grad: 0.234 | Time: 0.528s
INFO:__main__:Epoch 0 | Step  110/6501 | Loss: 0.8164 | PPL: 2.36 | LR: 3.00e-04 | Grad: 0.328 | Time: 0.537s
INFO:__main__:Epoch 0 | Step  120/6501 | Loss: 0.8390 | PPL: 2.41 | LR: 3.00e-04 | Grad: 0.384 | Time: 0.530s
INFO:__main__:Epoch 0 | Step  130/6501 | Loss: 0.8469 | PPL: 2.44 | LR: 3.00e-04 | Grad: 0.527 | Time: 0.528s
INFO:__main__:Epoch 0 | Step  140/6501 | Loss: 0.8165 | PPL: 2.37 | LR: 3.00e-04 | Grad: 0.366 | Time: 0.523s
INFO:__main__:Epoch 0 | Step  150/6501 | Loss: 0.8382 | PPL: 2.43 | LR: 3.00e-04 | Grad: 0.350 | Time: 0.527s
INFO:__main__:Epoch 0 | Step  160/6501 | Loss: 0.8285 | PPL: 2.40 | LR: 3.00e-04 | Grad: 0.292 | Time: 0.521s
INFO:__main__:Epoch 0 | Step  170/6501 | Loss: 0.8522 | PPL: 2.44 | LR: 3.00e-04 | Grad: 0.352 | Time: 0.521s
INFO:__main__:Epoch 0 | Step  180/6501 | Loss: 0.8339 | PPL: 2.39 | LR: 3.00e-04 | Grad: 0.295 | Time: 0.520s
INFO:__main__:Epoch 0 | Step  190/6501 | Loss: 0.8613 | PPL: 2.46 | LR: 3.00e-04 | Grad: 0.287 | Time: 0.518s
INFO:__main__:Epoch 0 | Step  200/6501 | Loss: 0.8710 | PPL: 2.48 | LR: 3.00e-04 | Grad: 0.242 | Time: 0.517s
INFO:__main__:Epoch 0 | Step  210/6501 | Loss: 0.8414 | PPL: 2.41 | LR: 3.00e-04 | Grad: 0.418 | Time: 0.517s
INFO:__main__:Epoch 0 | Step  220/6501 | Loss: 0.8342 | PPL: 2.41 | LR: 3.00e-04 | Grad: 0.611 | Time: 0.516s
INFO:__main__:Epoch 0 | Step  230/6501 | Loss: 0.8333 | PPL: 2.42 | LR: 3.00e-04 | Grad: 0.192 | Time: 0.516s
INFO:__main__:Epoch 0 | Step  240/6501 | Loss: 0.7892 | PPL: 2.31 | LR: 3.00e-04 | Grad: 0.269 | Time: 0.520s
INFO:__main__:Epoch 0 | Step  250/6501 | Loss: 0.7639 | PPL: 2.26 | LR: 3.00e-04 | Grad: 0.368 | Time: 0.518s
INFO:__main__:Epoch 0 | Step  260/6501 | Loss: 0.7925 | PPL: 2.32 | LR: 3.00e-04 | Grad: 0.289 | Time: 0.522s
INFO:__main__:Epoch 0 | Step  270/6501 | Loss: 0.7604 | PPL: 2.21 | LR: 3.00e-04 | Grad: 0.684 | Time: 0.521s
INFO:__main__:Epoch 0 | Step  280/6501 | Loss: 0.7582 | PPL: 2.20 | LR: 3.00e-04 | Grad: 0.213 | Time: 0.520s
INFO:__main__:Epoch 0 | Step  290/6501 | Loss: 0.7991 | PPL: 2.32 | LR: 3.00e-04 | Grad: 0.348 | Time: 0.522s
INFO:__main__:Epoch 0 | Step  300/6501 | Loss: 0.7891 | PPL: 2.31 | LR: 3.00e-04 | Grad: 0.355 | Time: 0.521s
INFO:__main__:Epoch 0 | Step  310/6501 | Loss: 0.7406 | PPL: 2.21 | LR: 3.00e-04 | Grad: 0.234 | Time: 0.522s
INFO:__main__:Epoch 0 | Step  320/6501 | Loss: 0.7347 | PPL: 2.20 | LR: 3.00e-04 | Grad: 0.244 | Time: 0.524s
INFO:__main__:Epoch 0 | Step  330/6501 | Loss: 0.7326 | PPL: 2.19 | LR: 3.00e-04 | Grad: 0.237 | Time: 0.523s
INFO:__main__:Epoch 0 | Step  340/6501 | Loss: 0.6819 | PPL: 2.05 | LR: 3.00e-04 | Grad: 0.284 | Time: 0.524s
INFO:__main__:Epoch 0 | Step  350/6501 | Loss: 0.6810 | PPL: 2.02 | LR: 3.00e-04 | Grad: 0.188 | Time: 0.524s
INFO:__main__:Epoch 0 | Step  360/6501 | Loss: 0.6991 | PPL: 2.05 | LR: 3.00e-04 | Grad: 0.253 | Time: 0.524s
INFO:__main__:Epoch 0 | Step  370/6501 | Loss: 0.7000 | PPL: 2.05 | LR: 3.00e-04 | Grad: 0.235 | Time: 0.521s
INFO:__main__:Epoch 0 | Step  380/6501 | Loss: 0.6943 | PPL: 2.05 | LR: 3.00e-04 | Grad: 0.294 | Time: 0.521s
INFO:__main__:Epoch 0 | Step  390/6501 | Loss: 0.7180 | PPL: 2.10 | LR: 3.00e-04 | Grad: 0.326 | Time: 0.520s
INFO:__main__:Epoch 0 | Step  400/6501 | Loss: 0.6979 | PPL: 2.06 | LR: 3.00e-04 | Grad: 0.302 | Time: 0.516s
INFO:__main__:Epoch 0 | Step  410/6501 | Loss: 0.6707 | PPL: 2.01 | LR: 3.00e-04 | Grad: 0.314 | Time: 0.522s
INFO:__main__:Epoch 0 | Step  420/6501 | Loss: 0.6781 | PPL: 2.05 | LR: 3.00e-04 | Grad: 0.304 | Time: 0.518s
INFO:__main__:Epoch 0 | Step  430/6501 | Loss: 0.6705 | PPL: 2.03 | LR: 3.00e-04 | Grad: 0.222 | Time: 0.519s
INFO:__main__:Epoch 0 | Step  440/6501 | Loss: 0.6390 | PPL: 1.96 | LR: 2.99e-04 | Grad: 0.228 | Time: 0.520s
INFO:__main__:Epoch 0 | Step  450/6501 | Loss: 0.6706 | PPL: 2.04 | LR: 2.99e-04 | Grad: 0.257 | Time: 0.520s
INFO:__main__:Epoch 0 | Step  460/6501 | Loss: 0.7103 | PPL: 2.13 | LR: 2.99e-04 | Grad: 0.349 | Time: 0.521s
INFO:__main__:Epoch 0 | Step  470/6501 | Loss: 0.6886 | PPL: 2.07 | LR: 2.99e-04 | Grad: 0.251 | Time: 0.522s
INFO:__main__:Epoch 0 | Step  480/6501 | Loss: 0.7165 | PPL: 2.15 | LR: 2.99e-04 | Grad: 0.248 | Time: 0.519s
INFO:__main__:Epoch 0 | Step  490/6501 | Loss: 0.7631 | PPL: 2.29 | LR: 2.99e-04 | Grad: 0.228 | Time: 0.520s
INFO:__main__:Epoch 0 | Step  500/6501 | Loss: 0.7336 | PPL: 2.21 | LR: 2.99e-04 | Grad: 0.316 | Time: 0.523s
INFO:__main__:Epoch 0 | Step  510/6501 | Loss: 0.7142 | PPL: 2.16 | LR: 2.99e-04 | Grad: 0.397 | Time: 0.526s
INFO:__main__:Epoch 0 | Step  520/6501 | Loss: 0.7116 | PPL: 2.14 | LR: 2.99e-04 | Grad: 0.236 | Time: 0.519s
INFO:__main__:Epoch 0 | Step  530/6501 | Loss: 0.6716 | PPL: 2.04 | LR: 2.99e-04 | Grad: 0.298 | Time: 0.518s
INFO:__main__:Epoch 0 | Step  540/6501 | Loss: 0.6528 | PPL: 1.97 | LR: 2.99e-04 | Grad: 0.256 | Time: 0.520s
INFO:__main__:Epoch 0 | Step  550/6501 | Loss: 0.6437 | PPL: 1.96 | LR: 2.99e-04 | Grad: 0.377 | Time: 0.520s
INFO:__main__:Epoch 0 | Step  560/6501 | Loss: 0.6567 | PPL: 1.98 | LR: 2.99e-04 | Grad: 0.470 | Time: 0.522s
INFO:__main__:Epoch 0 | Step  570/6501 | Loss: 0.6450 | PPL: 1.96 | LR: 2.99e-04 | Grad: 0.187 | Time: 0.526s
INFO:__main__:Epoch 0 | Step  580/6501 | Loss: 0.6717 | PPL: 2.01 | LR: 2.99e-04 | Grad: 0.358 | Time: 0.527s
INFO:__main__:Epoch 0 | Step  590/6501 | Loss: 0.6538 | PPL: 1.97 | LR: 2.99e-04 | Grad: 0.176 | Time: 0.524s
INFO:__main__:Epoch 0 | Step  600/6501 | Loss: 0.7059 | PPL: 2.08 | LR: 2.99e-04 | Grad: 0.313 | Time: 0.521s
INFO:__main__:Epoch 0 | Step  610/6501 | Loss: 0.7031 | PPL: 2.08 | LR: 2.99e-04 | Grad: 0.312 | Time: 0.520s
INFO:__main__:Epoch 0 | Step  620/6501 | Loss: 0.7231 | PPL: 2.11 | LR: 2.99e-04 | Grad: 0.280 | Time: 0.519s
INFO:__main__:Epoch 0 | Step  630/6501 | Loss: 0.7378 | PPL: 2.16 | LR: 2.99e-04 | Grad: 0.261 | Time: 0.518s
INFO:__main__:Epoch 0 | Step  640/6501 | Loss: 0.7686 | PPL: 2.24 | LR: 2.99e-04 | Grad: 0.454 | Time: 0.519s
INFO:__main__:Epoch 0 | Step  650/6501 | Loss: 0.7280 | PPL: 2.15 | LR: 2.99e-04 | Grad: 0.304 | Time: 0.519s
INFO:__main__:Epoch 0 | Step  660/6501 | Loss: 0.7523 | PPL: 2.20 | LR: 2.99e-04 | Grad: 0.337 | Time: 0.521s
INFO:__main__:Epoch 0 | Step  670/6501 | Loss: 0.7367 | PPL: 2.17 | LR: 2.99e-04 | Grad: 0.237 | Time: 0.518s
INFO:__main__:Epoch 0 | Step  680/6501 | Loss: 0.7218 | PPL: 2.12 | LR: 2.99e-04 | Grad: 0.333 | Time: 0.522s
INFO:__main__:Epoch 0 | Step  690/6501 | Loss: 0.7093 | PPL: 2.10 | LR: 2.98e-04 | Grad: 0.228 | Time: 0.522s
INFO:__main__:Epoch 0 | Step  700/6501 | Loss: 0.7166 | PPL: 2.11 | LR: 2.98e-04 | Grad: 0.306 | Time: 0.519s
INFO:__main__:Epoch 0 | Step  710/6501 | Loss: 0.7020 | PPL: 2.08 | LR: 2.98e-04 | Grad: 0.310 | Time: 0.522s
INFO:__main__:Epoch 0 | Step  720/6501 | Loss: 0.7187 | PPL: 2.11 | LR: 2.98e-04 | Grad: 0.349 | Time: 0.523s
INFO:__main__:Epoch 0 | Step  730/6501 | Loss: 0.7185 | PPL: 2.11 | LR: 2.98e-04 | Grad: 0.393 | Time: 0.524s
INFO:__main__:Epoch 0 | Step  740/6501 | Loss: 0.7059 | PPL: 2.08 | LR: 2.98e-04 | Grad: 0.212 | Time: 0.521s
INFO:__main__:Epoch 0 | Step  750/6501 | Loss: 0.6963 | PPL: 2.07 | LR: 2.98e-04 | Grad: 0.405 | Time: 0.518s
INFO:__main__:Epoch 0 | Step  760/6501 | Loss: 0.6699 | PPL: 2.01 | LR: 2.98e-04 | Grad: 0.371 | Time: 0.529s
INFO:__main__:Epoch 0 | Step  770/6501 | Loss: 0.6378 | PPL: 1.94 | LR: 2.98e-04 | Grad: 0.219 | Time: 0.523s
INFO:__main__:Epoch 0 | Step  780/6501 | Loss: 0.6128 | PPL: 1.90 | LR: 2.98e-04 | Grad: 0.305 | Time: 0.523s
INFO:__main__:Epoch 0 | Step  790/6501 | Loss: 0.6324 | PPL: 1.94 | LR: 2.98e-04 | Grad: 0.390 | Time: 0.522s
INFO:__main__:Epoch 0 | Step  800/6501 | Loss: 0.6445 | PPL: 1.97 | LR: 2.98e-04 | Grad: 0.387 | Time: 0.524s
INFO:__main__:Epoch 0 | Step  810/6501 | Loss: 0.6365 | PPL: 1.95 | LR: 2.98e-04 | Grad: 0.314 | Time: 0.525s
INFO:__main__:Epoch 0 | Step  820/6501 | Loss: 0.6632 | PPL: 2.01 | LR: 2.98e-04 | Grad: 0.247 | Time: 0.522s
INFO:__main__:Epoch 0 | Step  830/6501 | Loss: 0.6829 | PPL: 2.05 | LR: 2.98e-04 | Grad: 0.365 | Time: 0.521s
INFO:__main__:Epoch 0 | Step  840/6501 | Loss: 0.6519 | PPL: 1.97 | LR: 2.98e-04 | Grad: 0.278 | Time: 0.522s
INFO:__main__:Epoch 0 | Step  850/6501 | Loss: 0.6474 | PPL: 1.95 | LR: 2.98e-04 | Grad: 0.339 | Time: 0.524s
INFO:__main__:Epoch 0 | Step  860/6501 | Loss: 0.6563 | PPL: 1.97 | LR: 2.97e-04 | Grad: 0.315 | Time: 0.523s
INFO:__main__:Epoch 0 | Step  870/6501 | Loss: 0.6427 | PPL: 1.94 | LR: 2.97e-04 | Grad: 0.251 | Time: 0.519s
INFO:__main__:Epoch 0 | Step  880/6501 | Loss: 0.6272 | PPL: 1.90 | LR: 2.97e-04 | Grad: 0.194 | Time: 0.521s
INFO:__main__:Epoch 0 | Step  890/6501 | Loss: 0.6309 | PPL: 1.92 | LR: 2.97e-04 | Grad: 0.525 | Time: 0.522s
INFO:__main__:Epoch 0 | Step  900/6501 | Loss: 0.6363 | PPL: 1.95 | LR: 2.97e-04 | Grad: 0.257 | Time: 0.522s
INFO:__main__:Epoch 0 | Step  910/6501 | Loss: 0.6462 | PPL: 1.98 | LR: 2.97e-04 | Grad: 0.386 | Time: 0.522s
INFO:__main__:Epoch 0 | Step  920/6501 | Loss: 0.6587 | PPL: 2.01 | LR: 2.97e-04 | Grad: 0.256 | Time: 0.521s
INFO:__main__:Epoch 0 | Step  930/6501 | Loss: 0.6512 | PPL: 1.99 | LR: 2.97e-04 | Grad: 0.418 | Time: 0.523s
INFO:__main__:Epoch 0 | Step  940/6501 | Loss: 0.6425 | PPL: 1.96 | LR: 2.97e-04 | Grad: 0.251 | Time: 0.522s
INFO:__main__:Epoch 0 | Step  950/6501 | Loss: 0.6339 | PPL: 1.93 | LR: 2.97e-04 | Grad: 0.293 | Time: 0.522s
INFO:__main__:Epoch 0 | Step  960/6501 | Loss: 0.6495 | PPL: 1.96 | LR: 2.97e-04 | Grad: 0.462 | Time: 0.526s
INFO:__main__:Epoch 0 | Step  970/6501 | Loss: 0.6402 | PPL: 1.94 | LR: 2.97e-04 | Grad: 0.297 | Time: 0.526s
INFO:__main__:Epoch 0 | Step  980/6501 | Loss: 0.6699 | PPL: 2.03 | LR: 2.97e-04 | Grad: 0.333 | Time: 0.523s
INFO:__main__:Epoch 0 | Step  990/6501 | Loss: 0.6734 | PPL: 2.04 | LR: 2.96e-04 | Grad: 0.383 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 1000/6501 | Loss: 0.6941 | PPL: 2.08 | LR: 2.96e-04 | Grad: 0.274 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 1010/6501 | Loss: 0.6801 | PPL: 2.05 | LR: 2.96e-04 | Grad: 0.243 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 1020/6501 | Loss: 0.6896 | PPL: 2.06 | LR: 2.96e-04 | Grad: 0.228 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 1030/6501 | Loss: 0.6815 | PPL: 2.02 | LR: 2.96e-04 | Grad: 0.339 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 1040/6501 | Loss: 0.7034 | PPL: 2.07 | LR: 2.96e-04 | Grad: 0.422 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 1050/6501 | Loss: 0.6801 | PPL: 2.02 | LR: 2.96e-04 | Grad: 0.280 | Time: 0.526s
INFO:__main__:Epoch 0 | Step 1060/6501 | Loss: 0.6647 | PPL: 1.99 | LR: 2.96e-04 | Grad: 0.208 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 1070/6501 | Loss: 0.6829 | PPL: 2.04 | LR: 2.96e-04 | Grad: 0.747 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 1080/6501 | Loss: 0.6596 | PPL: 1.99 | LR: 2.96e-04 | Grad: 0.248 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 1090/6501 | Loss: 0.6317 | PPL: 1.93 | LR: 2.96e-04 | Grad: 0.306 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 1100/6501 | Loss: 0.6451 | PPL: 1.97 | LR: 2.96e-04 | Grad: 0.239 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 1110/6501 | Loss: 0.6564 | PPL: 1.99 | LR: 2.95e-04 | Grad: 0.387 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 1120/6501 | Loss: 0.6400 | PPL: 1.95 | LR: 2.95e-04 | Grad: 0.281 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 1130/6501 | Loss: 0.6372 | PPL: 1.95 | LR: 2.95e-04 | Grad: 0.359 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 1140/6501 | Loss: 0.6705 | PPL: 2.02 | LR: 2.95e-04 | Grad: 0.237 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 1150/6501 | Loss: 0.6780 | PPL: 2.02 | LR: 2.95e-04 | Grad: 0.358 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 1160/6501 | Loss: 0.6757 | PPL: 2.02 | LR: 2.95e-04 | Grad: 0.286 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 1170/6501 | Loss: 0.6632 | PPL: 1.99 | LR: 2.95e-04 | Grad: 0.325 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 1180/6501 | Loss: 0.6729 | PPL: 2.01 | LR: 2.95e-04 | Grad: 0.275 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 1190/6501 | Loss: 0.6495 | PPL: 1.95 | LR: 2.95e-04 | Grad: 0.295 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 1200/6501 | Loss: 0.6452 | PPL: 1.95 | LR: 2.95e-04 | Grad: 0.232 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 1210/6501 | Loss: 0.6626 | PPL: 2.00 | LR: 2.95e-04 | Grad: 0.191 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 1220/6501 | Loss: 0.6682 | PPL: 2.01 | LR: 2.94e-04 | Grad: 0.380 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 1230/6501 | Loss: 0.6851 | PPL: 2.05 | LR: 2.94e-04 | Grad: 0.728 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 1240/6501 | Loss: 0.6848 | PPL: 2.05 | LR: 2.94e-04 | Grad: 0.393 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 1250/6501 | Loss: 0.6744 | PPL: 2.02 | LR: 2.94e-04 | Grad: 0.297 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 1260/6501 | Loss: 0.6766 | PPL: 2.02 | LR: 2.94e-04 | Grad: 0.309 | Time: 0.518s
INFO:__main__:Epoch 0 | Step 1270/6501 | Loss: 0.6897 | PPL: 2.05 | LR: 2.94e-04 | Grad: 0.274 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 1280/6501 | Loss: 0.6796 | PPL: 2.02 | LR: 2.94e-04 | Grad: 0.303 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 1290/6501 | Loss: 0.6692 | PPL: 2.00 | LR: 2.94e-04 | Grad: 0.254 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 1300/6501 | Loss: 0.6639 | PPL: 1.99 | LR: 2.94e-04 | Grad: 0.342 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 1310/6501 | Loss: 0.6550 | PPL: 1.97 | LR: 2.94e-04 | Grad: 0.382 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 1320/6501 | Loss: 0.6328 | PPL: 1.93 | LR: 2.93e-04 | Grad: 0.343 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 1330/6501 | Loss: 0.6180 | PPL: 1.90 | LR: 2.93e-04 | Grad: 0.336 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 1340/6501 | Loss: 0.6227 | PPL: 1.91 | LR: 2.93e-04 | Grad: 0.359 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 1350/6501 | Loss: 0.6249 | PPL: 1.92 | LR: 2.93e-04 | Grad: 0.334 | Time: 0.527s
INFO:__main__:Epoch 0 | Step 1360/6501 | Loss: 0.6063 | PPL: 1.87 | LR: 2.93e-04 | Grad: 0.331 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 1370/6501 | Loss: 0.6079 | PPL: 1.87 | LR: 2.93e-04 | Grad: 0.258 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 1380/6501 | Loss: 0.6261 | PPL: 1.91 | LR: 2.93e-04 | Grad: 0.512 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 1390/6501 | Loss: 0.6428 | PPL: 1.94 | LR: 2.93e-04 | Grad: 0.364 | Time: 0.527s
INFO:__main__:Epoch 0 | Step 1400/6501 | Loss: 0.6275 | PPL: 1.91 | LR: 2.93e-04 | Grad: 0.302 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 1410/6501 | Loss: 0.6033 | PPL: 1.87 | LR: 2.92e-04 | Grad: 0.359 | Time: 0.518s
INFO:__main__:Epoch 0 | Step 1420/6501 | Loss: 0.6032 | PPL: 1.87 | LR: 2.92e-04 | Grad: 0.224 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 1430/6501 | Loss: 0.5989 | PPL: 1.86 | LR: 2.92e-04 | Grad: 0.493 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 1440/6501 | Loss: 0.6003 | PPL: 1.86 | LR: 2.92e-04 | Grad: 0.332 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 1450/6501 | Loss: 0.5916 | PPL: 1.84 | LR: 2.92e-04 | Grad: 0.259 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 1460/6501 | Loss: 0.6127 | PPL: 1.88 | LR: 2.92e-04 | Grad: 0.311 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 1470/6501 | Loss: 0.6270 | PPL: 1.90 | LR: 2.92e-04 | Grad: 0.372 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 1480/6501 | Loss: 0.6174 | PPL: 1.90 | LR: 2.92e-04 | Grad: 0.215 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 1490/6501 | Loss: 0.5857 | PPL: 1.83 | LR: 2.92e-04 | Grad: 0.368 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 1500/6501 | Loss: 0.6257 | PPL: 1.91 | LR: 2.91e-04 | Grad: 0.315 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 1510/6501 | Loss: 0.6199 | PPL: 1.91 | LR: 2.91e-04 | Grad: 0.387 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 1520/6501 | Loss: 0.5953 | PPL: 1.87 | LR: 2.91e-04 | Grad: 0.455 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 1530/6501 | Loss: 0.6321 | PPL: 1.94 | LR: 2.91e-04 | Grad: 0.426 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 1540/6501 | Loss: 0.6612 | PPL: 2.00 | LR: 2.91e-04 | Grad: 0.223 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 1550/6501 | Loss: 0.6427 | PPL: 1.97 | LR: 2.91e-04 | Grad: 0.232 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 1560/6501 | Loss: 0.6656 | PPL: 2.01 | LR: 2.91e-04 | Grad: 0.351 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 1570/6501 | Loss: 0.6824 | PPL: 2.03 | LR: 2.91e-04 | Grad: 0.283 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 1580/6501 | Loss: 0.6362 | PPL: 1.94 | LR: 2.90e-04 | Grad: 0.352 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 1590/6501 | Loss: 0.6038 | PPL: 1.87 | LR: 2.90e-04 | Grad: 0.370 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 1600/6501 | Loss: 0.5948 | PPL: 1.86 | LR: 2.90e-04 | Grad: 0.289 | Time: 0.518s
INFO:__main__:Epoch 0 | Step 1610/6501 | Loss: 0.5675 | PPL: 1.81 | LR: 2.90e-04 | Grad: 0.171 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 1620/6501 | Loss: 0.5666 | PPL: 1.81 | LR: 2.90e-04 | Grad: 0.287 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 1630/6501 | Loss: 0.5791 | PPL: 1.83 | LR: 2.90e-04 | Grad: 0.338 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 1640/6501 | Loss: 0.6018 | PPL: 1.88 | LR: 2.90e-04 | Grad: 0.268 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 1650/6501 | Loss: 0.6140 | PPL: 1.92 | LR: 2.89e-04 | Grad: 0.280 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 1660/6501 | Loss: 0.6120 | PPL: 1.92 | LR: 2.89e-04 | Grad: 0.335 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 1670/6501 | Loss: 0.6170 | PPL: 1.92 | LR: 2.89e-04 | Grad: 0.442 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 1680/6501 | Loss: 0.5949 | PPL: 1.88 | LR: 2.89e-04 | Grad: 0.175 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 1690/6501 | Loss: 0.5804 | PPL: 1.85 | LR: 2.89e-04 | Grad: 0.201 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 1700/6501 | Loss: 0.5440 | PPL: 1.75 | LR: 2.89e-04 | Grad: 0.269 | Time: 0.518s
INFO:__main__:Epoch 0 | Step 1710/6501 | Loss: 0.5645 | PPL: 1.79 | LR: 2.89e-04 | Grad: 0.225 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 1720/6501 | Loss: 0.5382 | PPL: 1.74 | LR: 2.89e-04 | Grad: 0.270 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 1730/6501 | Loss: 0.5303 | PPL: 1.73 | LR: 2.88e-04 | Grad: 0.345 | Time: 0.518s
INFO:__main__:Epoch 0 | Step 1740/6501 | Loss: 0.5485 | PPL: 1.76 | LR: 2.88e-04 | Grad: 0.272 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 1750/6501 | Loss: 0.5794 | PPL: 1.82 | LR: 2.88e-04 | Grad: 0.264 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 1760/6501 | Loss: 0.5742 | PPL: 1.81 | LR: 2.88e-04 | Grad: 0.295 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 1770/6501 | Loss: 0.5949 | PPL: 1.84 | LR: 2.88e-04 | Grad: 0.398 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 1780/6501 | Loss: 0.6096 | PPL: 1.86 | LR: 2.88e-04 | Grad: 0.310 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 1790/6501 | Loss: 0.6059 | PPL: 1.86 | LR: 2.88e-04 | Grad: 0.296 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 1800/6501 | Loss: 0.6184 | PPL: 1.88 | LR: 2.87e-04 | Grad: 0.515 | Time: 0.526s
INFO:__main__:Epoch 0 | Step 1810/6501 | Loss: 0.6553 | PPL: 1.97 | LR: 2.87e-04 | Grad: 0.301 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 1820/6501 | Loss: 0.6337 | PPL: 1.93 | LR: 2.87e-04 | Grad: 0.196 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 1830/6501 | Loss: 0.6187 | PPL: 1.90 | LR: 2.87e-04 | Grad: 0.275 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 1840/6501 | Loss: 0.6174 | PPL: 1.90 | LR: 2.87e-04 | Grad: 0.316 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 1850/6501 | Loss: 0.5919 | PPL: 1.85 | LR: 2.87e-04 | Grad: 0.280 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 1860/6501 | Loss: 0.5541 | PPL: 1.78 | LR: 2.86e-04 | Grad: 0.307 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 1870/6501 | Loss: 0.5758 | PPL: 1.83 | LR: 2.86e-04 | Grad: 0.340 | Time: 0.518s
INFO:__main__:Epoch 0 | Step 1880/6501 | Loss: 0.5921 | PPL: 1.85 | LR: 2.86e-04 | Grad: 0.274 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 1890/6501 | Loss: 0.5855 | PPL: 1.84 | LR: 2.86e-04 | Grad: 0.223 | Time: 0.518s
INFO:__main__:Epoch 0 | Step 1900/6501 | Loss: 0.5900 | PPL: 1.84 | LR: 2.86e-04 | Grad: 0.225 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 1910/6501 | Loss: 0.5830 | PPL: 1.83 | LR: 2.86e-04 | Grad: 0.323 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 1920/6501 | Loss: 0.5683 | PPL: 1.80 | LR: 2.86e-04 | Grad: 0.259 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 1930/6501 | Loss: 0.5996 | PPL: 1.86 | LR: 2.85e-04 | Grad: 0.435 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 1940/6501 | Loss: 0.5891 | PPL: 1.84 | LR: 2.85e-04 | Grad: 0.206 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 1950/6501 | Loss: 0.6109 | PPL: 1.89 | LR: 2.85e-04 | Grad: 0.284 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 1960/6501 | Loss: 0.6012 | PPL: 1.87 | LR: 2.85e-04 | Grad: 0.251 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 1970/6501 | Loss: 0.5983 | PPL: 1.86 | LR: 2.85e-04 | Grad: 0.372 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 1980/6501 | Loss: 0.5789 | PPL: 1.82 | LR: 2.85e-04 | Grad: 0.405 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 1990/6501 | Loss: 0.5797 | PPL: 1.83 | LR: 2.84e-04 | Grad: 0.199 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 2000/6501 | Loss: 0.5665 | PPL: 1.79 | LR: 2.84e-04 | Grad: 0.370 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 2010/6501 | Loss: 0.5922 | PPL: 1.84 | LR: 2.84e-04 | Grad: 0.317 | Time: 0.518s
INFO:__main__:Epoch 0 | Step 2020/6501 | Loss: 0.6309 | PPL: 1.92 | LR: 2.84e-04 | Grad: 0.416 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 2030/6501 | Loss: 0.6232 | PPL: 1.90 | LR: 2.84e-04 | Grad: 0.243 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 2040/6501 | Loss: 0.6385 | PPL: 1.92 | LR: 2.84e-04 | Grad: 0.264 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 2050/6501 | Loss: 0.6303 | PPL: 1.91 | LR: 2.83e-04 | Grad: 0.465 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 2060/6501 | Loss: 0.6280 | PPL: 1.90 | LR: 2.83e-04 | Grad: 0.559 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 2070/6501 | Loss: 0.5903 | PPL: 1.83 | LR: 2.83e-04 | Grad: 0.223 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 2080/6501 | Loss: 0.5538 | PPL: 1.78 | LR: 2.83e-04 | Grad: 0.238 | Time: 0.517s
INFO:__main__:Epoch 0 | Step 2090/6501 | Loss: 0.5384 | PPL: 1.74 | LR: 2.83e-04 | Grad: 0.377 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 2100/6501 | Loss: 0.5324 | PPL: 1.73 | LR: 2.83e-04 | Grad: 0.296 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 2110/6501 | Loss: 0.5355 | PPL: 1.74 | LR: 2.82e-04 | Grad: 0.220 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 2120/6501 | Loss: 0.5564 | PPL: 1.78 | LR: 2.82e-04 | Grad: 0.464 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 2130/6501 | Loss: 0.6071 | PPL: 1.86 | LR: 2.82e-04 | Grad: 0.406 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 2140/6501 | Loss: 0.6122 | PPL: 1.88 | LR: 2.82e-04 | Grad: 0.278 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 2150/6501 | Loss: 0.6463 | PPL: 1.95 | LR: 2.82e-04 | Grad: 0.362 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 2160/6501 | Loss: 0.6444 | PPL: 1.96 | LR: 2.82e-04 | Grad: 0.375 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 2170/6501 | Loss: 0.6355 | PPL: 1.94 | LR: 2.81e-04 | Grad: 0.379 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 2180/6501 | Loss: 0.6388 | PPL: 1.95 | LR: 2.81e-04 | Grad: 0.403 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 2190/6501 | Loss: 0.6398 | PPL: 1.96 | LR: 2.81e-04 | Grad: 0.314 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 2200/6501 | Loss: 0.6040 | PPL: 1.89 | LR: 2.81e-04 | Grad: 0.222 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 2210/6501 | Loss: 0.5834 | PPL: 1.85 | LR: 2.81e-04 | Grad: 0.293 | Time: 0.528s
INFO:__main__:Epoch 0 | Step 2220/6501 | Loss: 0.5621 | PPL: 1.80 | LR: 2.81e-04 | Grad: 0.310 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 2230/6501 | Loss: 0.5531 | PPL: 1.78 | LR: 2.80e-04 | Grad: 0.301 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 2240/6501 | Loss: 0.5495 | PPL: 1.76 | LR: 2.80e-04 | Grad: 0.240 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 2250/6501 | Loss: 0.5650 | PPL: 1.79 | LR: 2.80e-04 | Grad: 0.286 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 2260/6501 | Loss: 0.5882 | PPL: 1.83 | LR: 2.80e-04 | Grad: 0.398 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 2270/6501 | Loss: 0.6186 | PPL: 1.89 | LR: 2.80e-04 | Grad: 0.354 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 2280/6501 | Loss: 0.5862 | PPL: 1.83 | LR: 2.79e-04 | Grad: 0.236 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 2290/6501 | Loss: 0.5997 | PPL: 1.86 | LR: 2.79e-04 | Grad: 0.270 | Time: 0.517s
INFO:__main__:Epoch 0 | Step 2300/6501 | Loss: 0.5773 | PPL: 1.82 | LR: 2.79e-04 | Grad: 0.406 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 2310/6501 | Loss: 0.5740 | PPL: 1.82 | LR: 2.79e-04 | Grad: 0.265 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 2320/6501 | Loss: 0.5587 | PPL: 1.80 | LR: 2.79e-04 | Grad: 0.350 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 2330/6501 | Loss: 0.5521 | PPL: 1.79 | LR: 2.78e-04 | Grad: 0.326 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 2340/6501 | Loss: 0.5585 | PPL: 1.80 | LR: 2.78e-04 | Grad: 0.424 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 2350/6501 | Loss: 0.5796 | PPL: 1.84 | LR: 2.78e-04 | Grad: 0.346 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 2360/6501 | Loss: 0.5793 | PPL: 1.82 | LR: 2.78e-04 | Grad: 0.335 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 2370/6501 | Loss: 0.5802 | PPL: 1.82 | LR: 2.78e-04 | Grad: 0.324 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 2380/6501 | Loss: 0.6078 | PPL: 1.86 | LR: 2.78e-04 | Grad: 0.383 | Time: 0.518s
INFO:__main__:Epoch 0 | Step 2390/6501 | Loss: 0.5837 | PPL: 1.82 | LR: 2.77e-04 | Grad: 0.262 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 2400/6501 | Loss: 0.5620 | PPL: 1.78 | LR: 2.77e-04 | Grad: 0.320 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 2410/6501 | Loss: 0.5215 | PPL: 1.70 | LR: 2.77e-04 | Grad: 0.241 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 2420/6501 | Loss: 0.5228 | PPL: 1.73 | LR: 2.77e-04 | Grad: 0.256 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 2430/6501 | Loss: 0.5137 | PPL: 1.71 | LR: 2.77e-04 | Grad: 0.350 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 2440/6501 | Loss: 0.5175 | PPL: 1.72 | LR: 2.76e-04 | Grad: 0.278 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 2450/6501 | Loss: 0.5434 | PPL: 1.77 | LR: 2.76e-04 | Grad: 0.451 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 2460/6501 | Loss: 0.5559 | PPL: 1.79 | LR: 2.76e-04 | Grad: 0.238 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 2470/6501 | Loss: 0.5647 | PPL: 1.78 | LR: 2.76e-04 | Grad: 0.297 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 2480/6501 | Loss: 0.5693 | PPL: 1.80 | LR: 2.76e-04 | Grad: 0.307 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 2490/6501 | Loss: 0.5779 | PPL: 1.81 | LR: 2.75e-04 | Grad: 0.414 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 2500/6501 | Loss: 0.5655 | PPL: 1.79 | LR: 2.75e-04 | Grad: 0.275 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 2510/6501 | Loss: 0.5801 | PPL: 1.82 | LR: 2.75e-04 | Grad: 0.289 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 2520/6501 | Loss: 0.5590 | PPL: 1.79 | LR: 2.75e-04 | Grad: 0.312 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 2530/6501 | Loss: 0.5754 | PPL: 1.82 | LR: 2.75e-04 | Grad: 0.285 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 2540/6501 | Loss: 0.5741 | PPL: 1.81 | LR: 2.74e-04 | Grad: 0.251 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 2550/6501 | Loss: 0.5814 | PPL: 1.82 | LR: 2.74e-04 | Grad: 0.440 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 2560/6501 | Loss: 0.5602 | PPL: 1.78 | LR: 2.74e-04 | Grad: 0.315 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 2570/6501 | Loss: 0.6019 | PPL: 1.87 | LR: 2.74e-04 | Grad: 0.330 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 2580/6501 | Loss: 0.5916 | PPL: 1.85 | LR: 2.74e-04 | Grad: 0.235 | Time: 0.518s
INFO:__main__:Epoch 0 | Step 2590/6501 | Loss: 0.5773 | PPL: 1.83 | LR: 2.73e-04 | Grad: 0.394 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 2600/6501 | Loss: 0.5782 | PPL: 1.83 | LR: 2.73e-04 | Grad: 0.317 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 2610/6501 | Loss: 0.6311 | PPL: 1.94 | LR: 2.73e-04 | Grad: 0.450 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 2620/6501 | Loss: 0.5794 | PPL: 1.84 | LR: 2.73e-04 | Grad: 0.208 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 2630/6501 | Loss: 0.5991 | PPL: 1.88 | LR: 2.72e-04 | Grad: 0.415 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 2640/6501 | Loss: 0.6288 | PPL: 1.93 | LR: 2.72e-04 | Grad: 0.434 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 2650/6501 | Loss: 0.6347 | PPL: 1.95 | LR: 2.72e-04 | Grad: 0.268 | Time: 0.528s
INFO:__main__:Epoch 0 | Step 2660/6501 | Loss: 0.5927 | PPL: 1.86 | LR: 2.72e-04 | Grad: 0.258 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 2670/6501 | Loss: 0.5996 | PPL: 1.87 | LR: 2.72e-04 | Grad: 0.276 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 2680/6501 | Loss: 0.5948 | PPL: 1.85 | LR: 2.71e-04 | Grad: 0.244 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 2690/6501 | Loss: 0.5666 | PPL: 1.80 | LR: 2.71e-04 | Grad: 0.276 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 2700/6501 | Loss: 0.5534 | PPL: 1.77 | LR: 2.71e-04 | Grad: 0.272 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 2710/6501 | Loss: 0.5730 | PPL: 1.80 | LR: 2.71e-04 | Grad: 0.352 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 2720/6501 | Loss: 0.5809 | PPL: 1.81 | LR: 2.71e-04 | Grad: 0.328 | Time: 0.518s
INFO:__main__:Epoch 0 | Step 2730/6501 | Loss: 0.5701 | PPL: 1.80 | LR: 2.70e-04 | Grad: 0.272 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 2740/6501 | Loss: 0.5643 | PPL: 1.79 | LR: 2.70e-04 | Grad: 0.234 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 2750/6501 | Loss: 0.5640 | PPL: 1.78 | LR: 2.70e-04 | Grad: 0.309 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 2760/6501 | Loss: 0.5364 | PPL: 1.73 | LR: 2.70e-04 | Grad: 0.350 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 2770/6501 | Loss: 0.5524 | PPL: 1.77 | LR: 2.69e-04 | Grad: 0.385 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 2780/6501 | Loss: 0.5578 | PPL: 1.78 | LR: 2.69e-04 | Grad: 0.336 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 2790/6501 | Loss: 0.5698 | PPL: 1.80 | LR: 2.69e-04 | Grad: 0.340 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 2800/6501 | Loss: 0.5543 | PPL: 1.78 | LR: 2.69e-04 | Grad: 0.278 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 2810/6501 | Loss: 0.5827 | PPL: 1.82 | LR: 2.69e-04 | Grad: 0.335 | Time: 0.518s
INFO:__main__:Epoch 0 | Step 2820/6501 | Loss: 0.5804 | PPL: 1.82 | LR: 2.68e-04 | Grad: 0.347 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 2830/6501 | Loss: 0.5942 | PPL: 1.84 | LR: 2.68e-04 | Grad: 0.348 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 2840/6501 | Loss: 0.5972 | PPL: 1.85 | LR: 2.68e-04 | Grad: 0.292 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 2850/6501 | Loss: 0.6035 | PPL: 1.86 | LR: 2.68e-04 | Grad: 0.386 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 2860/6501 | Loss: 0.6029 | PPL: 1.88 | LR: 2.67e-04 | Grad: 0.286 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 2870/6501 | Loss: 0.5886 | PPL: 1.86 | LR: 2.67e-04 | Grad: 0.247 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 2880/6501 | Loss: 0.5649 | PPL: 1.81 | LR: 2.67e-04 | Grad: 0.367 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 2890/6501 | Loss: 0.5819 | PPL: 1.85 | LR: 2.67e-04 | Grad: 0.293 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 2900/6501 | Loss: 0.5932 | PPL: 1.87 | LR: 2.67e-04 | Grad: 0.339 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 2910/6501 | Loss: 0.5866 | PPL: 1.84 | LR: 2.66e-04 | Grad: 0.386 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 2920/6501 | Loss: 0.5996 | PPL: 1.87 | LR: 2.66e-04 | Grad: 0.361 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 2930/6501 | Loss: 0.5979 | PPL: 1.86 | LR: 2.66e-04 | Grad: 0.364 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 2940/6501 | Loss: 0.5791 | PPL: 1.82 | LR: 2.66e-04 | Grad: 0.336 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 2950/6501 | Loss: 0.5856 | PPL: 1.83 | LR: 2.65e-04 | Grad: 0.442 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 2960/6501 | Loss: 0.5798 | PPL: 1.82 | LR: 2.65e-04 | Grad: 0.285 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 2970/6501 | Loss: 0.5639 | PPL: 1.79 | LR: 2.65e-04 | Grad: 0.290 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 2980/6501 | Loss: 0.5748 | PPL: 1.81 | LR: 2.65e-04 | Grad: 0.300 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 2990/6501 | Loss: 0.5810 | PPL: 1.83 | LR: 2.64e-04 | Grad: 1.030 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 3000/6501 | Loss: 0.5669 | PPL: 1.80 | LR: 2.64e-04 | Grad: 0.248 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 3010/6501 | Loss: 0.5640 | PPL: 1.80 | LR: 2.64e-04 | Grad: 0.415 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 3020/6501 | Loss: 0.5796 | PPL: 1.82 | LR: 2.64e-04 | Grad: 0.268 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 3030/6501 | Loss: 0.5807 | PPL: 1.83 | LR: 2.64e-04 | Grad: 0.343 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 3040/6501 | Loss: 0.5901 | PPL: 1.84 | LR: 2.63e-04 | Grad: 0.303 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 3050/6501 | Loss: 0.5910 | PPL: 1.84 | LR: 2.63e-04 | Grad: 0.427 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 3060/6501 | Loss: 0.5835 | PPL: 1.83 | LR: 2.63e-04 | Grad: 0.327 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 3070/6501 | Loss: 0.5874 | PPL: 1.83 | LR: 2.63e-04 | Grad: 0.431 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 3080/6501 | Loss: 0.5967 | PPL: 1.85 | LR: 2.62e-04 | Grad: 0.396 | Time: 0.527s
INFO:__main__:Epoch 0 | Step 3090/6501 | Loss: 0.5729 | PPL: 1.81 | LR: 2.62e-04 | Grad: 0.255 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 3100/6501 | Loss: 0.5724 | PPL: 1.81 | LR: 2.62e-04 | Grad: 0.302 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 3110/6501 | Loss: 0.6109 | PPL: 1.89 | LR: 2.62e-04 | Grad: 0.365 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 3120/6501 | Loss: 0.5883 | PPL: 1.86 | LR: 2.61e-04 | Grad: 0.389 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 3130/6501 | Loss: 0.5785 | PPL: 1.84 | LR: 2.61e-04 | Grad: 0.392 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 3140/6501 | Loss: 0.5903 | PPL: 1.86 | LR: 2.61e-04 | Grad: 0.307 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 3150/6501 | Loss: 0.5969 | PPL: 1.87 | LR: 2.61e-04 | Grad: 0.356 | Time: 0.518s
INFO:__main__:Epoch 0 | Step 3160/6501 | Loss: 0.5619 | PPL: 1.79 | LR: 2.60e-04 | Grad: 0.282 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 3170/6501 | Loss: 0.5493 | PPL: 1.76 | LR: 2.60e-04 | Grad: 0.237 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 3180/6501 | Loss: 0.5368 | PPL: 1.74 | LR: 2.60e-04 | Grad: 0.279 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 3190/6501 | Loss: 0.5206 | PPL: 1.71 | LR: 2.60e-04 | Grad: 0.335 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 3200/6501 | Loss: 0.5066 | PPL: 1.68 | LR: 2.59e-04 | Grad: 0.211 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 3210/6501 | Loss: 0.5115 | PPL: 1.69 | LR: 2.59e-04 | Grad: 0.269 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 3220/6501 | Loss: 0.5126 | PPL: 1.69 | LR: 2.59e-04 | Grad: 0.329 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 3230/6501 | Loss: 0.5078 | PPL: 1.68 | LR: 2.59e-04 | Grad: 0.312 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 3240/6501 | Loss: 0.5371 | PPL: 1.73 | LR: 2.58e-04 | Grad: 0.447 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 3250/6501 | Loss: 0.5314 | PPL: 1.72 | LR: 2.58e-04 | Grad: 0.251 | Time: 0.526s
INFO:__main__:Epoch 0 | Step 3260/6501 | Loss: 0.5264 | PPL: 1.71 | LR: 2.58e-04 | Grad: 0.384 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 3270/6501 | Loss: 0.5942 | PPL: 1.87 | LR: 2.58e-04 | Grad: 0.558 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 3280/6501 | Loss: 0.6050 | PPL: 1.89 | LR: 2.57e-04 | Grad: 0.382 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 3290/6501 | Loss: 0.5904 | PPL: 1.86 | LR: 2.57e-04 | Grad: 0.342 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 3300/6501 | Loss: 0.5875 | PPL: 1.86 | LR: 2.57e-04 | Grad: 0.252 | Time: 0.517s
INFO:__main__:Epoch 0 | Step 3310/6501 | Loss: 0.5704 | PPL: 1.83 | LR: 2.57e-04 | Grad: 0.267 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 3320/6501 | Loss: 0.5324 | PPL: 1.73 | LR: 2.56e-04 | Grad: 0.265 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 3330/6501 | Loss: 0.5454 | PPL: 1.77 | LR: 2.56e-04 | Grad: 0.309 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 3340/6501 | Loss: 0.5422 | PPL: 1.77 | LR: 2.56e-04 | Grad: 0.288 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 3350/6501 | Loss: 0.5634 | PPL: 1.81 | LR: 2.56e-04 | Grad: 0.409 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 3360/6501 | Loss: 0.5841 | PPL: 1.84 | LR: 2.55e-04 | Grad: 0.314 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 3370/6501 | Loss: 0.5883 | PPL: 1.85 | LR: 2.55e-04 | Grad: 0.241 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 3380/6501 | Loss: 0.5636 | PPL: 1.79 | LR: 2.55e-04 | Grad: 0.470 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 3390/6501 | Loss: 0.5810 | PPL: 1.82 | LR: 2.55e-04 | Grad: 0.212 | Time: 0.527s
INFO:__main__:Epoch 0 | Step 3400/6501 | Loss: 0.5711 | PPL: 1.81 | LR: 2.54e-04 | Grad: 0.465 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 3410/6501 | Loss: 0.5768 | PPL: 1.82 | LR: 2.54e-04 | Grad: 0.258 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 3420/6501 | Loss: 0.5681 | PPL: 1.80 | LR: 2.54e-04 | Grad: 0.428 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 3430/6501 | Loss: 0.5708 | PPL: 1.81 | LR: 2.53e-04 | Grad: 0.338 | Time: 0.526s
INFO:__main__:Epoch 0 | Step 3440/6501 | Loss: 0.5483 | PPL: 1.77 | LR: 2.53e-04 | Grad: 0.256 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 3450/6501 | Loss: 0.5504 | PPL: 1.77 | LR: 2.53e-04 | Grad: 0.238 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 3460/6501 | Loss: 0.5516 | PPL: 1.76 | LR: 2.53e-04 | Grad: 0.460 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 3470/6501 | Loss: 0.5517 | PPL: 1.76 | LR: 2.52e-04 | Grad: 0.333 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 3480/6501 | Loss: 0.5606 | PPL: 1.78 | LR: 2.52e-04 | Grad: 0.296 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 3490/6501 | Loss: 0.5765 | PPL: 1.82 | LR: 2.52e-04 | Grad: 0.206 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 3500/6501 | Loss: 0.5834 | PPL: 1.83 | LR: 2.52e-04 | Grad: 0.303 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 3510/6501 | Loss: 0.5899 | PPL: 1.85 | LR: 2.51e-04 | Grad: 0.464 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 3520/6501 | Loss: 0.5891 | PPL: 1.85 | LR: 2.51e-04 | Grad: 0.300 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 3530/6501 | Loss: 0.5804 | PPL: 1.83 | LR: 2.51e-04 | Grad: 0.309 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 3540/6501 | Loss: 0.5723 | PPL: 1.80 | LR: 2.51e-04 | Grad: 0.312 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 3550/6501 | Loss: 0.5777 | PPL: 1.82 | LR: 2.50e-04 | Grad: 0.471 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 3560/6501 | Loss: 0.5632 | PPL: 1.79 | LR: 2.50e-04 | Grad: 0.378 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 3570/6501 | Loss: 0.5326 | PPL: 1.73 | LR: 2.50e-04 | Grad: 0.286 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 3580/6501 | Loss: 0.5323 | PPL: 1.74 | LR: 2.49e-04 | Grad: 0.334 | Time: 0.517s
INFO:__main__:Epoch 0 | Step 3590/6501 | Loss: 0.5413 | PPL: 1.75 | LR: 2.49e-04 | Grad: 0.451 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 3600/6501 | Loss: 0.5183 | PPL: 1.71 | LR: 2.49e-04 | Grad: 0.382 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 3610/6501 | Loss: 0.5356 | PPL: 1.75 | LR: 2.49e-04 | Grad: 0.554 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 3620/6501 | Loss: 0.5835 | PPL: 1.84 | LR: 2.48e-04 | Grad: 0.300 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 3630/6501 | Loss: 0.5843 | PPL: 1.84 | LR: 2.48e-04 | Grad: 0.292 | Time: 0.526s
INFO:__main__:Epoch 0 | Step 3640/6501 | Loss: 0.5814 | PPL: 1.85 | LR: 2.48e-04 | Grad: 0.376 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 3650/6501 | Loss: 0.6016 | PPL: 1.89 | LR: 2.48e-04 | Grad: 0.294 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 3660/6501 | Loss: 0.5916 | PPL: 1.88 | LR: 2.47e-04 | Grad: 0.394 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 3670/6501 | Loss: 0.5778 | PPL: 1.84 | LR: 2.47e-04 | Grad: 0.261 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 3680/6501 | Loss: 0.5863 | PPL: 1.85 | LR: 2.47e-04 | Grad: 0.670 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 3690/6501 | Loss: 0.5839 | PPL: 1.84 | LR: 2.46e-04 | Grad: 0.358 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 3700/6501 | Loss: 0.5876 | PPL: 1.84 | LR: 2.46e-04 | Grad: 0.403 | Time: 0.526s
INFO:__main__:Epoch 0 | Step 3710/6501 | Loss: 0.5912 | PPL: 1.84 | LR: 2.46e-04 | Grad: 0.359 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 3720/6501 | Loss: 0.5769 | PPL: 1.81 | LR: 2.46e-04 | Grad: 0.349 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 3730/6501 | Loss: 0.5705 | PPL: 1.80 | LR: 2.45e-04 | Grad: 0.345 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 3740/6501 | Loss: 0.5552 | PPL: 1.77 | LR: 2.45e-04 | Grad: 0.307 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 3750/6501 | Loss: 0.5369 | PPL: 1.74 | LR: 2.45e-04 | Grad: 0.253 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 3760/6501 | Loss: 0.5289 | PPL: 1.73 | LR: 2.44e-04 | Grad: 0.292 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 3770/6501 | Loss: 0.5392 | PPL: 1.75 | LR: 2.44e-04 | Grad: 0.374 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 3780/6501 | Loss: 0.5233 | PPL: 1.72 | LR: 2.44e-04 | Grad: 0.365 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 3790/6501 | Loss: 0.5261 | PPL: 1.73 | LR: 2.44e-04 | Grad: 0.202 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 3800/6501 | Loss: 0.5409 | PPL: 1.75 | LR: 2.43e-04 | Grad: 0.382 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 3810/6501 | Loss: 0.5407 | PPL: 1.75 | LR: 2.43e-04 | Grad: 0.318 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 3820/6501 | Loss: 0.5266 | PPL: 1.72 | LR: 2.43e-04 | Grad: 0.436 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 3830/6501 | Loss: 0.5601 | PPL: 1.79 | LR: 2.42e-04 | Grad: 0.512 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 3840/6501 | Loss: 0.5722 | PPL: 1.81 | LR: 2.42e-04 | Grad: 0.304 | Time: 0.526s
INFO:__main__:Epoch 0 | Step 3850/6501 | Loss: 0.5788 | PPL: 1.84 | LR: 2.42e-04 | Grad: 0.306 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 3860/6501 | Loss: 0.5758 | PPL: 1.83 | LR: 2.42e-04 | Grad: 0.272 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 3870/6501 | Loss: 0.5793 | PPL: 1.84 | LR: 2.41e-04 | Grad: 0.360 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 3880/6501 | Loss: 0.5451 | PPL: 1.78 | LR: 2.41e-04 | Grad: 0.378 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 3890/6501 | Loss: 0.5575 | PPL: 1.80 | LR: 2.41e-04 | Grad: 0.323 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 3900/6501 | Loss: 0.5385 | PPL: 1.74 | LR: 2.40e-04 | Grad: 0.467 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 3910/6501 | Loss: 0.5384 | PPL: 1.74 | LR: 2.40e-04 | Grad: 0.358 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 3920/6501 | Loss: 0.5471 | PPL: 1.75 | LR: 2.40e-04 | Grad: 0.321 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 3930/6501 | Loss: 0.5681 | PPL: 1.79 | LR: 2.40e-04 | Grad: 0.348 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 3940/6501 | Loss: 0.5341 | PPL: 1.73 | LR: 2.39e-04 | Grad: 0.347 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 3950/6501 | Loss: 0.5290 | PPL: 1.72 | LR: 2.39e-04 | Grad: 0.309 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 3960/6501 | Loss: 0.5341 | PPL: 1.73 | LR: 2.39e-04 | Grad: 0.338 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 3970/6501 | Loss: 0.5284 | PPL: 1.72 | LR: 2.38e-04 | Grad: 0.357 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 3980/6501 | Loss: 0.5278 | PPL: 1.72 | LR: 2.38e-04 | Grad: 0.403 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 3990/6501 | Loss: 0.5365 | PPL: 1.74 | LR: 2.38e-04 | Grad: 0.247 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 4000/6501 | Loss: 0.5405 | PPL: 1.74 | LR: 2.37e-04 | Grad: 0.205 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 4010/6501 | Loss: 0.5399 | PPL: 1.75 | LR: 2.37e-04 | Grad: 0.247 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 4020/6501 | Loss: 0.5434 | PPL: 1.75 | LR: 2.37e-04 | Grad: 0.282 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 4030/6501 | Loss: 0.5370 | PPL: 1.74 | LR: 2.37e-04 | Grad: 0.355 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 4040/6501 | Loss: 0.5548 | PPL: 1.77 | LR: 2.36e-04 | Grad: 0.343 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 4050/6501 | Loss: 0.5613 | PPL: 1.79 | LR: 2.36e-04 | Grad: 0.363 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 4060/6501 | Loss: 0.5410 | PPL: 1.75 | LR: 2.36e-04 | Grad: 0.367 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 4070/6501 | Loss: 0.5427 | PPL: 1.74 | LR: 2.35e-04 | Grad: 0.375 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 4080/6501 | Loss: 0.5389 | PPL: 1.74 | LR: 2.35e-04 | Grad: 0.361 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 4090/6501 | Loss: 0.5146 | PPL: 1.70 | LR: 2.35e-04 | Grad: 0.266 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 4100/6501 | Loss: 0.5134 | PPL: 1.69 | LR: 2.34e-04 | Grad: 0.299 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 4110/6501 | Loss: 0.5265 | PPL: 1.71 | LR: 2.34e-04 | Grad: 0.288 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 4120/6501 | Loss: 0.5399 | PPL: 1.74 | LR: 2.34e-04 | Grad: 0.498 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 4130/6501 | Loss: 0.5374 | PPL: 1.74 | LR: 2.34e-04 | Grad: 0.422 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 4140/6501 | Loss: 0.5468 | PPL: 1.75 | LR: 2.33e-04 | Grad: 0.304 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 4150/6501 | Loss: 0.5434 | PPL: 1.75 | LR: 2.33e-04 | Grad: 0.298 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 4160/6501 | Loss: 0.5487 | PPL: 1.76 | LR: 2.33e-04 | Grad: 0.352 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 4170/6501 | Loss: 0.5487 | PPL: 1.76 | LR: 2.32e-04 | Grad: 0.377 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 4180/6501 | Loss: 0.5603 | PPL: 1.78 | LR: 2.32e-04 | Grad: 0.438 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 4190/6501 | Loss: 0.5641 | PPL: 1.79 | LR: 2.32e-04 | Grad: 0.274 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 4200/6501 | Loss: 0.5861 | PPL: 1.84 | LR: 2.31e-04 | Grad: 0.344 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 4210/6501 | Loss: 0.6051 | PPL: 1.87 | LR: 2.31e-04 | Grad: 0.615 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 4220/6501 | Loss: 0.5953 | PPL: 1.85 | LR: 2.31e-04 | Grad: 0.319 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 4230/6501 | Loss: 0.5829 | PPL: 1.83 | LR: 2.31e-04 | Grad: 0.349 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 4240/6501 | Loss: 0.5587 | PPL: 1.79 | LR: 2.30e-04 | Grad: 0.319 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 4250/6501 | Loss: 0.5582 | PPL: 1.78 | LR: 2.30e-04 | Grad: 0.297 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 4260/6501 | Loss: 0.5322 | PPL: 1.72 | LR: 2.30e-04 | Grad: 0.321 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 4270/6501 | Loss: 0.5279 | PPL: 1.72 | LR: 2.29e-04 | Grad: 0.260 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 4280/6501 | Loss: 0.5433 | PPL: 1.76 | LR: 2.29e-04 | Grad: 0.365 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 4290/6501 | Loss: 0.5551 | PPL: 1.78 | LR: 2.29e-04 | Grad: 0.342 | Time: 0.517s
INFO:__main__:Epoch 0 | Step 4300/6501 | Loss: 0.5369 | PPL: 1.74 | LR: 2.28e-04 | Grad: 0.359 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 4310/6501 | Loss: 0.5336 | PPL: 1.73 | LR: 2.28e-04 | Grad: 0.268 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 4320/6501 | Loss: 0.5491 | PPL: 1.77 | LR: 2.28e-04 | Grad: 0.270 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 4330/6501 | Loss: 0.5478 | PPL: 1.76 | LR: 2.27e-04 | Grad: 0.379 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 4340/6501 | Loss: 0.5753 | PPL: 1.82 | LR: 2.27e-04 | Grad: 0.198 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 4350/6501 | Loss: 0.5814 | PPL: 1.84 | LR: 2.27e-04 | Grad: 0.360 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 4360/6501 | Loss: 0.5796 | PPL: 1.84 | LR: 2.26e-04 | Grad: 0.433 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 4370/6501 | Loss: 0.5710 | PPL: 1.82 | LR: 2.26e-04 | Grad: 0.276 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 4380/6501 | Loss: 0.5531 | PPL: 1.78 | LR: 2.26e-04 | Grad: 0.321 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 4390/6501 | Loss: 0.5259 | PPL: 1.72 | LR: 2.26e-04 | Grad: 0.411 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 4400/6501 | Loss: 0.5045 | PPL: 1.68 | LR: 2.25e-04 | Grad: 0.343 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 4410/6501 | Loss: 0.5352 | PPL: 1.74 | LR: 2.25e-04 | Grad: 0.494 | Time: 0.526s
INFO:__main__:Epoch 0 | Step 4420/6501 | Loss: 0.5375 | PPL: 1.75 | LR: 2.25e-04 | Grad: 0.376 | Time: 0.526s
INFO:__main__:Epoch 0 | Step 4430/6501 | Loss: 0.5374 | PPL: 1.74 | LR: 2.24e-04 | Grad: 0.275 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 4440/6501 | Loss: 0.5544 | PPL: 1.78 | LR: 2.24e-04 | Grad: 0.290 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 4450/6501 | Loss: 0.5679 | PPL: 1.80 | LR: 2.24e-04 | Grad: 0.212 | Time: 0.526s
INFO:__main__:Epoch 0 | Step 4460/6501 | Loss: 0.5611 | PPL: 1.78 | LR: 2.23e-04 | Grad: 0.400 | Time: 0.527s
INFO:__main__:Epoch 0 | Step 4470/6501 | Loss: 0.5593 | PPL: 1.78 | LR: 2.23e-04 | Grad: 0.304 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 4480/6501 | Loss: 0.5683 | PPL: 1.80 | LR: 2.23e-04 | Grad: 0.362 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 4490/6501 | Loss: 0.5296 | PPL: 1.74 | LR: 2.22e-04 | Grad: 0.221 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 4500/6501 | Loss: 0.5382 | PPL: 1.76 | LR: 2.22e-04 | Grad: 0.375 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 4510/6501 | Loss: 0.5260 | PPL: 1.73 | LR: 2.22e-04 | Grad: 0.265 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 4520/6501 | Loss: 0.5003 | PPL: 1.68 | LR: 2.21e-04 | Grad: 0.284 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 4530/6501 | Loss: 0.5019 | PPL: 1.68 | LR: 2.21e-04 | Grad: 0.253 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 4540/6501 | Loss: 0.5197 | PPL: 1.71 | LR: 2.21e-04 | Grad: 0.291 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 4550/6501 | Loss: 0.5226 | PPL: 1.71 | LR: 2.20e-04 | Grad: 0.302 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 4560/6501 | Loss: 0.4994 | PPL: 1.67 | LR: 2.20e-04 | Grad: 0.375 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 4570/6501 | Loss: 0.5364 | PPL: 1.75 | LR: 2.20e-04 | Grad: 0.246 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 4580/6501 | Loss: 0.5432 | PPL: 1.76 | LR: 2.19e-04 | Grad: 0.340 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 4590/6501 | Loss: 0.5543 | PPL: 1.78 | LR: 2.19e-04 | Grad: 0.238 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 4600/6501 | Loss: 0.5610 | PPL: 1.79 | LR: 2.19e-04 | Grad: 0.343 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 4610/6501 | Loss: 0.5805 | PPL: 1.82 | LR: 2.19e-04 | Grad: 0.413 | Time: 0.514s
INFO:__main__:Epoch 0 | Step 4620/6501 | Loss: 0.5449 | PPL: 1.75 | LR: 2.18e-04 | Grad: 0.343 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 4630/6501 | Loss: 0.5243 | PPL: 1.71 | LR: 2.18e-04 | Grad: 0.288 | Time: 0.526s
INFO:__main__:Epoch 0 | Step 4640/6501 | Loss: 0.5274 | PPL: 1.71 | LR: 2.18e-04 | Grad: 0.361 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 4650/6501 | Loss: 0.5353 | PPL: 1.75 | LR: 2.17e-04 | Grad: 0.472 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 4660/6501 | Loss: 0.5372 | PPL: 1.75 | LR: 2.17e-04 | Grad: 0.288 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 4670/6501 | Loss: 0.5512 | PPL: 1.77 | LR: 2.17e-04 | Grad: 0.401 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 4680/6501 | Loss: 0.5704 | PPL: 1.81 | LR: 2.16e-04 | Grad: 0.205 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 4690/6501 | Loss: 0.5703 | PPL: 1.81 | LR: 2.16e-04 | Grad: 0.361 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 4700/6501 | Loss: 0.5385 | PPL: 1.73 | LR: 2.16e-04 | Grad: 0.353 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 4710/6501 | Loss: 0.5357 | PPL: 1.73 | LR: 2.15e-04 | Grad: 0.233 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 4720/6501 | Loss: 0.5303 | PPL: 1.72 | LR: 2.15e-04 | Grad: 0.274 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 4730/6501 | Loss: 0.5550 | PPL: 1.77 | LR: 2.15e-04 | Grad: 0.480 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 4740/6501 | Loss: 0.5724 | PPL: 1.81 | LR: 2.14e-04 | Grad: 0.347 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 4750/6501 | Loss: 0.5677 | PPL: 1.80 | LR: 2.14e-04 | Grad: 0.212 | Time: 0.526s
INFO:__main__:Epoch 0 | Step 4760/6501 | Loss: 0.5484 | PPL: 1.77 | LR: 2.14e-04 | Grad: 0.293 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 4770/6501 | Loss: 0.5538 | PPL: 1.77 | LR: 2.13e-04 | Grad: 0.327 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 4780/6501 | Loss: 0.5308 | PPL: 1.73 | LR: 2.13e-04 | Grad: 0.272 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 4790/6501 | Loss: 0.4969 | PPL: 1.67 | LR: 2.13e-04 | Grad: 0.270 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 4800/6501 | Loss: 0.5029 | PPL: 1.68 | LR: 2.12e-04 | Grad: 0.309 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 4810/6501 | Loss: 0.5324 | PPL: 1.73 | LR: 2.12e-04 | Grad: 0.331 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 4820/6501 | Loss: 0.5263 | PPL: 1.72 | LR: 2.12e-04 | Grad: 0.318 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 4830/6501 | Loss: 0.4950 | PPL: 1.66 | LR: 2.11e-04 | Grad: 0.274 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 4840/6501 | Loss: 0.4958 | PPL: 1.66 | LR: 2.11e-04 | Grad: 0.283 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 4850/6501 | Loss: 0.5088 | PPL: 1.68 | LR: 2.11e-04 | Grad: 0.452 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 4860/6501 | Loss: 0.5248 | PPL: 1.72 | LR: 2.10e-04 | Grad: 0.274 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 4870/6501 | Loss: 0.5234 | PPL: 1.72 | LR: 2.10e-04 | Grad: 0.404 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 4880/6501 | Loss: 0.5681 | PPL: 1.80 | LR: 2.10e-04 | Grad: 0.431 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 4890/6501 | Loss: 0.5850 | PPL: 1.83 | LR: 2.09e-04 | Grad: 0.377 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 4900/6501 | Loss: 0.5899 | PPL: 1.84 | LR: 2.09e-04 | Grad: 0.348 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 4910/6501 | Loss: 0.5447 | PPL: 1.75 | LR: 2.09e-04 | Grad: 0.304 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 4920/6501 | Loss: 0.5466 | PPL: 1.76 | LR: 2.08e-04 | Grad: 0.282 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 4930/6501 | Loss: 0.5093 | PPL: 1.69 | LR: 2.08e-04 | Grad: 0.222 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 4940/6501 | Loss: 0.5156 | PPL: 1.70 | LR: 2.08e-04 | Grad: 0.302 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 4950/6501 | Loss: 0.5170 | PPL: 1.70 | LR: 2.07e-04 | Grad: 0.401 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 4960/6501 | Loss: 0.5393 | PPL: 1.74 | LR: 2.07e-04 | Grad: 0.287 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 4970/6501 | Loss: 0.5477 | PPL: 1.75 | LR: 2.07e-04 | Grad: 0.326 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 4980/6501 | Loss: 0.5618 | PPL: 1.78 | LR: 2.06e-04 | Grad: 0.361 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 4990/6501 | Loss: 0.5348 | PPL: 1.73 | LR: 2.06e-04 | Grad: 0.283 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 5000/6501 | Loss: 0.5378 | PPL: 1.74 | LR: 2.06e-04 | Grad: 0.300 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 5010/6501 | Loss: 0.5226 | PPL: 1.71 | LR: 2.05e-04 | Grad: 0.374 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 5020/6501 | Loss: 0.5271 | PPL: 1.73 | LR: 2.05e-04 | Grad: 0.426 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 5030/6501 | Loss: 0.5283 | PPL: 1.74 | LR: 2.05e-04 | Grad: 0.249 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 5040/6501 | Loss: 0.5321 | PPL: 1.75 | LR: 2.04e-04 | Grad: 0.319 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 5050/6501 | Loss: 0.5112 | PPL: 1.71 | LR: 2.04e-04 | Grad: 0.406 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 5060/6501 | Loss: 0.5254 | PPL: 1.74 | LR: 2.04e-04 | Grad: 0.444 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 5070/6501 | Loss: 0.5027 | PPL: 1.70 | LR: 2.03e-04 | Grad: 0.201 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 5080/6501 | Loss: 0.4955 | PPL: 1.68 | LR: 2.03e-04 | Grad: 0.172 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 5090/6501 | Loss: 0.4981 | PPL: 1.68 | LR: 2.03e-04 | Grad: 0.296 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 5100/6501 | Loss: 0.4866 | PPL: 1.66 | LR: 2.02e-04 | Grad: 0.337 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 5110/6501 | Loss: 0.4988 | PPL: 1.68 | LR: 2.02e-04 | Grad: 0.448 | Time: 0.518s
INFO:__main__:Epoch 0 | Step 5120/6501 | Loss: 0.5030 | PPL: 1.69 | LR: 2.02e-04 | Grad: 0.297 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 5130/6501 | Loss: 0.5008 | PPL: 1.68 | LR: 2.01e-04 | Grad: 0.342 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 5140/6501 | Loss: 0.5341 | PPL: 1.74 | LR: 2.01e-04 | Grad: 0.305 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 5150/6501 | Loss: 0.5349 | PPL: 1.74 | LR: 2.00e-04 | Grad: 0.454 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 5160/6501 | Loss: 0.5291 | PPL: 1.73 | LR: 2.00e-04 | Grad: 0.291 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 5170/6501 | Loss: 0.5307 | PPL: 1.73 | LR: 2.00e-04 | Grad: 0.286 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 5180/6501 | Loss: 0.5523 | PPL: 1.78 | LR: 1.99e-04 | Grad: 0.248 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 5190/6501 | Loss: 0.5500 | PPL: 1.77 | LR: 1.99e-04 | Grad: 0.347 | Time: 0.526s
INFO:__main__:Epoch 0 | Step 5200/6501 | Loss: 0.5862 | PPL: 1.84 | LR: 1.99e-04 | Grad: 0.379 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 5210/6501 | Loss: 0.5812 | PPL: 1.83 | LR: 1.98e-04 | Grad: 0.630 | Time: 0.526s
INFO:__main__:Epoch 0 | Step 5220/6501 | Loss: 0.5947 | PPL: 1.86 | LR: 1.98e-04 | Grad: 0.317 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 5230/6501 | Loss: 0.6102 | PPL: 1.88 | LR: 1.98e-04 | Grad: 0.296 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 5240/6501 | Loss: 0.5983 | PPL: 1.86 | LR: 1.97e-04 | Grad: 0.444 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 5250/6501 | Loss: 0.5868 | PPL: 1.83 | LR: 1.97e-04 | Grad: 0.278 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 5260/6501 | Loss: 0.5679 | PPL: 1.80 | LR: 1.97e-04 | Grad: 0.257 | Time: 0.526s
INFO:__main__:Epoch 0 | Step 5270/6501 | Loss: 0.5713 | PPL: 1.81 | LR: 1.96e-04 | Grad: 0.294 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 5280/6501 | Loss: 0.5361 | PPL: 1.75 | LR: 1.96e-04 | Grad: 0.461 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 5290/6501 | Loss: 0.5148 | PPL: 1.71 | LR: 1.96e-04 | Grad: 0.429 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 5300/6501 | Loss: 0.5010 | PPL: 1.69 | LR: 1.95e-04 | Grad: 0.330 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 5310/6501 | Loss: 0.5006 | PPL: 1.69 | LR: 1.95e-04 | Grad: 0.295 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 5320/6501 | Loss: 0.4920 | PPL: 1.66 | LR: 1.95e-04 | Grad: 0.490 | Time: 0.526s
INFO:__main__:Epoch 0 | Step 5330/6501 | Loss: 0.4906 | PPL: 1.66 | LR: 1.94e-04 | Grad: 0.588 | Time: 0.528s
INFO:__main__:Epoch 0 | Step 5340/6501 | Loss: 0.5079 | PPL: 1.68 | LR: 1.94e-04 | Grad: 0.246 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 5350/6501 | Loss: 0.5220 | PPL: 1.73 | LR: 1.94e-04 | Grad: 0.418 | Time: 0.526s
INFO:__main__:Epoch 0 | Step 5360/6501 | Loss: 0.5286 | PPL: 1.74 | LR: 1.93e-04 | Grad: 0.329 | Time: 0.526s
INFO:__main__:Epoch 0 | Step 5370/6501 | Loss: 0.5218 | PPL: 1.73 | LR: 1.93e-04 | Grad: 0.288 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 5380/6501 | Loss: 0.5229 | PPL: 1.73 | LR: 1.93e-04 | Grad: 0.423 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 5390/6501 | Loss: 0.5203 | PPL: 1.73 | LR: 1.92e-04 | Grad: 0.244 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 5400/6501 | Loss: 0.5037 | PPL: 1.68 | LR: 1.92e-04 | Grad: 0.220 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 5410/6501 | Loss: 0.5033 | PPL: 1.68 | LR: 1.91e-04 | Grad: 0.356 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 5420/6501 | Loss: 0.4995 | PPL: 1.67 | LR: 1.91e-04 | Grad: 0.362 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 5430/6501 | Loss: 0.5262 | PPL: 1.74 | LR: 1.91e-04 | Grad: 0.464 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 5440/6501 | Loss: 0.5099 | PPL: 1.71 | LR: 1.90e-04 | Grad: 0.243 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 5450/6501 | Loss: 0.5195 | PPL: 1.72 | LR: 1.90e-04 | Grad: 0.451 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 5460/6501 | Loss: 0.4985 | PPL: 1.69 | LR: 1.90e-04 | Grad: 0.212 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 5470/6501 | Loss: 0.4986 | PPL: 1.69 | LR: 1.89e-04 | Grad: 0.274 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 5480/6501 | Loss: 0.4693 | PPL: 1.62 | LR: 1.89e-04 | Grad: 0.480 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 5490/6501 | Loss: 0.4741 | PPL: 1.63 | LR: 1.89e-04 | Grad: 0.290 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 5500/6501 | Loss: 0.4654 | PPL: 1.62 | LR: 1.88e-04 | Grad: 0.299 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 5510/6501 | Loss: 0.4822 | PPL: 1.64 | LR: 1.88e-04 | Grad: 0.291 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 5520/6501 | Loss: 0.4994 | PPL: 1.67 | LR: 1.88e-04 | Grad: 0.245 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 5530/6501 | Loss: 0.5095 | PPL: 1.69 | LR: 1.87e-04 | Grad: 0.380 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 5540/6501 | Loss: 0.5348 | PPL: 1.74 | LR: 1.87e-04 | Grad: 0.380 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 5550/6501 | Loss: 0.5535 | PPL: 1.78 | LR: 1.87e-04 | Grad: 0.354 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 5560/6501 | Loss: 0.5578 | PPL: 1.78 | LR: 1.86e-04 | Grad: 0.287 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 5570/6501 | Loss: 0.5609 | PPL: 1.79 | LR: 1.86e-04 | Grad: 0.348 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 5580/6501 | Loss: 0.5654 | PPL: 1.80 | LR: 1.85e-04 | Grad: 0.278 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 5590/6501 | Loss: 0.5562 | PPL: 1.78 | LR: 1.85e-04 | Grad: 0.292 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 5600/6501 | Loss: 0.5351 | PPL: 1.73 | LR: 1.85e-04 | Grad: 0.337 | Time: 0.518s
INFO:__main__:Epoch 0 | Step 5610/6501 | Loss: 0.5358 | PPL: 1.73 | LR: 1.84e-04 | Grad: 0.329 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 5620/6501 | Loss: 0.5132 | PPL: 1.69 | LR: 1.84e-04 | Grad: 0.372 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 5630/6501 | Loss: 0.5144 | PPL: 1.70 | LR: 1.84e-04 | Grad: 0.216 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 5640/6501 | Loss: 0.5111 | PPL: 1.69 | LR: 1.83e-04 | Grad: 0.384 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 5650/6501 | Loss: 0.4959 | PPL: 1.67 | LR: 1.83e-04 | Grad: 0.250 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 5660/6501 | Loss: 0.4855 | PPL: 1.65 | LR: 1.83e-04 | Grad: 0.338 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 5670/6501 | Loss: 0.4983 | PPL: 1.68 | LR: 1.82e-04 | Grad: 0.270 | Time: 0.527s
INFO:__main__:Epoch 0 | Step 5680/6501 | Loss: 0.4852 | PPL: 1.65 | LR: 1.82e-04 | Grad: 0.398 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 5690/6501 | Loss: 0.4663 | PPL: 1.63 | LR: 1.82e-04 | Grad: 0.257 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 5700/6501 | Loss: 0.4870 | PPL: 1.66 | LR: 1.81e-04 | Grad: 0.380 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 5710/6501 | Loss: 0.5081 | PPL: 1.70 | LR: 1.81e-04 | Grad: 0.329 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 5720/6501 | Loss: 0.5290 | PPL: 1.74 | LR: 1.81e-04 | Grad: 0.414 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 5730/6501 | Loss: 0.5176 | PPL: 1.72 | LR: 1.80e-04 | Grad: 0.298 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 5740/6501 | Loss: 0.5315 | PPL: 1.73 | LR: 1.80e-04 | Grad: 0.340 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 5750/6501 | Loss: 0.5212 | PPL: 1.71 | LR: 1.79e-04 | Grad: 0.405 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 5760/6501 | Loss: 0.5475 | PPL: 1.76 | LR: 1.79e-04 | Grad: 0.373 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 5770/6501 | Loss: 0.4943 | PPL: 1.66 | LR: 1.79e-04 | Grad: 0.206 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 5780/6501 | Loss: 0.5177 | PPL: 1.70 | LR: 1.78e-04 | Grad: 0.288 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 5790/6501 | Loss: 0.5287 | PPL: 1.73 | LR: 1.78e-04 | Grad: 0.429 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 5800/6501 | Loss: 0.5501 | PPL: 1.77 | LR: 1.78e-04 | Grad: 0.364 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 5810/6501 | Loss: 0.5095 | PPL: 1.70 | LR: 1.77e-04 | Grad: 0.370 | Time: 0.526s
INFO:__main__:Epoch 0 | Step 5820/6501 | Loss: 0.5373 | PPL: 1.75 | LR: 1.77e-04 | Grad: 0.214 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 5830/6501 | Loss: 0.5275 | PPL: 1.73 | LR: 1.77e-04 | Grad: 0.393 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 5840/6501 | Loss: 0.4996 | PPL: 1.67 | LR: 1.76e-04 | Grad: 0.220 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 5850/6501 | Loss: 0.4767 | PPL: 1.63 | LR: 1.76e-04 | Grad: 0.364 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 5860/6501 | Loss: 0.4846 | PPL: 1.65 | LR: 1.75e-04 | Grad: 0.358 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 5870/6501 | Loss: 0.4877 | PPL: 1.65 | LR: 1.75e-04 | Grad: 0.377 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 5880/6501 | Loss: 0.4768 | PPL: 1.63 | LR: 1.75e-04 | Grad: 0.266 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 5890/6501 | Loss: 0.5012 | PPL: 1.68 | LR: 1.74e-04 | Grad: 0.305 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 5900/6501 | Loss: 0.4981 | PPL: 1.67 | LR: 1.74e-04 | Grad: 0.391 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 5910/6501 | Loss: 0.4968 | PPL: 1.67 | LR: 1.74e-04 | Grad: 0.319 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 5920/6501 | Loss: 0.5020 | PPL: 1.68 | LR: 1.73e-04 | Grad: 0.273 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 5930/6501 | Loss: 0.5129 | PPL: 1.70 | LR: 1.73e-04 | Grad: 0.375 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 5940/6501 | Loss: 0.4967 | PPL: 1.67 | LR: 1.73e-04 | Grad: 0.382 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 5950/6501 | Loss: 0.5316 | PPL: 1.73 | LR: 1.72e-04 | Grad: 0.300 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 5960/6501 | Loss: 0.5302 | PPL: 1.72 | LR: 1.72e-04 | Grad: 0.278 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 5970/6501 | Loss: 0.5167 | PPL: 1.70 | LR: 1.72e-04 | Grad: 0.366 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 5980/6501 | Loss: 0.5240 | PPL: 1.71 | LR: 1.71e-04 | Grad: 0.318 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 5990/6501 | Loss: 0.5221 | PPL: 1.71 | LR: 1.71e-04 | Grad: 0.293 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 6000/6501 | Loss: 0.5104 | PPL: 1.68 | LR: 1.70e-04 | Grad: 0.346 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 6010/6501 | Loss: 0.5356 | PPL: 1.75 | LR: 1.70e-04 | Grad: 0.299 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 6020/6501 | Loss: 0.5324 | PPL: 1.75 | LR: 1.70e-04 | Grad: 0.339 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 6030/6501 | Loss: 0.5259 | PPL: 1.74 | LR: 1.69e-04 | Grad: 0.262 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 6040/6501 | Loss: 0.5301 | PPL: 1.76 | LR: 1.69e-04 | Grad: 0.286 | Time: 0.526s
INFO:__main__:Epoch 0 | Step 6050/6501 | Loss: 0.5107 | PPL: 1.73 | LR: 1.69e-04 | Grad: 0.312 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 6060/6501 | Loss: 0.4894 | PPL: 1.66 | LR: 1.68e-04 | Grad: 0.355 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 6070/6501 | Loss: 0.5114 | PPL: 1.71 | LR: 1.68e-04 | Grad: 0.316 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 6080/6501 | Loss: 0.5109 | PPL: 1.70 | LR: 1.68e-04 | Grad: 0.216 | Time: 0.519s
INFO:__main__:Epoch 0 | Step 6090/6501 | Loss: 0.5229 | PPL: 1.72 | LR: 1.67e-04 | Grad: 0.294 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 6100/6501 | Loss: 0.5216 | PPL: 1.71 | LR: 1.67e-04 | Grad: 0.374 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 6110/6501 | Loss: 0.5441 | PPL: 1.76 | LR: 1.66e-04 | Grad: 0.486 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 6120/6501 | Loss: 0.5383 | PPL: 1.75 | LR: 1.66e-04 | Grad: 0.479 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 6130/6501 | Loss: 0.5430 | PPL: 1.75 | LR: 1.66e-04 | Grad: 0.314 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 6140/6501 | Loss: 0.5504 | PPL: 1.77 | LR: 1.65e-04 | Grad: 0.462 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 6150/6501 | Loss: 0.5614 | PPL: 1.79 | LR: 1.65e-04 | Grad: 0.291 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 6160/6501 | Loss: 0.5125 | PPL: 1.70 | LR: 1.65e-04 | Grad: 0.333 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 6170/6501 | Loss: 0.5076 | PPL: 1.68 | LR: 1.64e-04 | Grad: 0.326 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 6180/6501 | Loss: 0.4959 | PPL: 1.66 | LR: 1.64e-04 | Grad: 0.292 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 6190/6501 | Loss: 0.4906 | PPL: 1.65 | LR: 1.64e-04 | Grad: 0.384 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 6200/6501 | Loss: 0.4967 | PPL: 1.66 | LR: 1.63e-04 | Grad: 0.364 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 6210/6501 | Loss: 0.5193 | PPL: 1.70 | LR: 1.63e-04 | Grad: 0.308 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 6220/6501 | Loss: 0.5143 | PPL: 1.69 | LR: 1.62e-04 | Grad: 0.264 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 6230/6501 | Loss: 0.5228 | PPL: 1.70 | LR: 1.62e-04 | Grad: 0.325 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 6240/6501 | Loss: 0.5128 | PPL: 1.69 | LR: 1.62e-04 | Grad: 0.245 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 6250/6501 | Loss: 0.5201 | PPL: 1.70 | LR: 1.61e-04 | Grad: 0.421 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 6260/6501 | Loss: 0.5324 | PPL: 1.73 | LR: 1.61e-04 | Grad: 0.340 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 6270/6501 | Loss: 0.5355 | PPL: 1.73 | LR: 1.61e-04 | Grad: 0.319 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 6280/6501 | Loss: 0.5467 | PPL: 1.76 | LR: 1.60e-04 | Grad: 0.345 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 6290/6501 | Loss: 0.5627 | PPL: 1.78 | LR: 1.60e-04 | Grad: 0.702 | Time: 0.526s
INFO:__main__:Epoch 0 | Step 6300/6501 | Loss: 0.5592 | PPL: 1.78 | LR: 1.60e-04 | Grad: 0.308 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 6310/6501 | Loss: 0.5399 | PPL: 1.74 | LR: 1.59e-04 | Grad: 0.316 | Time: 0.528s
INFO:__main__:Epoch 0 | Step 6320/6501 | Loss: 0.5519 | PPL: 1.76 | LR: 1.59e-04 | Grad: 0.421 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 6330/6501 | Loss: 0.5511 | PPL: 1.76 | LR: 1.59e-04 | Grad: 0.255 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 6340/6501 | Loss: 0.5218 | PPL: 1.71 | LR: 1.58e-04 | Grad: 0.260 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 6350/6501 | Loss: 0.5264 | PPL: 1.72 | LR: 1.58e-04 | Grad: 0.343 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 6360/6501 | Loss: 0.5505 | PPL: 1.76 | LR: 1.57e-04 | Grad: 0.355 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 6370/6501 | Loss: 0.5262 | PPL: 1.72 | LR: 1.57e-04 | Grad: 0.272 | Time: 0.520s
INFO:__main__:Epoch 0 | Step 6380/6501 | Loss: 0.4929 | PPL: 1.66 | LR: 1.57e-04 | Grad: 0.320 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 6390/6501 | Loss: 0.5163 | PPL: 1.70 | LR: 1.56e-04 | Grad: 0.283 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 6400/6501 | Loss: 0.5033 | PPL: 1.68 | LR: 1.56e-04 | Grad: 0.521 | Time: 0.523s
INFO:__main__:Epoch 0 | Step 6410/6501 | Loss: 0.5061 | PPL: 1.69 | LR: 1.56e-04 | Grad: 0.239 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 6420/6501 | Loss: 0.5199 | PPL: 1.71 | LR: 1.55e-04 | Grad: 0.305 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 6430/6501 | Loss: 0.5449 | PPL: 1.75 | LR: 1.55e-04 | Grad: 0.370 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 6440/6501 | Loss: 0.5359 | PPL: 1.75 | LR: 1.55e-04 | Grad: 0.650 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 6450/6501 | Loss: 0.5387 | PPL: 1.75 | LR: 1.54e-04 | Grad: 0.363 | Time: 0.525s
INFO:__main__:Epoch 0 | Step 6460/6501 | Loss: 0.5187 | PPL: 1.71 | LR: 1.54e-04 | Grad: 0.290 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 6470/6501 | Loss: 0.5245 | PPL: 1.72 | LR: 1.53e-04 | Grad: 0.447 | Time: 0.522s
INFO:__main__:Epoch 0 | Step 6480/6501 | Loss: 0.5241 | PPL: 1.72 | LR: 1.53e-04 | Grad: 0.182 | Time: 0.521s
INFO:__main__:Epoch 0 | Step 6490/6501 | Loss: 0.5270 | PPL: 1.72 | LR: 1.53e-04 | Grad: 0.281 | Time: 0.524s
INFO:__main__:Epoch 0 | Step 6500/6501 | Loss: 0.5133 | PPL: 1.70 | LR: 1.52e-04 | Grad: 0.203 | Time: 0.138s
INFO:__main__:📊 Evaluation Results:
INFO:__main__:   Loss: 0.5301
INFO:__main__:   Perplexity: 1.70
INFO:__main__:   Tokens: 2,982,385
INFO:__main__:✅ Epoch 0 completed in 4729.23s | Avg Loss: 0.6070
INFO:__main__:💎 New best checkpoint saved: outputs/lora_experiment/best_checkpoint.pt
INFO:__main__:💾 Checkpoint saved: outputs/lora_experiment/checkpoint_epoch_0_step_0.pt
INFO:__main__:Epoch 1 | Step    0/6501 | Loss: 0.5187 | PPL: 1.71 | LR: 1.52e-04 | Grad: 0.360 | Time: 0.503s
INFO:__main__:Epoch 1 | Step   10/6501 | Loss: 0.5151 | PPL: 1.71 | LR: 1.52e-04 | Grad: 0.277 | Time: 0.522s
INFO:__main__:Epoch 1 | Step   20/6501 | Loss: 0.4960 | PPL: 1.67 | LR: 1.52e-04 | Grad: 0.323 | Time: 0.526s
INFO:__main__:Epoch 1 | Step   30/6501 | Loss: 0.4782 | PPL: 1.64 | LR: 1.51e-04 | Grad: 0.264 | Time: 0.523s
INFO:__main__:Epoch 1 | Step   40/6501 | Loss: 0.4768 | PPL: 1.63 | LR: 1.51e-04 | Grad: 0.327 | Time: 0.524s
INFO:__main__:Epoch 1 | Step   50/6501 | Loss: 0.4831 | PPL: 1.65 | LR: 1.50e-04 | Grad: 0.324 | Time: 0.523s
INFO:__main__:Epoch 1 | Step   60/6501 | Loss: 0.4829 | PPL: 1.64 | LR: 1.50e-04 | Grad: 0.288 | Time: 0.524s
INFO:__main__:Epoch 1 | Step   70/6501 | Loss: 0.4776 | PPL: 1.64 | LR: 1.50e-04 | Grad: 0.377 | Time: 0.522s
INFO:__main__:Epoch 1 | Step   80/6501 | Loss: 0.4730 | PPL: 1.63 | LR: 1.49e-04 | Grad: 0.445 | Time: 0.522s
INFO:__main__:Epoch 1 | Step   90/6501 | Loss: 0.4851 | PPL: 1.65 | LR: 1.49e-04 | Grad: 0.287 | Time: 0.526s
INFO:__main__:Epoch 1 | Step  100/6501 | Loss: 0.4838 | PPL: 1.65 | LR: 1.49e-04 | Grad: 0.422 | Time: 0.518s
INFO:__main__:Epoch 1 | Step  110/6501 | Loss: 0.4744 | PPL: 1.63 | LR: 1.48e-04 | Grad: 0.255 | Time: 0.524s
INFO:__main__:Epoch 1 | Step  120/6501 | Loss: 0.4792 | PPL: 1.63 | LR: 1.48e-04 | Grad: 0.303 | Time: 0.521s
INFO:__main__:Epoch 1 | Step  130/6501 | Loss: 0.4841 | PPL: 1.64 | LR: 1.48e-04 | Grad: 0.345 | Time: 0.524s
INFO:__main__:Epoch 1 | Step  140/6501 | Loss: 0.4746 | PPL: 1.62 | LR: 1.47e-04 | Grad: 0.373 | Time: 0.520s
INFO:__main__:Epoch 1 | Step  150/6501 | Loss: 0.4662 | PPL: 1.61 | LR: 1.47e-04 | Grad: 0.307 | Time: 0.520s
INFO:__main__:Epoch 1 | Step  160/6501 | Loss: 0.4755 | PPL: 1.62 | LR: 1.46e-04 | Grad: 0.279 | Time: 0.520s
INFO:__main__:Epoch 1 | Step  170/6501 | Loss: 0.4761 | PPL: 1.62 | LR: 1.46e-04 | Grad: 0.292 | Time: 0.521s
INFO:__main__:Epoch 1 | Step  180/6501 | Loss: 0.4884 | PPL: 1.65 | LR: 1.46e-04 | Grad: 0.331 | Time: 0.522s
INFO:__main__:Epoch 1 | Step  190/6501 | Loss: 0.4842 | PPL: 1.65 | LR: 1.45e-04 | Grad: 0.279 | Time: 0.526s
INFO:__main__:Epoch 1 | Step  200/6501 | Loss: 0.4950 | PPL: 1.67 | LR: 1.45e-04 | Grad: 0.504 | Time: 0.524s
INFO:__main__:Epoch 1 | Step  210/6501 | Loss: 0.4944 | PPL: 1.66 | LR: 1.45e-04 | Grad: 0.321 | Time: 0.522s
INFO:__main__:Epoch 1 | Step  220/6501 | Loss: 0.4867 | PPL: 1.65 | LR: 1.44e-04 | Grad: 0.267 | Time: 0.520s
INFO:__main__:Epoch 1 | Step  230/6501 | Loss: 0.4817 | PPL: 1.65 | LR: 1.44e-04 | Grad: 0.402 | Time: 0.519s
INFO:__main__:Epoch 1 | Step  240/6501 | Loss: 0.5024 | PPL: 1.68 | LR: 1.44e-04 | Grad: 0.494 | Time: 0.523s
INFO:__main__:Epoch 1 | Step  250/6501 | Loss: 0.4902 | PPL: 1.66 | LR: 1.43e-04 | Grad: 0.213 | Time: 0.518s
INFO:__main__:Epoch 1 | Step  260/6501 | Loss: 0.4833 | PPL: 1.65 | LR: 1.43e-04 | Grad: 0.306 | Time: 0.520s
INFO:__main__:Epoch 1 | Step  270/6501 | Loss: 0.4921 | PPL: 1.66 | LR: 1.42e-04 | Grad: 0.322 | Time: 0.522s
INFO:__main__:Epoch 1 | Step  280/6501 | Loss: 0.4840 | PPL: 1.64 | LR: 1.42e-04 | Grad: 0.254 | Time: 0.523s
INFO:__main__:Epoch 1 | Step  290/6501 | Loss: 0.4643 | PPL: 1.60 | LR: 1.42e-04 | Grad: 0.326 | Time: 0.524s
INFO:__main__:Epoch 1 | Step  300/6501 | Loss: 0.4812 | PPL: 1.63 | LR: 1.41e-04 | Grad: 0.301 | Time: 0.520s
INFO:__main__:Epoch 1 | Step  310/6501 | Loss: 0.4878 | PPL: 1.65 | LR: 1.41e-04 | Grad: 0.285 | Time: 0.520s
INFO:__main__:Epoch 1 | Step  320/6501 | Loss: 0.4763 | PPL: 1.63 | LR: 1.41e-04 | Grad: 0.346 | Time: 0.522s
INFO:__main__:Epoch 1 | Step  330/6501 | Loss: 0.4998 | PPL: 1.67 | LR: 1.40e-04 | Grad: 0.390 | Time: 0.522s
INFO:__main__:Epoch 1 | Step  340/6501 | Loss: 0.4883 | PPL: 1.65 | LR: 1.40e-04 | Grad: 0.460 | Time: 0.521s
INFO:__main__:Epoch 1 | Step  350/6501 | Loss: 0.4744 | PPL: 1.63 | LR: 1.40e-04 | Grad: 0.377 | Time: 0.522s
INFO:__main__:Epoch 1 | Step  360/6501 | Loss: 0.4690 | PPL: 1.62 | LR: 1.39e-04 | Grad: 0.299 | Time: 0.523s
INFO:__main__:Epoch 1 | Step  370/6501 | Loss: 0.4816 | PPL: 1.64 | LR: 1.39e-04 | Grad: 0.312 | Time: 0.525s
INFO:__main__:Epoch 1 | Step  380/6501 | Loss: 0.4752 | PPL: 1.63 | LR: 1.38e-04 | Grad: 0.294 | Time: 0.522s
INFO:__main__:Epoch 1 | Step  390/6501 | Loss: 0.4828 | PPL: 1.64 | LR: 1.38e-04 | Grad: 0.222 | Time: 0.519s
INFO:__main__:Epoch 1 | Step  400/6501 | Loss: 0.4770 | PPL: 1.63 | LR: 1.38e-04 | Grad: 0.296 | Time: 0.524s
INFO:__main__:Epoch 1 | Step  410/6501 | Loss: 0.4879 | PPL: 1.65 | LR: 1.37e-04 | Grad: 0.282 | Time: 0.521s
INFO:__main__:Epoch 1 | Step  420/6501 | Loss: 0.4937 | PPL: 1.66 | LR: 1.37e-04 | Grad: 0.299 | Time: 0.521s
INFO:__main__:Epoch 1 | Step  430/6501 | Loss: 0.4936 | PPL: 1.66 | LR: 1.37e-04 | Grad: 0.339 | Time: 0.523s
INFO:__main__:Epoch 1 | Step  440/6501 | Loss: 0.4981 | PPL: 1.67 | LR: 1.36e-04 | Grad: 0.308 | Time: 0.524s
INFO:__main__:Epoch 1 | Step  450/6501 | Loss: 0.5090 | PPL: 1.70 | LR: 1.36e-04 | Grad: 0.326 | Time: 0.522s
INFO:__main__:Epoch 1 | Step  460/6501 | Loss: 0.5115 | PPL: 1.70 | LR: 1.36e-04 | Grad: 0.289 | Time: 0.521s
INFO:__main__:Epoch 1 | Step  470/6501 | Loss: 0.5191 | PPL: 1.71 | LR: 1.35e-04 | Grad: 0.373 | Time: 0.522s
INFO:__main__:Epoch 1 | Step  480/6501 | Loss: 0.5138 | PPL: 1.70 | LR: 1.35e-04 | Grad: 0.334 | Time: 0.523s
INFO:__main__:Epoch 1 | Step  490/6501 | Loss: 0.4995 | PPL: 1.68 | LR: 1.34e-04 | Grad: 0.312 | Time: 0.519s
INFO:__main__:Epoch 1 | Step  500/6501 | Loss: 0.5001 | PPL: 1.67 | LR: 1.34e-04 | Grad: 0.364 | Time: 0.521s
INFO:__main__:Epoch 1 | Step  510/6501 | Loss: 0.4848 | PPL: 1.65 | LR: 1.34e-04 | Grad: 0.168 | Time: 0.521s
INFO:__main__:Epoch 1 | Step  520/6501 | Loss: 0.4945 | PPL: 1.68 | LR: 1.33e-04 | Grad: 0.332 | Time: 0.520s
INFO:__main__:Epoch 1 | Step  530/6501 | Loss: 0.4991 | PPL: 1.68 | LR: 1.33e-04 | Grad: 0.316 | Time: 0.523s
INFO:__main__:Epoch 1 | Step  540/6501 | Loss: 0.5200 | PPL: 1.72 | LR: 1.33e-04 | Grad: 0.423 | Time: 0.520s
INFO:__main__:Epoch 1 | Step  550/6501 | Loss: 0.5188 | PPL: 1.72 | LR: 1.32e-04 | Grad: 0.214 | Time: 0.520s
INFO:__main__:Epoch 1 | Step  560/6501 | Loss: 0.5066 | PPL: 1.70 | LR: 1.32e-04 | Grad: 0.215 | Time: 0.522s
INFO:__main__:Epoch 1 | Step  570/6501 | Loss: 0.4839 | PPL: 1.65 | LR: 1.32e-04 | Grad: 0.513 | Time: 0.521s
INFO:__main__:Epoch 1 | Step  580/6501 | Loss: 0.4770 | PPL: 1.64 | LR: 1.31e-04 | Grad: 0.327 | Time: 0.522s
INFO:__main__:Epoch 1 | Step  590/6501 | Loss: 0.4569 | PPL: 1.60 | LR: 1.31e-04 | Grad: 0.232 | Time: 0.518s
INFO:__main__:Epoch 1 | Step  600/6501 | Loss: 0.4580 | PPL: 1.60 | LR: 1.31e-04 | Grad: 0.364 | Time: 0.521s
INFO:__main__:Epoch 1 | Step  610/6501 | Loss: 0.4741 | PPL: 1.63 | LR: 1.30e-04 | Grad: 0.321 | Time: 0.523s
INFO:__main__:Epoch 1 | Step  620/6501 | Loss: 0.4736 | PPL: 1.63 | LR: 1.30e-04 | Grad: 0.328 | Time: 0.522s
INFO:__main__:Epoch 1 | Step  630/6501 | Loss: 0.4799 | PPL: 1.64 | LR: 1.29e-04 | Grad: 0.344 | Time: 0.524s
INFO:__main__:Epoch 1 | Step  640/6501 | Loss: 0.4915 | PPL: 1.66 | LR: 1.29e-04 | Grad: 0.299 | Time: 0.524s
INFO:__main__:Epoch 1 | Step  650/6501 | Loss: 0.4664 | PPL: 1.62 | LR: 1.29e-04 | Grad: 0.242 | Time: 0.520s
INFO:__main__:Epoch 1 | Step  660/6501 | Loss: 0.4736 | PPL: 1.63 | LR: 1.28e-04 | Grad: 0.326 | Time: 0.521s
INFO:__main__:Epoch 1 | Step  670/6501 | Loss: 0.4669 | PPL: 1.62 | LR: 1.28e-04 | Grad: 0.429 | Time: 0.525s
INFO:__main__:Epoch 1 | Step  680/6501 | Loss: 0.4718 | PPL: 1.63 | LR: 1.28e-04 | Grad: 0.405 | Time: 0.521s
INFO:__main__:Epoch 1 | Step  690/6501 | Loss: 0.4736 | PPL: 1.63 | LR: 1.27e-04 | Grad: 0.261 | Time: 0.522s
INFO:__main__:Epoch 1 | Step  700/6501 | Loss: 0.5032 | PPL: 1.68 | LR: 1.27e-04 | Grad: 0.337 | Time: 0.522s
INFO:__main__:Epoch 1 | Step  710/6501 | Loss: 0.5145 | PPL: 1.70 | LR: 1.27e-04 | Grad: 0.304 | Time: 0.522s
INFO:__main__:Epoch 1 | Step  720/6501 | Loss: 0.5204 | PPL: 1.71 | LR: 1.26e-04 | Grad: 0.333 | Time: 0.522s
INFO:__main__:Epoch 1 | Step  730/6501 | Loss: 0.4864 | PPL: 1.65 | LR: 1.26e-04 | Grad: 0.302 | Time: 0.521s
INFO:__main__:Epoch 1 | Step  740/6501 | Loss: 0.4821 | PPL: 1.64 | LR: 1.25e-04 | Grad: 0.301 | Time: 0.520s
INFO:__main__:Epoch 1 | Step  750/6501 | Loss: 0.5015 | PPL: 1.68 | LR: 1.25e-04 | Grad: 0.394 | Time: 0.521s
INFO:__main__:Epoch 1 | Step  760/6501 | Loss: 0.4963 | PPL: 1.67 | LR: 1.25e-04 | Grad: 0.420 | Time: 0.522s
INFO:__main__:Epoch 1 | Step  770/6501 | Loss: 0.4798 | PPL: 1.65 | LR: 1.24e-04 | Grad: 0.209 | Time: 0.520s
INFO:__main__:Epoch 1 | Step  780/6501 | Loss: 0.4775 | PPL: 1.65 | LR: 1.24e-04 | Grad: 0.294 | Time: 0.518s
INFO:__main__:Epoch 1 | Step  790/6501 | Loss: 0.4729 | PPL: 1.64 | LR: 1.24e-04 | Grad: 0.466 | Time: 0.521s
INFO:__main__:Epoch 1 | Step  800/6501 | Loss: 0.4522 | PPL: 1.60 | LR: 1.23e-04 | Grad: 0.351 | Time: 0.520s
INFO:__main__:Epoch 1 | Step  810/6501 | Loss: 0.4345 | PPL: 1.56 | LR: 1.23e-04 | Grad: 0.285 | Time: 0.520s
INFO:__main__:Epoch 1 | Step  820/6501 | Loss: 0.4517 | PPL: 1.59 | LR: 1.23e-04 | Grad: 0.414 | Time: 0.524s
INFO:__main__:Epoch 1 | Step  830/6501 | Loss: 0.4887 | PPL: 1.65 | LR: 1.22e-04 | Grad: 0.456 | Time: 0.523s
INFO:__main__:Epoch 1 | Step  840/6501 | Loss: 0.4856 | PPL: 1.65 | LR: 1.22e-04 | Grad: 0.417 | Time: 0.522s
INFO:__main__:Epoch 1 | Step  850/6501 | Loss: 0.4863 | PPL: 1.65 | LR: 1.22e-04 | Grad: 0.444 | Time: 0.527s
INFO:__main__:Epoch 1 | Step  860/6501 | Loss: 0.5052 | PPL: 1.68 | LR: 1.21e-04 | Grad: 0.314 | Time: 0.521s
INFO:__main__:Epoch 1 | Step  870/6501 | Loss: 0.5139 | PPL: 1.70 | LR: 1.21e-04 | Grad: 0.294 | Time: 0.523s
INFO:__main__:Epoch 1 | Step  880/6501 | Loss: 0.5145 | PPL: 1.70 | LR: 1.20e-04 | Grad: 0.329 | Time: 0.522s
INFO:__main__:Epoch 1 | Step  890/6501 | Loss: 0.5325 | PPL: 1.73 | LR: 1.20e-04 | Grad: 0.361 | Time: 0.527s
INFO:__main__:Epoch 1 | Step  900/6501 | Loss: 0.5226 | PPL: 1.72 | LR: 1.20e-04 | Grad: 0.439 | Time: 0.522s
INFO:__main__:Epoch 1 | Step  910/6501 | Loss: 0.5275 | PPL: 1.73 | LR: 1.19e-04 | Grad: 0.379 | Time: 0.523s
INFO:__main__:Epoch 1 | Step  920/6501 | Loss: 0.5008 | PPL: 1.69 | LR: 1.19e-04 | Grad: 0.300 | Time: 0.521s
INFO:__main__:Epoch 1 | Step  930/6501 | Loss: 0.5049 | PPL: 1.70 | LR: 1.19e-04 | Grad: 0.404 | Time: 0.523s
INFO:__main__:Epoch 1 | Step  940/6501 | Loss: 0.4922 | PPL: 1.68 | LR: 1.18e-04 | Grad: 0.226 | Time: 0.519s
INFO:__main__:Epoch 1 | Step  950/6501 | Loss: 0.5184 | PPL: 1.72 | LR: 1.18e-04 | Grad: 0.453 | Time: 0.522s
INFO:__main__:Epoch 1 | Step  960/6501 | Loss: 0.5043 | PPL: 1.70 | LR: 1.18e-04 | Grad: 0.354 | Time: 0.523s
INFO:__main__:Epoch 1 | Step  970/6501 | Loss: 0.5137 | PPL: 1.72 | LR: 1.17e-04 | Grad: 0.376 | Time: 0.521s
INFO:__main__:Epoch 1 | Step  980/6501 | Loss: 0.4913 | PPL: 1.66 | LR: 1.17e-04 | Grad: 0.369 | Time: 0.520s
INFO:__main__:Epoch 1 | Step  990/6501 | Loss: 0.4908 | PPL: 1.67 | LR: 1.17e-04 | Grad: 0.339 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 1000/6501 | Loss: 0.4917 | PPL: 1.67 | LR: 1.16e-04 | Grad: 0.337 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 1010/6501 | Loss: 0.4944 | PPL: 1.67 | LR: 1.16e-04 | Grad: 0.424 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 1020/6501 | Loss: 0.4944 | PPL: 1.67 | LR: 1.15e-04 | Grad: 0.346 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 1030/6501 | Loss: 0.4710 | PPL: 1.63 | LR: 1.15e-04 | Grad: 0.287 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 1040/6501 | Loss: 0.4922 | PPL: 1.66 | LR: 1.15e-04 | Grad: 0.435 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 1050/6501 | Loss: 0.4719 | PPL: 1.62 | LR: 1.14e-04 | Grad: 0.263 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 1060/6501 | Loss: 0.4668 | PPL: 1.61 | LR: 1.14e-04 | Grad: 0.296 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 1070/6501 | Loss: 0.4810 | PPL: 1.64 | LR: 1.14e-04 | Grad: 0.370 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 1080/6501 | Loss: 0.5109 | PPL: 1.69 | LR: 1.13e-04 | Grad: 0.383 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 1090/6501 | Loss: 0.4989 | PPL: 1.67 | LR: 1.13e-04 | Grad: 0.259 | Time: 0.525s
INFO:__main__:Epoch 1 | Step 1100/6501 | Loss: 0.5095 | PPL: 1.70 | LR: 1.13e-04 | Grad: 0.296 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 1110/6501 | Loss: 0.5111 | PPL: 1.71 | LR: 1.12e-04 | Grad: 0.301 | Time: 0.526s
INFO:__main__:Epoch 1 | Step 1120/6501 | Loss: 0.5008 | PPL: 1.69 | LR: 1.12e-04 | Grad: 0.266 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 1130/6501 | Loss: 0.4887 | PPL: 1.67 | LR: 1.12e-04 | Grad: 0.193 | Time: 0.519s
INFO:__main__:Epoch 1 | Step 1140/6501 | Loss: 0.4736 | PPL: 1.64 | LR: 1.11e-04 | Grad: 0.330 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 1150/6501 | Loss: 0.4525 | PPL: 1.60 | LR: 1.11e-04 | Grad: 0.369 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 1160/6501 | Loss: 0.4825 | PPL: 1.66 | LR: 1.11e-04 | Grad: 0.408 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 1170/6501 | Loss: 0.4865 | PPL: 1.67 | LR: 1.10e-04 | Grad: 0.349 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 1180/6501 | Loss: 0.4932 | PPL: 1.69 | LR: 1.10e-04 | Grad: 0.376 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 1190/6501 | Loss: 0.5027 | PPL: 1.70 | LR: 1.09e-04 | Grad: 0.547 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 1200/6501 | Loss: 0.4926 | PPL: 1.68 | LR: 1.09e-04 | Grad: 0.501 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 1210/6501 | Loss: 0.4602 | PPL: 1.61 | LR: 1.09e-04 | Grad: 0.290 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 1220/6501 | Loss: 0.4590 | PPL: 1.61 | LR: 1.08e-04 | Grad: 0.357 | Time: 0.526s
INFO:__main__:Epoch 1 | Step 1230/6501 | Loss: 0.4514 | PPL: 1.59 | LR: 1.08e-04 | Grad: 0.485 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 1240/6501 | Loss: 0.4855 | PPL: 1.66 | LR: 1.08e-04 | Grad: 0.495 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 1250/6501 | Loss: 0.4988 | PPL: 1.68 | LR: 1.07e-04 | Grad: 0.389 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 1260/6501 | Loss: 0.4978 | PPL: 1.68 | LR: 1.07e-04 | Grad: 0.289 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 1270/6501 | Loss: 0.4771 | PPL: 1.64 | LR: 1.07e-04 | Grad: 0.366 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 1280/6501 | Loss: 0.4915 | PPL: 1.67 | LR: 1.06e-04 | Grad: 0.277 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 1290/6501 | Loss: 0.4558 | PPL: 1.61 | LR: 1.06e-04 | Grad: 0.369 | Time: 0.525s
INFO:__main__:Epoch 1 | Step 1300/6501 | Loss: 0.4825 | PPL: 1.65 | LR: 1.06e-04 | Grad: 0.740 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 1310/6501 | Loss: 0.4488 | PPL: 1.60 | LR: 1.05e-04 | Grad: 0.240 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 1320/6501 | Loss: 0.4597 | PPL: 1.62 | LR: 1.05e-04 | Grad: 0.322 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 1330/6501 | Loss: 0.4562 | PPL: 1.61 | LR: 1.05e-04 | Grad: 0.397 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 1340/6501 | Loss: 0.4486 | PPL: 1.59 | LR: 1.04e-04 | Grad: 0.228 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 1350/6501 | Loss: 0.4201 | PPL: 1.54 | LR: 1.04e-04 | Grad: 0.406 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 1360/6501 | Loss: 0.4474 | PPL: 1.58 | LR: 1.04e-04 | Grad: 0.242 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 1370/6501 | Loss: 0.4676 | PPL: 1.62 | LR: 1.03e-04 | Grad: 0.535 | Time: 0.526s
INFO:__main__:Epoch 1 | Step 1380/6501 | Loss: 0.4772 | PPL: 1.63 | LR: 1.03e-04 | Grad: 0.385 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 1390/6501 | Loss: 0.4845 | PPL: 1.65 | LR: 1.03e-04 | Grad: 0.297 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 1400/6501 | Loss: 0.5054 | PPL: 1.68 | LR: 1.02e-04 | Grad: 0.400 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 1410/6501 | Loss: 0.5162 | PPL: 1.70 | LR: 1.02e-04 | Grad: 0.452 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 1420/6501 | Loss: 0.5071 | PPL: 1.69 | LR: 1.02e-04 | Grad: 0.400 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 1430/6501 | Loss: 0.4810 | PPL: 1.64 | LR: 1.01e-04 | Grad: 0.365 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 1440/6501 | Loss: 0.4756 | PPL: 1.63 | LR: 1.01e-04 | Grad: 0.311 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 1450/6501 | Loss: 0.4526 | PPL: 1.60 | LR: 1.00e-04 | Grad: 0.369 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 1460/6501 | Loss: 0.4576 | PPL: 1.60 | LR: 1.00e-04 | Grad: 0.364 | Time: 0.519s
INFO:__main__:Epoch 1 | Step 1470/6501 | Loss: 0.4606 | PPL: 1.60 | LR: 9.98e-05 | Grad: 0.404 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 1480/6501 | Loss: 0.4712 | PPL: 1.62 | LR: 9.95e-05 | Grad: 0.382 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 1490/6501 | Loss: 0.5074 | PPL: 1.70 | LR: 9.91e-05 | Grad: 0.647 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 1500/6501 | Loss: 0.5074 | PPL: 1.70 | LR: 9.88e-05 | Grad: 0.464 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 1510/6501 | Loss: 0.5000 | PPL: 1.69 | LR: 9.84e-05 | Grad: 0.442 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 1520/6501 | Loss: 0.5069 | PPL: 1.70 | LR: 9.81e-05 | Grad: 0.334 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 1530/6501 | Loss: 0.5315 | PPL: 1.75 | LR: 9.77e-05 | Grad: 0.332 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 1540/6501 | Loss: 0.5016 | PPL: 1.68 | LR: 9.74e-05 | Grad: 0.336 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 1550/6501 | Loss: 0.5235 | PPL: 1.72 | LR: 9.71e-05 | Grad: 0.312 | Time: 0.519s
INFO:__main__:Epoch 1 | Step 1560/6501 | Loss: 0.5143 | PPL: 1.69 | LR: 9.67e-05 | Grad: 0.277 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 1570/6501 | Loss: 0.5183 | PPL: 1.70 | LR: 9.64e-05 | Grad: 0.407 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 1580/6501 | Loss: 0.5101 | PPL: 1.69 | LR: 9.60e-05 | Grad: 0.360 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 1590/6501 | Loss: 0.4962 | PPL: 1.67 | LR: 9.57e-05 | Grad: 0.190 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 1600/6501 | Loss: 0.4934 | PPL: 1.66 | LR: 9.54e-05 | Grad: 0.304 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 1610/6501 | Loss: 0.5021 | PPL: 1.68 | LR: 9.50e-05 | Grad: 0.335 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 1620/6501 | Loss: 0.4936 | PPL: 1.67 | LR: 9.47e-05 | Grad: 0.315 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 1630/6501 | Loss: 0.4596 | PPL: 1.61 | LR: 9.44e-05 | Grad: 0.223 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 1640/6501 | Loss: 0.4539 | PPL: 1.60 | LR: 9.40e-05 | Grad: 0.302 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 1650/6501 | Loss: 0.4517 | PPL: 1.60 | LR: 9.37e-05 | Grad: 0.403 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 1660/6501 | Loss: 0.4547 | PPL: 1.61 | LR: 9.33e-05 | Grad: 0.374 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 1670/6501 | Loss: 0.4242 | PPL: 1.56 | LR: 9.30e-05 | Grad: 0.246 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 1680/6501 | Loss: 0.4448 | PPL: 1.59 | LR: 9.27e-05 | Grad: 0.261 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 1690/6501 | Loss: 0.4586 | PPL: 1.61 | LR: 9.23e-05 | Grad: 0.255 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 1700/6501 | Loss: 0.4736 | PPL: 1.64 | LR: 9.20e-05 | Grad: 0.363 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 1710/6501 | Loss: 0.4695 | PPL: 1.63 | LR: 9.17e-05 | Grad: 0.361 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 1720/6501 | Loss: 0.4934 | PPL: 1.66 | LR: 9.13e-05 | Grad: 0.426 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 1730/6501 | Loss: 0.5056 | PPL: 1.68 | LR: 9.10e-05 | Grad: 0.432 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 1740/6501 | Loss: 0.4972 | PPL: 1.67 | LR: 9.07e-05 | Grad: 0.385 | Time: 0.528s
INFO:__main__:Epoch 1 | Step 1750/6501 | Loss: 0.4826 | PPL: 1.64 | LR: 9.03e-05 | Grad: 0.444 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 1760/6501 | Loss: 0.4778 | PPL: 1.63 | LR: 9.00e-05 | Grad: 0.239 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 1770/6501 | Loss: 0.4759 | PPL: 1.63 | LR: 8.97e-05 | Grad: 0.304 | Time: 0.525s
INFO:__main__:Epoch 1 | Step 1780/6501 | Loss: 0.4549 | PPL: 1.59 | LR: 8.93e-05 | Grad: 0.345 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 1790/6501 | Loss: 0.4607 | PPL: 1.61 | LR: 8.90e-05 | Grad: 0.495 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 1800/6501 | Loss: 0.4410 | PPL: 1.57 | LR: 8.87e-05 | Grad: 0.238 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 1810/6501 | Loss: 0.4578 | PPL: 1.60 | LR: 8.83e-05 | Grad: 0.418 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 1820/6501 | Loss: 0.4536 | PPL: 1.60 | LR: 8.80e-05 | Grad: 0.448 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 1830/6501 | Loss: 0.4406 | PPL: 1.58 | LR: 8.77e-05 | Grad: 0.233 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 1840/6501 | Loss: 0.4342 | PPL: 1.56 | LR: 8.73e-05 | Grad: 0.351 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 1850/6501 | Loss: 0.4501 | PPL: 1.59 | LR: 8.70e-05 | Grad: 0.329 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 1860/6501 | Loss: 0.4483 | PPL: 1.59 | LR: 8.67e-05 | Grad: 0.463 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 1870/6501 | Loss: 0.4486 | PPL: 1.59 | LR: 8.64e-05 | Grad: 0.300 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 1880/6501 | Loss: 0.4918 | PPL: 1.66 | LR: 8.60e-05 | Grad: 0.359 | Time: 0.525s
INFO:__main__:Epoch 1 | Step 1890/6501 | Loss: 0.5231 | PPL: 1.72 | LR: 8.57e-05 | Grad: 0.341 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 1900/6501 | Loss: 0.5372 | PPL: 1.74 | LR: 8.54e-05 | Grad: 0.310 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 1910/6501 | Loss: 0.5283 | PPL: 1.73 | LR: 8.50e-05 | Grad: 0.663 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 1920/6501 | Loss: 0.5315 | PPL: 1.73 | LR: 8.47e-05 | Grad: 0.303 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 1930/6501 | Loss: 0.5119 | PPL: 1.70 | LR: 8.44e-05 | Grad: 0.489 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 1940/6501 | Loss: 0.4747 | PPL: 1.63 | LR: 8.41e-05 | Grad: 0.277 | Time: 0.518s
INFO:__main__:Epoch 1 | Step 1950/6501 | Loss: 0.4608 | PPL: 1.62 | LR: 8.37e-05 | Grad: 0.405 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 1960/6501 | Loss: 0.4577 | PPL: 1.61 | LR: 8.34e-05 | Grad: 0.241 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 1970/6501 | Loss: 0.4686 | PPL: 1.64 | LR: 8.31e-05 | Grad: 0.665 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 1980/6501 | Loss: 0.4657 | PPL: 1.63 | LR: 8.28e-05 | Grad: 0.503 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 1990/6501 | Loss: 0.4757 | PPL: 1.65 | LR: 8.24e-05 | Grad: 0.191 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 2000/6501 | Loss: 0.4461 | PPL: 1.60 | LR: 8.21e-05 | Grad: 0.307 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 2010/6501 | Loss: 0.4681 | PPL: 1.63 | LR: 8.18e-05 | Grad: 0.286 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 2020/6501 | Loss: 0.4629 | PPL: 1.61 | LR: 8.15e-05 | Grad: 0.316 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 2030/6501 | Loss: 0.4621 | PPL: 1.61 | LR: 8.12e-05 | Grad: 0.398 | Time: 0.526s
INFO:__main__:Epoch 1 | Step 2040/6501 | Loss: 0.4721 | PPL: 1.63 | LR: 8.08e-05 | Grad: 0.283 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 2050/6501 | Loss: 0.4986 | PPL: 1.67 | LR: 8.05e-05 | Grad: 0.457 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 2060/6501 | Loss: 0.4801 | PPL: 1.64 | LR: 8.02e-05 | Grad: 0.532 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 2070/6501 | Loss: 0.4682 | PPL: 1.62 | LR: 7.99e-05 | Grad: 0.415 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 2080/6501 | Loss: 0.4700 | PPL: 1.62 | LR: 7.95e-05 | Grad: 0.288 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 2090/6501 | Loss: 0.4565 | PPL: 1.60 | LR: 7.92e-05 | Grad: 0.417 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 2100/6501 | Loss: 0.4604 | PPL: 1.61 | LR: 7.89e-05 | Grad: 0.381 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 2110/6501 | Loss: 0.4496 | PPL: 1.58 | LR: 7.86e-05 | Grad: 0.322 | Time: 0.519s
INFO:__main__:Epoch 1 | Step 2120/6501 | Loss: 0.4653 | PPL: 1.61 | LR: 7.83e-05 | Grad: 0.338 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 2130/6501 | Loss: 0.4632 | PPL: 1.61 | LR: 7.79e-05 | Grad: 0.427 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 2140/6501 | Loss: 0.4725 | PPL: 1.62 | LR: 7.76e-05 | Grad: 0.249 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 2150/6501 | Loss: 0.4696 | PPL: 1.62 | LR: 7.73e-05 | Grad: 0.443 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 2160/6501 | Loss: 0.4779 | PPL: 1.63 | LR: 7.70e-05 | Grad: 0.335 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 2170/6501 | Loss: 0.4940 | PPL: 1.66 | LR: 7.67e-05 | Grad: 0.503 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 2180/6501 | Loss: 0.5066 | PPL: 1.69 | LR: 7.64e-05 | Grad: 0.459 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 2190/6501 | Loss: 0.4937 | PPL: 1.67 | LR: 7.60e-05 | Grad: 0.460 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 2200/6501 | Loss: 0.4888 | PPL: 1.66 | LR: 7.57e-05 | Grad: 0.399 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 2210/6501 | Loss: 0.4735 | PPL: 1.64 | LR: 7.54e-05 | Grad: 0.300 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 2220/6501 | Loss: 0.4518 | PPL: 1.60 | LR: 7.51e-05 | Grad: 0.310 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 2230/6501 | Loss: 0.4359 | PPL: 1.57 | LR: 7.48e-05 | Grad: 0.247 | Time: 0.528s
INFO:__main__:Epoch 1 | Step 2240/6501 | Loss: 0.4476 | PPL: 1.58 | LR: 7.45e-05 | Grad: 0.346 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 2250/6501 | Loss: 0.4523 | PPL: 1.59 | LR: 7.42e-05 | Grad: 0.401 | Time: 0.519s
INFO:__main__:Epoch 1 | Step 2260/6501 | Loss: 0.4767 | PPL: 1.63 | LR: 7.38e-05 | Grad: 0.285 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 2270/6501 | Loss: 0.4975 | PPL: 1.67 | LR: 7.35e-05 | Grad: 0.392 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 2280/6501 | Loss: 0.5056 | PPL: 1.69 | LR: 7.32e-05 | Grad: 0.380 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 2290/6501 | Loss: 0.5151 | PPL: 1.71 | LR: 7.29e-05 | Grad: 0.440 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 2300/6501 | Loss: 0.5285 | PPL: 1.73 | LR: 7.26e-05 | Grad: 0.409 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 2310/6501 | Loss: 0.5203 | PPL: 1.71 | LR: 7.23e-05 | Grad: 0.551 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 2320/6501 | Loss: 0.5117 | PPL: 1.69 | LR: 7.20e-05 | Grad: 0.437 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 2330/6501 | Loss: 0.5047 | PPL: 1.68 | LR: 7.17e-05 | Grad: 0.403 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 2340/6501 | Loss: 0.5001 | PPL: 1.67 | LR: 7.14e-05 | Grad: 0.454 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 2350/6501 | Loss: 0.4748 | PPL: 1.63 | LR: 7.11e-05 | Grad: 0.330 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 2360/6501 | Loss: 0.4674 | PPL: 1.62 | LR: 7.07e-05 | Grad: 0.263 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 2370/6501 | Loss: 0.4566 | PPL: 1.60 | LR: 7.04e-05 | Grad: 0.397 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 2380/6501 | Loss: 0.4559 | PPL: 1.60 | LR: 7.01e-05 | Grad: 0.330 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 2390/6501 | Loss: 0.4608 | PPL: 1.61 | LR: 6.98e-05 | Grad: 0.292 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 2400/6501 | Loss: 0.4617 | PPL: 1.60 | LR: 6.95e-05 | Grad: 0.290 | Time: 0.525s
INFO:__main__:Epoch 1 | Step 2410/6501 | Loss: 0.4645 | PPL: 1.61 | LR: 6.92e-05 | Grad: 0.375 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 2420/6501 | Loss: 0.4418 | PPL: 1.57 | LR: 6.89e-05 | Grad: 0.331 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 2430/6501 | Loss: 0.4319 | PPL: 1.55 | LR: 6.86e-05 | Grad: 0.336 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 2440/6501 | Loss: 0.4407 | PPL: 1.57 | LR: 6.83e-05 | Grad: 0.396 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 2450/6501 | Loss: 0.4372 | PPL: 1.56 | LR: 6.80e-05 | Grad: 0.400 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 2460/6501 | Loss: 0.4417 | PPL: 1.57 | LR: 6.77e-05 | Grad: 0.400 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 2470/6501 | Loss: 0.4564 | PPL: 1.60 | LR: 6.74e-05 | Grad: 0.374 | Time: 0.525s
INFO:__main__:Epoch 1 | Step 2480/6501 | Loss: 0.4656 | PPL: 1.62 | LR: 6.71e-05 | Grad: 0.283 | Time: 0.519s
INFO:__main__:Epoch 1 | Step 2490/6501 | Loss: 0.4417 | PPL: 1.58 | LR: 6.68e-05 | Grad: 0.216 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 2500/6501 | Loss: 0.4859 | PPL: 1.66 | LR: 6.65e-05 | Grad: 0.291 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 2510/6501 | Loss: 0.4737 | PPL: 1.64 | LR: 6.62e-05 | Grad: 0.324 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 2520/6501 | Loss: 0.4723 | PPL: 1.63 | LR: 6.59e-05 | Grad: 0.402 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 2530/6501 | Loss: 0.4697 | PPL: 1.63 | LR: 6.56e-05 | Grad: 0.273 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 2540/6501 | Loss: 0.4846 | PPL: 1.65 | LR: 6.53e-05 | Grad: 0.306 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 2550/6501 | Loss: 0.4433 | PPL: 1.57 | LR: 6.50e-05 | Grad: 0.400 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 2560/6501 | Loss: 0.4478 | PPL: 1.58 | LR: 6.47e-05 | Grad: 0.309 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 2570/6501 | Loss: 0.4539 | PPL: 1.60 | LR: 6.44e-05 | Grad: 0.232 | Time: 0.527s
INFO:__main__:Epoch 1 | Step 2580/6501 | Loss: 0.4520 | PPL: 1.59 | LR: 6.41e-05 | Grad: 0.367 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 2590/6501 | Loss: 0.4689 | PPL: 1.62 | LR: 6.38e-05 | Grad: 0.377 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 2600/6501 | Loss: 0.4843 | PPL: 1.65 | LR: 6.35e-05 | Grad: 0.312 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 2610/6501 | Loss: 0.5058 | PPL: 1.68 | LR: 6.32e-05 | Grad: 0.439 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 2620/6501 | Loss: 0.4953 | PPL: 1.66 | LR: 6.29e-05 | Grad: 0.303 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 2630/6501 | Loss: 0.5052 | PPL: 1.68 | LR: 6.26e-05 | Grad: 0.331 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 2640/6501 | Loss: 0.5035 | PPL: 1.68 | LR: 6.23e-05 | Grad: 0.391 | Time: 0.519s
INFO:__main__:Epoch 1 | Step 2650/6501 | Loss: 0.4973 | PPL: 1.66 | LR: 6.20e-05 | Grad: 0.331 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 2660/6501 | Loss: 0.5146 | PPL: 1.70 | LR: 6.17e-05 | Grad: 0.240 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 2670/6501 | Loss: 0.5254 | PPL: 1.72 | LR: 6.14e-05 | Grad: 0.375 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 2680/6501 | Loss: 0.5258 | PPL: 1.72 | LR: 6.11e-05 | Grad: 0.326 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 2690/6501 | Loss: 0.5163 | PPL: 1.71 | LR: 6.09e-05 | Grad: 0.235 | Time: 0.519s
INFO:__main__:Epoch 1 | Step 2700/6501 | Loss: 0.5186 | PPL: 1.72 | LR: 6.06e-05 | Grad: 0.378 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 2710/6501 | Loss: 0.5130 | PPL: 1.71 | LR: 6.03e-05 | Grad: 0.342 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 2720/6501 | Loss: 0.5009 | PPL: 1.69 | LR: 6.00e-05 | Grad: 0.420 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 2730/6501 | Loss: 0.4981 | PPL: 1.69 | LR: 5.97e-05 | Grad: 0.242 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 2740/6501 | Loss: 0.4953 | PPL: 1.67 | LR: 5.94e-05 | Grad: 0.428 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 2750/6501 | Loss: 0.4968 | PPL: 1.68 | LR: 5.91e-05 | Grad: 0.390 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 2760/6501 | Loss: 0.4671 | PPL: 1.62 | LR: 5.88e-05 | Grad: 0.286 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 2770/6501 | Loss: 0.4931 | PPL: 1.66 | LR: 5.85e-05 | Grad: 0.263 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 2780/6501 | Loss: 0.4874 | PPL: 1.65 | LR: 5.83e-05 | Grad: 0.282 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 2790/6501 | Loss: 0.4830 | PPL: 1.64 | LR: 5.80e-05 | Grad: 0.336 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 2800/6501 | Loss: 0.4879 | PPL: 1.65 | LR: 5.77e-05 | Grad: 0.469 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 2810/6501 | Loss: 0.4853 | PPL: 1.64 | LR: 5.74e-05 | Grad: 0.313 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 2820/6501 | Loss: 0.4631 | PPL: 1.60 | LR: 5.71e-05 | Grad: 0.277 | Time: 0.519s
INFO:__main__:Epoch 1 | Step 2830/6501 | Loss: 0.4574 | PPL: 1.60 | LR: 5.68e-05 | Grad: 0.339 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 2840/6501 | Loss: 0.4516 | PPL: 1.59 | LR: 5.65e-05 | Grad: 0.421 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 2850/6501 | Loss: 0.4616 | PPL: 1.61 | LR: 5.63e-05 | Grad: 0.332 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 2860/6501 | Loss: 0.4677 | PPL: 1.62 | LR: 5.60e-05 | Grad: 0.370 | Time: 0.525s
INFO:__main__:Epoch 1 | Step 2870/6501 | Loss: 0.4857 | PPL: 1.66 | LR: 5.57e-05 | Grad: 0.292 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 2880/6501 | Loss: 0.4897 | PPL: 1.66 | LR: 5.54e-05 | Grad: 0.309 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 2890/6501 | Loss: 0.4963 | PPL: 1.67 | LR: 5.51e-05 | Grad: 0.518 | Time: 0.519s
INFO:__main__:Epoch 1 | Step 2900/6501 | Loss: 0.4840 | PPL: 1.64 | LR: 5.49e-05 | Grad: 0.334 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 2910/6501 | Loss: 0.4938 | PPL: 1.66 | LR: 5.46e-05 | Grad: 0.331 | Time: 0.525s
INFO:__main__:Epoch 1 | Step 2920/6501 | Loss: 0.4807 | PPL: 1.63 | LR: 5.43e-05 | Grad: 0.418 | Time: 0.525s
INFO:__main__:Epoch 1 | Step 2930/6501 | Loss: 0.4894 | PPL: 1.64 | LR: 5.40e-05 | Grad: 0.325 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 2940/6501 | Loss: 0.4922 | PPL: 1.65 | LR: 5.37e-05 | Grad: 0.308 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 2950/6501 | Loss: 0.4615 | PPL: 1.61 | LR: 5.35e-05 | Grad: 0.329 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 2960/6501 | Loss: 0.4444 | PPL: 1.58 | LR: 5.32e-05 | Grad: 0.368 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 2970/6501 | Loss: 0.4435 | PPL: 1.58 | LR: 5.29e-05 | Grad: 0.268 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 2980/6501 | Loss: 0.4471 | PPL: 1.58 | LR: 5.26e-05 | Grad: 0.421 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 2990/6501 | Loss: 0.4725 | PPL: 1.62 | LR: 5.24e-05 | Grad: 0.527 | Time: 0.519s
INFO:__main__:Epoch 1 | Step 3000/6501 | Loss: 0.4832 | PPL: 1.64 | LR: 5.21e-05 | Grad: 0.340 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 3010/6501 | Loss: 0.4745 | PPL: 1.62 | LR: 5.18e-05 | Grad: 0.286 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 3020/6501 | Loss: 0.4761 | PPL: 1.63 | LR: 5.15e-05 | Grad: 0.347 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 3030/6501 | Loss: 0.4795 | PPL: 1.64 | LR: 5.13e-05 | Grad: 0.258 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 3040/6501 | Loss: 0.4487 | PPL: 1.59 | LR: 5.10e-05 | Grad: 0.238 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 3050/6501 | Loss: 0.4491 | PPL: 1.59 | LR: 5.07e-05 | Grad: 0.344 | Time: 0.518s
INFO:__main__:Epoch 1 | Step 3060/6501 | Loss: 0.4603 | PPL: 1.61 | LR: 5.05e-05 | Grad: 0.343 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 3070/6501 | Loss: 0.4488 | PPL: 1.59 | LR: 5.02e-05 | Grad: 0.355 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 3080/6501 | Loss: 0.4406 | PPL: 1.57 | LR: 4.99e-05 | Grad: 0.410 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 3090/6501 | Loss: 0.4283 | PPL: 1.55 | LR: 4.96e-05 | Grad: 0.500 | Time: 0.528s
INFO:__main__:Epoch 1 | Step 3100/6501 | Loss: 0.4581 | PPL: 1.61 | LR: 4.94e-05 | Grad: 0.334 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 3110/6501 | Loss: 0.4588 | PPL: 1.61 | LR: 4.91e-05 | Grad: 0.331 | Time: 0.527s
INFO:__main__:Epoch 1 | Step 3120/6501 | Loss: 0.4690 | PPL: 1.63 | LR: 4.88e-05 | Grad: 0.297 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 3130/6501 | Loss: 0.4879 | PPL: 1.66 | LR: 4.86e-05 | Grad: 0.461 | Time: 0.525s
INFO:__main__:Epoch 1 | Step 3140/6501 | Loss: 0.4814 | PPL: 1.65 | LR: 4.83e-05 | Grad: 0.394 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 3150/6501 | Loss: 0.4500 | PPL: 1.59 | LR: 4.80e-05 | Grad: 0.339 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 3160/6501 | Loss: 0.4588 | PPL: 1.60 | LR: 4.78e-05 | Grad: 0.421 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 3170/6501 | Loss: 0.4563 | PPL: 1.60 | LR: 4.75e-05 | Grad: 0.309 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 3180/6501 | Loss: 0.4252 | PPL: 1.55 | LR: 4.72e-05 | Grad: 0.310 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 3190/6501 | Loss: 0.4514 | PPL: 1.59 | LR: 4.70e-05 | Grad: 0.458 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 3200/6501 | Loss: 0.4539 | PPL: 1.60 | LR: 4.67e-05 | Grad: 0.266 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 3210/6501 | Loss: 0.4499 | PPL: 1.59 | LR: 4.65e-05 | Grad: 0.320 | Time: 0.518s
INFO:__main__:Epoch 1 | Step 3220/6501 | Loss: 0.4735 | PPL: 1.63 | LR: 4.62e-05 | Grad: 0.360 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 3230/6501 | Loss: 0.4731 | PPL: 1.62 | LR: 4.59e-05 | Grad: 0.305 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 3240/6501 | Loss: 0.4543 | PPL: 1.59 | LR: 4.57e-05 | Grad: 0.263 | Time: 0.519s
INFO:__main__:Epoch 1 | Step 3250/6501 | Loss: 0.4717 | PPL: 1.62 | LR: 4.54e-05 | Grad: 0.276 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 3260/6501 | Loss: 0.4817 | PPL: 1.64 | LR: 4.52e-05 | Grad: 0.457 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 3270/6501 | Loss: 0.4882 | PPL: 1.66 | LR: 4.49e-05 | Grad: 0.759 | Time: 0.527s
INFO:__main__:Epoch 1 | Step 3280/6501 | Loss: 0.4943 | PPL: 1.67 | LR: 4.46e-05 | Grad: 0.263 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 3290/6501 | Loss: 0.5026 | PPL: 1.68 | LR: 4.44e-05 | Grad: 0.384 | Time: 0.526s
INFO:__main__:Epoch 1 | Step 3300/6501 | Loss: 0.4957 | PPL: 1.67 | LR: 4.41e-05 | Grad: 0.368 | Time: 0.526s
INFO:__main__:Epoch 1 | Step 3310/6501 | Loss: 0.5016 | PPL: 1.68 | LR: 4.39e-05 | Grad: 0.311 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 3320/6501 | Loss: 0.4730 | PPL: 1.63 | LR: 4.36e-05 | Grad: 0.298 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 3330/6501 | Loss: 0.4937 | PPL: 1.66 | LR: 4.34e-05 | Grad: 0.360 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 3340/6501 | Loss: 0.4912 | PPL: 1.66 | LR: 4.31e-05 | Grad: 0.313 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 3350/6501 | Loss: 0.4931 | PPL: 1.66 | LR: 4.29e-05 | Grad: 0.379 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 3360/6501 | Loss: 0.4914 | PPL: 1.66 | LR: 4.26e-05 | Grad: 0.351 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 3370/6501 | Loss: 0.4709 | PPL: 1.63 | LR: 4.24e-05 | Grad: 0.234 | Time: 0.519s
INFO:__main__:Epoch 1 | Step 3380/6501 | Loss: 0.4635 | PPL: 1.61 | LR: 4.21e-05 | Grad: 0.309 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 3390/6501 | Loss: 0.4874 | PPL: 1.65 | LR: 4.19e-05 | Grad: 0.357 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 3400/6501 | Loss: 0.4620 | PPL: 1.61 | LR: 4.16e-05 | Grad: 0.371 | Time: 0.519s
INFO:__main__:Epoch 1 | Step 3410/6501 | Loss: 0.4408 | PPL: 1.58 | LR: 4.14e-05 | Grad: 0.314 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 3420/6501 | Loss: 0.4611 | PPL: 1.61 | LR: 4.11e-05 | Grad: 0.197 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 3430/6501 | Loss: 0.4530 | PPL: 1.60 | LR: 4.09e-05 | Grad: 0.301 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 3440/6501 | Loss: 0.4349 | PPL: 1.57 | LR: 4.06e-05 | Grad: 0.443 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 3450/6501 | Loss: 0.4517 | PPL: 1.59 | LR: 4.04e-05 | Grad: 0.279 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 3460/6501 | Loss: 0.4627 | PPL: 1.61 | LR: 4.01e-05 | Grad: 0.400 | Time: 0.527s
INFO:__main__:Epoch 1 | Step 3470/6501 | Loss: 0.4695 | PPL: 1.62 | LR: 3.99e-05 | Grad: 0.265 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 3480/6501 | Loss: 0.4652 | PPL: 1.61 | LR: 3.96e-05 | Grad: 0.362 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 3490/6501 | Loss: 0.4714 | PPL: 1.62 | LR: 3.94e-05 | Grad: 0.337 | Time: 0.526s
INFO:__main__:Epoch 1 | Step 3500/6501 | Loss: 0.4553 | PPL: 1.59 | LR: 3.91e-05 | Grad: 0.361 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 3510/6501 | Loss: 0.4378 | PPL: 1.57 | LR: 3.89e-05 | Grad: 0.303 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 3520/6501 | Loss: 0.4290 | PPL: 1.55 | LR: 3.87e-05 | Grad: 0.366 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 3530/6501 | Loss: 0.4290 | PPL: 1.56 | LR: 3.84e-05 | Grad: 0.349 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 3540/6501 | Loss: 0.4251 | PPL: 1.55 | LR: 3.82e-05 | Grad: 0.451 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 3550/6501 | Loss: 0.4637 | PPL: 1.62 | LR: 3.79e-05 | Grad: 0.404 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 3560/6501 | Loss: 0.4734 | PPL: 1.64 | LR: 3.77e-05 | Grad: 0.339 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 3570/6501 | Loss: 0.4812 | PPL: 1.65 | LR: 3.75e-05 | Grad: 0.299 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 3580/6501 | Loss: 0.4848 | PPL: 1.64 | LR: 3.72e-05 | Grad: 0.358 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 3590/6501 | Loss: 0.4878 | PPL: 1.65 | LR: 3.70e-05 | Grad: 0.365 | Time: 0.526s
INFO:__main__:Epoch 1 | Step 3600/6501 | Loss: 0.4800 | PPL: 1.63 | LR: 3.67e-05 | Grad: 0.357 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 3610/6501 | Loss: 0.4854 | PPL: 1.64 | LR: 3.65e-05 | Grad: 0.277 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 3620/6501 | Loss: 0.4828 | PPL: 1.64 | LR: 3.63e-05 | Grad: 0.273 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 3630/6501 | Loss: 0.4802 | PPL: 1.64 | LR: 3.60e-05 | Grad: 0.169 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 3640/6501 | Loss: 0.4683 | PPL: 1.62 | LR: 3.58e-05 | Grad: 0.269 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 3650/6501 | Loss: 0.4660 | PPL: 1.62 | LR: 3.56e-05 | Grad: 0.400 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 3660/6501 | Loss: 0.4753 | PPL: 1.64 | LR: 3.53e-05 | Grad: 0.626 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 3670/6501 | Loss: 0.4898 | PPL: 1.66 | LR: 3.51e-05 | Grad: 0.512 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 3680/6501 | Loss: 0.4972 | PPL: 1.67 | LR: 3.49e-05 | Grad: 0.283 | Time: 0.519s
INFO:__main__:Epoch 1 | Step 3690/6501 | Loss: 0.5159 | PPL: 1.70 | LR: 3.47e-05 | Grad: 0.257 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 3700/6501 | Loss: 0.5050 | PPL: 1.69 | LR: 3.44e-05 | Grad: 0.273 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 3710/6501 | Loss: 0.4968 | PPL: 1.67 | LR: 3.42e-05 | Grad: 0.364 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 3720/6501 | Loss: 0.4637 | PPL: 1.62 | LR: 3.40e-05 | Grad: 0.381 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 3730/6501 | Loss: 0.4556 | PPL: 1.60 | LR: 3.37e-05 | Grad: 0.329 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 3740/6501 | Loss: 0.4473 | PPL: 1.59 | LR: 3.35e-05 | Grad: 0.395 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 3750/6501 | Loss: 0.4487 | PPL: 1.58 | LR: 3.33e-05 | Grad: 0.274 | Time: 0.518s
INFO:__main__:Epoch 1 | Step 3760/6501 | Loss: 0.4524 | PPL: 1.59 | LR: 3.31e-05 | Grad: 0.314 | Time: 0.525s
INFO:__main__:Epoch 1 | Step 3770/6501 | Loss: 0.4637 | PPL: 1.61 | LR: 3.28e-05 | Grad: 0.242 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 3780/6501 | Loss: 0.4610 | PPL: 1.61 | LR: 3.26e-05 | Grad: 0.281 | Time: 0.525s
INFO:__main__:Epoch 1 | Step 3790/6501 | Loss: 0.4561 | PPL: 1.60 | LR: 3.24e-05 | Grad: 0.363 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 3800/6501 | Loss: 0.4604 | PPL: 1.61 | LR: 3.22e-05 | Grad: 0.376 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 3810/6501 | Loss: 0.4420 | PPL: 1.58 | LR: 3.19e-05 | Grad: 0.331 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 3820/6501 | Loss: 0.4579 | PPL: 1.60 | LR: 3.17e-05 | Grad: 0.526 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 3830/6501 | Loss: 0.4620 | PPL: 1.61 | LR: 3.15e-05 | Grad: 0.304 | Time: 0.519s
INFO:__main__:Epoch 1 | Step 3840/6501 | Loss: 0.4737 | PPL: 1.64 | LR: 3.13e-05 | Grad: 0.238 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 3850/6501 | Loss: 0.4757 | PPL: 1.65 | LR: 3.11e-05 | Grad: 0.279 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 3860/6501 | Loss: 0.4890 | PPL: 1.67 | LR: 3.08e-05 | Grad: 0.334 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 3870/6501 | Loss: 0.4847 | PPL: 1.66 | LR: 3.06e-05 | Grad: 0.390 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 3880/6501 | Loss: 0.4966 | PPL: 1.68 | LR: 3.04e-05 | Grad: 0.354 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 3890/6501 | Loss: 0.4660 | PPL: 1.63 | LR: 3.02e-05 | Grad: 0.261 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 3900/6501 | Loss: 0.4396 | PPL: 1.58 | LR: 3.00e-05 | Grad: 0.351 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 3910/6501 | Loss: 0.4344 | PPL: 1.57 | LR: 2.98e-05 | Grad: 0.460 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 3920/6501 | Loss: 0.4337 | PPL: 1.57 | LR: 2.95e-05 | Grad: 0.427 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 3930/6501 | Loss: 0.4150 | PPL: 1.53 | LR: 2.93e-05 | Grad: 0.385 | Time: 0.527s
INFO:__main__:Epoch 1 | Step 3940/6501 | Loss: 0.4370 | PPL: 1.57 | LR: 2.91e-05 | Grad: 0.335 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 3950/6501 | Loss: 0.4748 | PPL: 1.63 | LR: 2.89e-05 | Grad: 0.441 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 3960/6501 | Loss: 0.4732 | PPL: 1.62 | LR: 2.87e-05 | Grad: 0.310 | Time: 0.527s
INFO:__main__:Epoch 1 | Step 3970/6501 | Loss: 0.4712 | PPL: 1.62 | LR: 2.85e-05 | Grad: 0.302 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 3980/6501 | Loss: 0.4849 | PPL: 1.64 | LR: 2.83e-05 | Grad: 0.441 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 3990/6501 | Loss: 0.4819 | PPL: 1.63 | LR: 2.81e-05 | Grad: 0.331 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 4000/6501 | Loss: 0.4677 | PPL: 1.61 | LR: 2.79e-05 | Grad: 0.384 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 4010/6501 | Loss: 0.4898 | PPL: 1.65 | LR: 2.76e-05 | Grad: 0.294 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 4020/6501 | Loss: 0.5195 | PPL: 1.71 | LR: 2.74e-05 | Grad: 0.567 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 4030/6501 | Loss: 0.5235 | PPL: 1.71 | LR: 2.72e-05 | Grad: 0.454 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 4040/6501 | Loss: 0.5357 | PPL: 1.74 | LR: 2.70e-05 | Grad: 0.458 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 4050/6501 | Loss: 0.5411 | PPL: 1.75 | LR: 2.68e-05 | Grad: 0.276 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 4060/6501 | Loss: 0.5115 | PPL: 1.70 | LR: 2.66e-05 | Grad: 0.277 | Time: 0.525s
INFO:__main__:Epoch 1 | Step 4070/6501 | Loss: 0.4968 | PPL: 1.68 | LR: 2.64e-05 | Grad: 0.435 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 4080/6501 | Loss: 0.4909 | PPL: 1.66 | LR: 2.62e-05 | Grad: 0.359 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 4090/6501 | Loss: 0.4772 | PPL: 1.64 | LR: 2.60e-05 | Grad: 0.315 | Time: 0.527s
INFO:__main__:Epoch 1 | Step 4100/6501 | Loss: 0.4719 | PPL: 1.63 | LR: 2.58e-05 | Grad: 0.279 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 4110/6501 | Loss: 0.4949 | PPL: 1.67 | LR: 2.56e-05 | Grad: 0.388 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 4120/6501 | Loss: 0.4809 | PPL: 1.64 | LR: 2.54e-05 | Grad: 0.320 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 4130/6501 | Loss: 0.4773 | PPL: 1.63 | LR: 2.52e-05 | Grad: 0.388 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 4140/6501 | Loss: 0.4738 | PPL: 1.62 | LR: 2.50e-05 | Grad: 0.413 | Time: 0.525s
INFO:__main__:Epoch 1 | Step 4150/6501 | Loss: 0.4680 | PPL: 1.61 | LR: 2.48e-05 | Grad: 0.273 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 4160/6501 | Loss: 0.4632 | PPL: 1.61 | LR: 2.46e-05 | Grad: 0.365 | Time: 0.526s
INFO:__main__:Epoch 1 | Step 4170/6501 | Loss: 0.4725 | PPL: 1.62 | LR: 2.44e-05 | Grad: 0.361 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 4180/6501 | Loss: 0.4890 | PPL: 1.65 | LR: 2.42e-05 | Grad: 0.369 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 4190/6501 | Loss: 0.4934 | PPL: 1.66 | LR: 2.40e-05 | Grad: 0.422 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 4200/6501 | Loss: 0.5119 | PPL: 1.69 | LR: 2.38e-05 | Grad: 0.338 | Time: 0.518s
INFO:__main__:Epoch 1 | Step 4210/6501 | Loss: 0.4941 | PPL: 1.66 | LR: 2.36e-05 | Grad: 0.373 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 4220/6501 | Loss: 0.4764 | PPL: 1.63 | LR: 2.35e-05 | Grad: 0.268 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 4230/6501 | Loss: 0.4519 | PPL: 1.59 | LR: 2.33e-05 | Grad: 0.319 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 4240/6501 | Loss: 0.4594 | PPL: 1.60 | LR: 2.31e-05 | Grad: 0.435 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 4250/6501 | Loss: 0.4343 | PPL: 1.55 | LR: 2.29e-05 | Grad: 0.300 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 4260/6501 | Loss: 0.4686 | PPL: 1.61 | LR: 2.27e-05 | Grad: 0.350 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 4270/6501 | Loss: 0.4786 | PPL: 1.63 | LR: 2.25e-05 | Grad: 0.234 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 4280/6501 | Loss: 0.4846 | PPL: 1.64 | LR: 2.23e-05 | Grad: 0.314 | Time: 0.526s
INFO:__main__:Epoch 1 | Step 4290/6501 | Loss: 0.4630 | PPL: 1.61 | LR: 2.21e-05 | Grad: 0.209 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 4300/6501 | Loss: 0.4839 | PPL: 1.65 | LR: 2.19e-05 | Grad: 0.212 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 4310/6501 | Loss: 0.4722 | PPL: 1.62 | LR: 2.18e-05 | Grad: 0.632 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 4320/6501 | Loss: 0.4729 | PPL: 1.63 | LR: 2.16e-05 | Grad: 0.353 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 4330/6501 | Loss: 0.4972 | PPL: 1.67 | LR: 2.14e-05 | Grad: 0.473 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 4340/6501 | Loss: 0.5329 | PPL: 1.73 | LR: 2.12e-05 | Grad: 0.427 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 4350/6501 | Loss: 0.5094 | PPL: 1.69 | LR: 2.10e-05 | Grad: 0.310 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 4360/6501 | Loss: 0.4972 | PPL: 1.67 | LR: 2.08e-05 | Grad: 0.384 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 4370/6501 | Loss: 0.4820 | PPL: 1.64 | LR: 2.07e-05 | Grad: 0.339 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 4380/6501 | Loss: 0.4480 | PPL: 1.58 | LR: 2.05e-05 | Grad: 0.306 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 4390/6501 | Loss: 0.4344 | PPL: 1.56 | LR: 2.03e-05 | Grad: 0.422 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 4400/6501 | Loss: 0.4416 | PPL: 1.57 | LR: 2.01e-05 | Grad: 0.246 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 4410/6501 | Loss: 0.4479 | PPL: 1.58 | LR: 1.99e-05 | Grad: 0.422 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 4420/6501 | Loss: 0.4523 | PPL: 1.59 | LR: 1.98e-05 | Grad: 0.415 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 4430/6501 | Loss: 0.4675 | PPL: 1.61 | LR: 1.96e-05 | Grad: 0.325 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 4440/6501 | Loss: 0.4427 | PPL: 1.57 | LR: 1.94e-05 | Grad: 0.368 | Time: 0.525s
INFO:__main__:Epoch 1 | Step 4450/6501 | Loss: 0.4571 | PPL: 1.60 | LR: 1.92e-05 | Grad: 0.318 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 4460/6501 | Loss: 0.4463 | PPL: 1.58 | LR: 1.91e-05 | Grad: 0.180 | Time: 0.519s
INFO:__main__:Epoch 1 | Step 4470/6501 | Loss: 0.4511 | PPL: 1.59 | LR: 1.89e-05 | Grad: 0.269 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 4480/6501 | Loss: 0.4439 | PPL: 1.58 | LR: 1.87e-05 | Grad: 0.268 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 4490/6501 | Loss: 0.4690 | PPL: 1.63 | LR: 1.86e-05 | Grad: 0.389 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 4500/6501 | Loss: 0.4624 | PPL: 1.61 | LR: 1.84e-05 | Grad: 0.298 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 4510/6501 | Loss: 0.4686 | PPL: 1.62 | LR: 1.82e-05 | Grad: 0.281 | Time: 0.517s
INFO:__main__:Epoch 1 | Step 4520/6501 | Loss: 0.4815 | PPL: 1.64 | LR: 1.80e-05 | Grad: 0.433 | Time: 0.525s
INFO:__main__:Epoch 1 | Step 4530/6501 | Loss: 0.4902 | PPL: 1.65 | LR: 1.79e-05 | Grad: 0.311 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 4540/6501 | Loss: 0.4820 | PPL: 1.64 | LR: 1.77e-05 | Grad: 0.376 | Time: 0.526s
INFO:__main__:Epoch 1 | Step 4550/6501 | Loss: 0.4775 | PPL: 1.63 | LR: 1.75e-05 | Grad: 0.370 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 4560/6501 | Loss: 0.4817 | PPL: 1.64 | LR: 1.74e-05 | Grad: 0.469 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 4570/6501 | Loss: 0.4632 | PPL: 1.61 | LR: 1.72e-05 | Grad: 0.322 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 4580/6501 | Loss: 0.4722 | PPL: 1.62 | LR: 1.70e-05 | Grad: 0.301 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 4590/6501 | Loss: 0.4668 | PPL: 1.61 | LR: 1.69e-05 | Grad: 0.428 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 4600/6501 | Loss: 0.4670 | PPL: 1.61 | LR: 1.67e-05 | Grad: 0.421 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 4610/6501 | Loss: 0.4711 | PPL: 1.62 | LR: 1.66e-05 | Grad: 0.625 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 4620/6501 | Loss: 0.4684 | PPL: 1.61 | LR: 1.64e-05 | Grad: 0.322 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 4630/6501 | Loss: 0.4565 | PPL: 1.59 | LR: 1.62e-05 | Grad: 0.357 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 4640/6501 | Loss: 0.4788 | PPL: 1.63 | LR: 1.61e-05 | Grad: 0.517 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 4650/6501 | Loss: 0.4833 | PPL: 1.64 | LR: 1.59e-05 | Grad: 0.417 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 4660/6501 | Loss: 0.4688 | PPL: 1.62 | LR: 1.58e-05 | Grad: 0.437 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 4670/6501 | Loss: 0.4687 | PPL: 1.62 | LR: 1.56e-05 | Grad: 0.352 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 4680/6501 | Loss: 0.4778 | PPL: 1.63 | LR: 1.54e-05 | Grad: 0.414 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 4690/6501 | Loss: 0.4427 | PPL: 1.57 | LR: 1.53e-05 | Grad: 0.276 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 4700/6501 | Loss: 0.4492 | PPL: 1.59 | LR: 1.51e-05 | Grad: 0.253 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 4710/6501 | Loss: 0.4652 | PPL: 1.63 | LR: 1.50e-05 | Grad: 0.385 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 4720/6501 | Loss: 0.4803 | PPL: 1.66 | LR: 1.48e-05 | Grad: 0.363 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 4730/6501 | Loss: 0.4776 | PPL: 1.65 | LR: 1.47e-05 | Grad: 0.299 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 4740/6501 | Loss: 0.5023 | PPL: 1.69 | LR: 1.45e-05 | Grad: 0.402 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 4750/6501 | Loss: 0.4786 | PPL: 1.65 | LR: 1.44e-05 | Grad: 0.339 | Time: 0.526s
INFO:__main__:Epoch 1 | Step 4760/6501 | Loss: 0.4974 | PPL: 1.66 | LR: 1.42e-05 | Grad: 0.387 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 4770/6501 | Loss: 0.4799 | PPL: 1.64 | LR: 1.41e-05 | Grad: 0.268 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 4780/6501 | Loss: 0.4759 | PPL: 1.63 | LR: 1.39e-05 | Grad: 0.254 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 4790/6501 | Loss: 0.4774 | PPL: 1.63 | LR: 1.38e-05 | Grad: 0.468 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 4800/6501 | Loss: 0.5100 | PPL: 1.69 | LR: 1.36e-05 | Grad: 0.340 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 4810/6501 | Loss: 0.4843 | PPL: 1.64 | LR: 1.35e-05 | Grad: 0.380 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 4820/6501 | Loss: 0.4907 | PPL: 1.65 | LR: 1.33e-05 | Grad: 0.264 | Time: 0.517s
INFO:__main__:Epoch 1 | Step 4830/6501 | Loss: 0.4923 | PPL: 1.66 | LR: 1.32e-05 | Grad: 0.356 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 4840/6501 | Loss: 0.4719 | PPL: 1.62 | LR: 1.30e-05 | Grad: 0.330 | Time: 0.519s
INFO:__main__:Epoch 1 | Step 4850/6501 | Loss: 0.4717 | PPL: 1.62 | LR: 1.29e-05 | Grad: 0.469 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 4860/6501 | Loss: 0.4881 | PPL: 1.65 | LR: 1.28e-05 | Grad: 0.409 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 4870/6501 | Loss: 0.4820 | PPL: 1.64 | LR: 1.26e-05 | Grad: 0.372 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 4880/6501 | Loss: 0.4797 | PPL: 1.64 | LR: 1.25e-05 | Grad: 0.283 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 4890/6501 | Loss: 0.4935 | PPL: 1.66 | LR: 1.23e-05 | Grad: 0.270 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 4900/6501 | Loss: 0.4702 | PPL: 1.62 | LR: 1.22e-05 | Grad: 0.266 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 4910/6501 | Loss: 0.4615 | PPL: 1.60 | LR: 1.21e-05 | Grad: 0.352 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 4920/6501 | Loss: 0.4669 | PPL: 1.62 | LR: 1.19e-05 | Grad: 0.332 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 4930/6501 | Loss: 0.4707 | PPL: 1.62 | LR: 1.18e-05 | Grad: 0.376 | Time: 0.528s
INFO:__main__:Epoch 1 | Step 4940/6501 | Loss: 0.4613 | PPL: 1.60 | LR: 1.17e-05 | Grad: 0.310 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 4950/6501 | Loss: 0.4633 | PPL: 1.61 | LR: 1.15e-05 | Grad: 0.328 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 4960/6501 | Loss: 0.4445 | PPL: 1.58 | LR: 1.14e-05 | Grad: 0.386 | Time: 0.525s
INFO:__main__:Epoch 1 | Step 4970/6501 | Loss: 0.4563 | PPL: 1.59 | LR: 1.13e-05 | Grad: 0.382 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 4980/6501 | Loss: 0.4513 | PPL: 1.59 | LR: 1.11e-05 | Grad: 0.260 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 4990/6501 | Loss: 0.4482 | PPL: 1.59 | LR: 1.10e-05 | Grad: 0.398 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 5000/6501 | Loss: 0.4431 | PPL: 1.58 | LR: 1.09e-05 | Grad: 0.298 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 5010/6501 | Loss: 0.4449 | PPL: 1.58 | LR: 1.07e-05 | Grad: 0.359 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 5020/6501 | Loss: 0.4541 | PPL: 1.60 | LR: 1.06e-05 | Grad: 0.444 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 5030/6501 | Loss: 0.4325 | PPL: 1.56 | LR: 1.05e-05 | Grad: 0.283 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 5040/6501 | Loss: 0.4389 | PPL: 1.57 | LR: 1.03e-05 | Grad: 0.375 | Time: 0.518s
INFO:__main__:Epoch 1 | Step 5050/6501 | Loss: 0.4592 | PPL: 1.61 | LR: 1.02e-05 | Grad: 0.405 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 5060/6501 | Loss: 0.4643 | PPL: 1.61 | LR: 1.01e-05 | Grad: 0.357 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 5070/6501 | Loss: 0.4690 | PPL: 1.62 | LR: 9.97e-06 | Grad: 0.401 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 5080/6501 | Loss: 0.5051 | PPL: 1.68 | LR: 9.85e-06 | Grad: 0.487 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 5090/6501 | Loss: 0.5261 | PPL: 1.71 | LR: 9.72e-06 | Grad: 0.361 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 5100/6501 | Loss: 0.5391 | PPL: 1.73 | LR: 9.60e-06 | Grad: 0.405 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 5110/6501 | Loss: 0.5353 | PPL: 1.73 | LR: 9.48e-06 | Grad: 0.356 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 5120/6501 | Loss: 0.5081 | PPL: 1.68 | LR: 9.36e-06 | Grad: 0.269 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 5130/6501 | Loss: 0.4851 | PPL: 1.64 | LR: 9.24e-06 | Grad: 0.315 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 5140/6501 | Loss: 0.4466 | PPL: 1.58 | LR: 9.12e-06 | Grad: 0.206 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 5150/6501 | Loss: 0.4261 | PPL: 1.55 | LR: 9.00e-06 | Grad: 0.311 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 5160/6501 | Loss: 0.4448 | PPL: 1.58 | LR: 8.89e-06 | Grad: 0.372 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 5170/6501 | Loss: 0.4466 | PPL: 1.59 | LR: 8.77e-06 | Grad: 0.243 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 5180/6501 | Loss: 0.4596 | PPL: 1.61 | LR: 8.66e-06 | Grad: 0.382 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 5190/6501 | Loss: 0.4794 | PPL: 1.65 | LR: 8.54e-06 | Grad: 0.384 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 5200/6501 | Loss: 0.4560 | PPL: 1.61 | LR: 8.43e-06 | Grad: 0.297 | Time: 0.519s
INFO:__main__:Epoch 1 | Step 5210/6501 | Loss: 0.4275 | PPL: 1.56 | LR: 8.31e-06 | Grad: 0.330 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 5220/6501 | Loss: 0.4389 | PPL: 1.57 | LR: 8.20e-06 | Grad: 0.277 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 5230/6501 | Loss: 0.4340 | PPL: 1.56 | LR: 8.09e-06 | Grad: 0.377 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 5240/6501 | Loss: 0.4566 | PPL: 1.60 | LR: 7.98e-06 | Grad: 0.390 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 5250/6501 | Loss: 0.4829 | PPL: 1.65 | LR: 7.87e-06 | Grad: 0.319 | Time: 0.518s
INFO:__main__:Epoch 1 | Step 5260/6501 | Loss: 0.5159 | PPL: 1.70 | LR: 7.76e-06 | Grad: 0.375 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 5270/6501 | Loss: 0.5016 | PPL: 1.68 | LR: 7.66e-06 | Grad: 0.274 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 5280/6501 | Loss: 0.4891 | PPL: 1.66 | LR: 7.55e-06 | Grad: 0.345 | Time: 0.518s
INFO:__main__:Epoch 1 | Step 5290/6501 | Loss: 0.4595 | PPL: 1.61 | LR: 7.44e-06 | Grad: 0.354 | Time: 0.519s
INFO:__main__:Epoch 1 | Step 5300/6501 | Loss: 0.4605 | PPL: 1.60 | LR: 7.34e-06 | Grad: 0.399 | Time: 0.519s
INFO:__main__:Epoch 1 | Step 5310/6501 | Loss: 0.4224 | PPL: 1.54 | LR: 7.23e-06 | Grad: 0.293 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 5320/6501 | Loss: 0.4360 | PPL: 1.57 | LR: 7.13e-06 | Grad: 0.405 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 5330/6501 | Loss: 0.4513 | PPL: 1.59 | LR: 7.03e-06 | Grad: 0.452 | Time: 0.526s
INFO:__main__:Epoch 1 | Step 5340/6501 | Loss: 0.4739 | PPL: 1.63 | LR: 6.92e-06 | Grad: 0.278 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 5350/6501 | Loss: 0.4851 | PPL: 1.65 | LR: 6.82e-06 | Grad: 0.417 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 5360/6501 | Loss: 0.5078 | PPL: 1.69 | LR: 6.72e-06 | Grad: 0.321 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 5370/6501 | Loss: 0.4939 | PPL: 1.66 | LR: 6.62e-06 | Grad: 0.283 | Time: 0.519s
INFO:__main__:Epoch 1 | Step 5380/6501 | Loss: 0.5032 | PPL: 1.68 | LR: 6.53e-06 | Grad: 0.218 | Time: 0.519s
INFO:__main__:Epoch 1 | Step 5390/6501 | Loss: 0.4865 | PPL: 1.65 | LR: 6.43e-06 | Grad: 0.359 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 5400/6501 | Loss: 0.4753 | PPL: 1.63 | LR: 6.33e-06 | Grad: 0.380 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 5410/6501 | Loss: 0.4586 | PPL: 1.60 | LR: 6.23e-06 | Grad: 0.288 | Time: 0.526s
INFO:__main__:Epoch 1 | Step 5420/6501 | Loss: 0.4687 | PPL: 1.62 | LR: 6.14e-06 | Grad: 0.327 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 5430/6501 | Loss: 0.4594 | PPL: 1.60 | LR: 6.05e-06 | Grad: 0.306 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 5440/6501 | Loss: 0.4668 | PPL: 1.61 | LR: 5.95e-06 | Grad: 0.258 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 5450/6501 | Loss: 0.4683 | PPL: 1.62 | LR: 5.86e-06 | Grad: 0.430 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 5460/6501 | Loss: 0.4701 | PPL: 1.62 | LR: 5.77e-06 | Grad: 0.273 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 5470/6501 | Loss: 0.4603 | PPL: 1.60 | LR: 5.68e-06 | Grad: 0.364 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 5480/6501 | Loss: 0.4553 | PPL: 1.59 | LR: 5.59e-06 | Grad: 0.306 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 5490/6501 | Loss: 0.4543 | PPL: 1.59 | LR: 5.50e-06 | Grad: 0.433 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 5500/6501 | Loss: 0.4533 | PPL: 1.59 | LR: 5.41e-06 | Grad: 0.481 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 5510/6501 | Loss: 0.4691 | PPL: 1.62 | LR: 5.32e-06 | Grad: 0.463 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 5520/6501 | Loss: 0.4954 | PPL: 1.67 | LR: 5.24e-06 | Grad: 0.456 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 5530/6501 | Loss: 0.4933 | PPL: 1.67 | LR: 5.15e-06 | Grad: 0.300 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 5540/6501 | Loss: 0.4905 | PPL: 1.67 | LR: 5.07e-06 | Grad: 0.296 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 5550/6501 | Loss: 0.4810 | PPL: 1.65 | LR: 4.98e-06 | Grad: 0.346 | Time: 0.527s
INFO:__main__:Epoch 1 | Step 5560/6501 | Loss: 0.4810 | PPL: 1.65 | LR: 4.90e-06 | Grad: 0.294 | Time: 0.526s
INFO:__main__:Epoch 1 | Step 5570/6501 | Loss: 0.4548 | PPL: 1.59 | LR: 4.82e-06 | Grad: 0.432 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 5580/6501 | Loss: 0.4565 | PPL: 1.60 | LR: 4.74e-06 | Grad: 0.334 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 5590/6501 | Loss: 0.4274 | PPL: 1.55 | LR: 4.66e-06 | Grad: 0.293 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 5600/6501 | Loss: 0.4280 | PPL: 1.55 | LR: 4.58e-06 | Grad: 0.308 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 5610/6501 | Loss: 0.4168 | PPL: 1.53 | LR: 4.50e-06 | Grad: 0.244 | Time: 0.518s
INFO:__main__:Epoch 1 | Step 5620/6501 | Loss: 0.4031 | PPL: 1.51 | LR: 4.42e-06 | Grad: 0.307 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 5630/6501 | Loss: 0.4118 | PPL: 1.52 | LR: 4.34e-06 | Grad: 0.327 | Time: 0.527s
INFO:__main__:Epoch 1 | Step 5640/6501 | Loss: 0.4652 | PPL: 1.61 | LR: 4.27e-06 | Grad: 0.456 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 5650/6501 | Loss: 0.4647 | PPL: 1.61 | LR: 4.19e-06 | Grad: 0.343 | Time: 0.519s
INFO:__main__:Epoch 1 | Step 5660/6501 | Loss: 0.4573 | PPL: 1.60 | LR: 4.12e-06 | Grad: 0.324 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 5670/6501 | Loss: 0.4775 | PPL: 1.63 | LR: 4.04e-06 | Grad: 0.330 | Time: 0.519s
INFO:__main__:Epoch 1 | Step 5680/6501 | Loss: 0.4682 | PPL: 1.62 | LR: 3.97e-06 | Grad: 0.343 | Time: 0.525s
INFO:__main__:Epoch 1 | Step 5690/6501 | Loss: 0.4337 | PPL: 1.56 | LR: 3.90e-06 | Grad: 0.386 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 5700/6501 | Loss: 0.4257 | PPL: 1.54 | LR: 3.83e-06 | Grad: 0.260 | Time: 0.528s
INFO:__main__:Epoch 1 | Step 5710/6501 | Loss: 0.4435 | PPL: 1.57 | LR: 3.76e-06 | Grad: 0.320 | Time: 0.526s
INFO:__main__:Epoch 1 | Step 5720/6501 | Loss: 0.4384 | PPL: 1.57 | LR: 3.69e-06 | Grad: 0.281 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 5730/6501 | Loss: 0.4485 | PPL: 1.59 | LR: 3.62e-06 | Grad: 0.341 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 5740/6501 | Loss: 0.4684 | PPL: 1.62 | LR: 3.55e-06 | Grad: 0.324 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 5750/6501 | Loss: 0.4689 | PPL: 1.62 | LR: 3.49e-06 | Grad: 0.373 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 5760/6501 | Loss: 0.4736 | PPL: 1.64 | LR: 3.42e-06 | Grad: 0.481 | Time: 0.525s
INFO:__main__:Epoch 1 | Step 5770/6501 | Loss: 0.4720 | PPL: 1.62 | LR: 3.36e-06 | Grad: 0.314 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 5780/6501 | Loss: 0.4657 | PPL: 1.61 | LR: 3.29e-06 | Grad: 0.362 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 5790/6501 | Loss: 0.4423 | PPL: 1.57 | LR: 3.23e-06 | Grad: 0.358 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 5800/6501 | Loss: 0.4407 | PPL: 1.58 | LR: 3.17e-06 | Grad: 0.182 | Time: 0.518s
INFO:__main__:Epoch 1 | Step 5810/6501 | Loss: 0.4379 | PPL: 1.57 | LR: 3.11e-06 | Grad: 0.321 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 5820/6501 | Loss: 0.4313 | PPL: 1.56 | LR: 3.04e-06 | Grad: 0.163 | Time: 0.525s
INFO:__main__:Epoch 1 | Step 5830/6501 | Loss: 0.4188 | PPL: 1.54 | LR: 2.99e-06 | Grad: 0.308 | Time: 0.525s
INFO:__main__:Epoch 1 | Step 5840/6501 | Loss: 0.4272 | PPL: 1.55 | LR: 2.93e-06 | Grad: 0.340 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 5850/6501 | Loss: 0.4308 | PPL: 1.55 | LR: 2.87e-06 | Grad: 0.485 | Time: 0.525s
INFO:__main__:Epoch 1 | Step 5860/6501 | Loss: 0.4236 | PPL: 1.54 | LR: 2.81e-06 | Grad: 0.361 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 5870/6501 | Loss: 0.4277 | PPL: 1.55 | LR: 2.76e-06 | Grad: 0.323 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 5880/6501 | Loss: 0.4467 | PPL: 1.58 | LR: 2.70e-06 | Grad: 0.398 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 5890/6501 | Loss: 0.4528 | PPL: 1.59 | LR: 2.65e-06 | Grad: 0.471 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 5900/6501 | Loss: 0.4425 | PPL: 1.57 | LR: 2.59e-06 | Grad: 0.338 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 5910/6501 | Loss: 0.4629 | PPL: 1.61 | LR: 2.54e-06 | Grad: 0.469 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 5920/6501 | Loss: 0.4710 | PPL: 1.62 | LR: 2.49e-06 | Grad: 0.399 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 5930/6501 | Loss: 0.4560 | PPL: 1.60 | LR: 2.44e-06 | Grad: 0.278 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 5940/6501 | Loss: 0.4435 | PPL: 1.58 | LR: 2.39e-06 | Grad: 0.282 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 5950/6501 | Loss: 0.4584 | PPL: 1.60 | LR: 2.34e-06 | Grad: 0.411 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 5960/6501 | Loss: 0.4384 | PPL: 1.57 | LR: 2.29e-06 | Grad: 0.308 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 5970/6501 | Loss: 0.4247 | PPL: 1.55 | LR: 2.24e-06 | Grad: 0.277 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 5980/6501 | Loss: 0.4182 | PPL: 1.53 | LR: 2.20e-06 | Grad: 0.335 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 5990/6501 | Loss: 0.4283 | PPL: 1.55 | LR: 2.15e-06 | Grad: 0.390 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 6000/6501 | Loss: 0.4394 | PPL: 1.57 | LR: 2.11e-06 | Grad: 0.467 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 6010/6501 | Loss: 0.4496 | PPL: 1.59 | LR: 2.06e-06 | Grad: 0.264 | Time: 0.528s
INFO:__main__:Epoch 1 | Step 6020/6501 | Loss: 0.4816 | PPL: 1.64 | LR: 2.02e-06 | Grad: 0.310 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 6030/6501 | Loss: 0.4995 | PPL: 1.67 | LR: 1.98e-06 | Grad: 0.385 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 6040/6501 | Loss: 0.5116 | PPL: 1.70 | LR: 1.94e-06 | Grad: 0.513 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 6050/6501 | Loss: 0.5034 | PPL: 1.69 | LR: 1.90e-06 | Grad: 0.272 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 6060/6501 | Loss: 0.4874 | PPL: 1.66 | LR: 1.86e-06 | Grad: 0.347 | Time: 0.519s
INFO:__main__:Epoch 1 | Step 6070/6501 | Loss: 0.4615 | PPL: 1.61 | LR: 1.82e-06 | Grad: 0.317 | Time: 0.524s
INFO:__main__:Epoch 1 | Step 6080/6501 | Loss: 0.4651 | PPL: 1.62 | LR: 1.78e-06 | Grad: 0.503 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 6090/6501 | Loss: 0.4373 | PPL: 1.57 | LR: 1.74e-06 | Grad: 0.295 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 6100/6501 | Loss: 0.4420 | PPL: 1.57 | LR: 1.71e-06 | Grad: 0.298 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 6110/6501 | Loss: 0.4332 | PPL: 1.56 | LR: 1.67e-06 | Grad: 0.405 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 6120/6501 | Loss: 0.4348 | PPL: 1.56 | LR: 1.64e-06 | Grad: 0.315 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 6130/6501 | Loss: 0.4319 | PPL: 1.56 | LR: 1.61e-06 | Grad: 0.447 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 6140/6501 | Loss: 0.4505 | PPL: 1.59 | LR: 1.57e-06 | Grad: 0.320 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 6150/6501 | Loss: 0.4453 | PPL: 1.58 | LR: 1.54e-06 | Grad: 0.284 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 6160/6501 | Loss: 0.4565 | PPL: 1.60 | LR: 1.51e-06 | Grad: 0.507 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 6170/6501 | Loss: 0.4546 | PPL: 1.60 | LR: 1.48e-06 | Grad: 0.255 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 6180/6501 | Loss: 0.4494 | PPL: 1.59 | LR: 1.45e-06 | Grad: 0.439 | Time: 0.525s
INFO:__main__:Epoch 1 | Step 6190/6501 | Loss: 0.4543 | PPL: 1.59 | LR: 1.43e-06 | Grad: 0.342 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 6200/6501 | Loss: 0.4376 | PPL: 1.57 | LR: 1.40e-06 | Grad: 0.300 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 6210/6501 | Loss: 0.4538 | PPL: 1.59 | LR: 1.37e-06 | Grad: 0.371 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 6220/6501 | Loss: 0.4830 | PPL: 1.64 | LR: 1.35e-06 | Grad: 0.368 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 6230/6501 | Loss: 0.5062 | PPL: 1.68 | LR: 1.32e-06 | Grad: 0.429 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 6240/6501 | Loss: 0.5236 | PPL: 1.71 | LR: 1.30e-06 | Grad: 0.326 | Time: 0.525s
INFO:__main__:Epoch 1 | Step 6250/6501 | Loss: 0.5309 | PPL: 1.72 | LR: 1.28e-06 | Grad: 0.336 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 6260/6501 | Loss: 0.5096 | PPL: 1.69 | LR: 1.26e-06 | Grad: 0.317 | Time: 0.518s
INFO:__main__:Epoch 1 | Step 6270/6501 | Loss: 0.4844 | PPL: 1.64 | LR: 1.23e-06 | Grad: 0.396 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 6280/6501 | Loss: 0.4686 | PPL: 1.62 | LR: 1.21e-06 | Grad: 0.368 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 6290/6501 | Loss: 0.4278 | PPL: 1.54 | LR: 1.20e-06 | Grad: 0.328 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 6300/6501 | Loss: 0.4371 | PPL: 1.56 | LR: 1.18e-06 | Grad: 0.309 | Time: 0.520s
INFO:__main__:Epoch 1 | Step 6310/6501 | Loss: 0.4599 | PPL: 1.60 | LR: 1.16e-06 | Grad: 0.496 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 6320/6501 | Loss: 0.4630 | PPL: 1.60 | LR: 1.14e-06 | Grad: 0.303 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 6330/6501 | Loss: 0.4504 | PPL: 1.59 | LR: 1.13e-06 | Grad: 0.310 | Time: 0.519s
INFO:__main__:Epoch 1 | Step 6340/6501 | Loss: 0.4582 | PPL: 1.60 | LR: 1.11e-06 | Grad: 0.353 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 6350/6501 | Loss: 0.4640 | PPL: 1.61 | LR: 1.10e-06 | Grad: 0.363 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 6360/6501 | Loss: 0.4418 | PPL: 1.57 | LR: 1.09e-06 | Grad: 0.275 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 6370/6501 | Loss: 0.4414 | PPL: 1.57 | LR: 1.07e-06 | Grad: 0.349 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 6380/6501 | Loss: 0.4403 | PPL: 1.57 | LR: 1.06e-06 | Grad: 0.506 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 6390/6501 | Loss: 0.4332 | PPL: 1.56 | LR: 1.05e-06 | Grad: 0.233 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 6400/6501 | Loss: 0.4404 | PPL: 1.58 | LR: 1.04e-06 | Grad: 0.337 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 6410/6501 | Loss: 0.4539 | PPL: 1.60 | LR: 1.04e-06 | Grad: 0.478 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 6420/6501 | Loss: 0.4443 | PPL: 1.59 | LR: 1.03e-06 | Grad: 0.293 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 6430/6501 | Loss: 0.4430 | PPL: 1.58 | LR: 1.02e-06 | Grad: 0.232 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 6440/6501 | Loss: 0.4849 | PPL: 1.65 | LR: 1.02e-06 | Grad: 0.446 | Time: 0.522s
INFO:__main__:Epoch 1 | Step 6450/6501 | Loss: 0.4824 | PPL: 1.64 | LR: 1.01e-06 | Grad: 0.295 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 6460/6501 | Loss: 0.4984 | PPL: 1.67 | LR: 1.01e-06 | Grad: 0.309 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 6470/6501 | Loss: 0.5131 | PPL: 1.70 | LR: 1.00e-06 | Grad: 0.505 | Time: 0.521s
INFO:__main__:Epoch 1 | Step 6480/6501 | Loss: 0.5215 | PPL: 1.71 | LR: 1.00e-06 | Grad: 0.275 | Time: 0.525s
INFO:__main__:Epoch 1 | Step 6490/6501 | Loss: 0.4933 | PPL: 1.67 | LR: 1.00e-06 | Grad: 0.420 | Time: 0.523s
INFO:__main__:Epoch 1 | Step 6500/6501 | Loss: 0.4631 | PPL: 1.61 | LR: 1.00e-06 | Grad: 0.618 | Time: 0.130s
INFO:__main__:📊 Evaluation Results:
INFO:__main__:   Loss: 0.4887
INFO:__main__:   Perplexity: 1.63
INFO:__main__:   Tokens: 2,982,385
INFO:__main__:✅ Epoch 1 completed in 4730.85s | Avg Loss: 0.4735
INFO:__main__:💎 New best checkpoint saved: outputs/lora_experiment/best_checkpoint.pt
INFO:__main__:💾 Checkpoint saved: outputs/lora_experiment/checkpoint_epoch_1_step_6501.pt
INFO:__main__:🏁 Final evaluation...
INFO:__main__:📊 Evaluation Results:
INFO:__main__:   Loss: 0.4886
INFO:__main__:   Perplexity: 1.63
INFO:__main__:   Tokens: 2,982,385
INFO:__main__:🎉 Final Results: Loss=0.4886, PPL=1.63
INFO:__main__:📊 Final metrics saved: outputs/lora_experiment/final_metrics.json
INFO:__main__:✅ Training completed successfully!
INFO:__main__:🎉 Training pipeline completed successfully!
